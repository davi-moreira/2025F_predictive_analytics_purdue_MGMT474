[
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, time series, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025F_predictive_analytics_purdue_MGMT474/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 1007\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required):\n\nGoogle Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\nMicrosoft Copilot: is an AI-powered assistant designed to enhance productivity and streamline workflows across various applications and services. It utilizes large language models and is integrated within Microsoft 365 apps like Word, Excel, PowerPoint, Outlook, and Teams, providing real-time, context-aware assistance for tasks such as drafting documents, analyzing data, managing projects, and communicating more efficiently. Users can leverage Copilot to automate repetitive tasks, generate ideas, summarize information, and access data across their work environment and the web, all within a secure and privacy-conscious framework.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Deep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#overview",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nDeep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#deep-learning-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\nEarly Rise (1980s)\n\nNeural networks first gained popularity.\n\nHigh levels of excitement, with dedicated conferences (e.g., NeurIPS, Snowbird).\n\n1990s Shift\n\nEmergence of other methods (SVMs, Random Forests, Boosting).\n\nNeural networks receded into the background.\n\nResurgence (2010)\n\nRebranded and refined under the banner of Deep Learning.\n\nBy the 2020s, became extremely successful and widely adopted.\n\nKey Drivers of Success\n\nRapid increases in computing power (GPUs, parallel computing).\n\nAvailability of large-scale datasets.\n\nUser-friendly deep learning libraries (e.g., TensorFlow, PyTorch).\n\n\n\n\nMuch of the credit goes to three pioneers and their students:\n\nYann LeCun, Geoffrey Hinton, and Yoshua Bengio,\nwho received the 2019 ACM Turing Award for their work in Neural Networks."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#ai-visionaries-interviews",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#ai-visionaries-interviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AI Visionaries: Interviews",
    "text": "AI Visionaries: Interviews\n\n\n\n\n\n\n\n\n Yann LeCunThe Future of AIDec 16, 2023 \n\n\n\n\n\n\n\n Geoffrey Hinton60 Minutes InterviewOct 9, 2023 \n\n\n\n\n\n\n\n Yoshua BengioPath to Human-Level AIApr 24, 2024"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#what-are-deep-learning-frameworks",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#what-are-deep-learning-frameworks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What Are Deep Learning Frameworks?",
    "text": "What Are Deep Learning Frameworks?\n\nDeep learning frameworks reduce boilerplate code, handle tensor operations efficiently, and make it easier to prototype and iterate on new architectures.\nSoftware libraries designed to streamline the creation, training, and deployment of neural networks.\n\nProvide pre-built functions, automatic differentiation, and GPU/TPU support.\n\nNecessity: They allow researchers and developers to focus on model design rather than low-level implementation details."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#pytorch-and-tensor-flow",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#pytorch-and-tensor-flow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PyTorch and Tensor Flow",
    "text": "PyTorch and Tensor Flow\n\n\n\nWhat is PyTorch?\n\nDeveloped primarily by Facebook (Meta) and released on September 2016.\nEmphasizes a dynamic computation graph (eager execution).\nHighly “Pythonic”: feels natural for Python developers.\nStrong community presence in academia and research.\n\n\nWhy is PyTorch Necessary?\n\nEase of Use & Debugging\n\nEvaluate expressions immediately without building a separate graph.\n\nMore intuitive for experimenting with complex, innovative models.\n\nResearch Focus\n\nQuickly prototype new ideas and iterate.\n\nActive Ecosystem\n\nLibraries like torchvision, torchaudio, and others for specialized tasks.\n\n\n\n\nHow to begin\n\nhttps://pytorch.org/tutorials/beginner/basics/intro.html.\nThere is also a YouTube Series (PyTorch Beginner Series) also here (Introduction to PyTorch)\n\n\n\n\nWhat is TensorFlow?\n\nDeveloped primarily by Google and released in November 2015.\nHistorically used a static graph approach (with an “eager mode” added later).\nComes with extensive tools for deployment (mobile, web, and production).\nLarge ecosystem with well-integrated components (e.g., TensorBoard, TFX, TensorFlow Lite).\n\n\n\nWhy is TensorFlow Necessary?\n\nProduction-Ready\n\nStrong support for model serving at scale in enterprise environments.\n\nComprehensive Ecosystem\n\nVisualization (TensorBoard), data processing (TFX), and model deployment pipelines.\n\n\nCross-Platform & Hardware Support\n\nEasily deploy models to cloud infrastructures, mobile devices, and specialized hardware (TPUs).\n\n\n\n\nHow to begin\n\nhttps://www.tensorflow.org/tutorials. There is also a Quick Start!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#key-differences",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#key-differences",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Key Differences",
    "text": "Key Differences\n\n\n\n\n\n\n\n\nAspect\nPyTorch\nTensorFlow\n\n\n\n\nComputation Graph\nDynamic graph (eager execution by default).\nHistorically static graph with a build-and-execute phase (now supports eager execution).\n\n\nDebugging & Development Style\nMore straightforward for Python developers, immediate error feedback.\nCan be trickier to debug in graph mode; eager mode helps but is relatively newer.\n\n\nDeployment & Production\nTorchServe and growing enterprise support, but historically overshadowed by TensorFlow’s tools.\nTensorFlow Serving, TensorFlow Lite, and easy Google Cloud integration.\n\n\n\n\nWhile the fundamental math and building blocks are similar, the biggest difference typically lies in how you prototype, debug, and deploy models."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#similarities",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#similarities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Similarities",
    "text": "Similarities\n\n\n\n\n\n\n\nSimilarity\nDescription\n\n\n\n\nWide Range of Neural Network Layers\nConvolutional, Recurrent, Transformers, etc. Both frameworks maintain robust libraries for standard and advanced layers.\n\n\nAuto-Differentiation\nNo need to manually compute gradients; backpropagation is handled automatically.\n\n\nGPU Acceleration\nBoth leverage CUDA (NVIDIA GPUs) or other backends to speed up training.\n\n\nRich Communities\nAbundant tutorials, example code, pretrained models, and Q&A forums.\n\n\n\n\nDespite differing philosophies, PyTorch and TensorFlow share many core functionalities and have large, supportive user communities."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Advantages and Disadvantages",
    "text": "Comparison of Advantages and Disadvantages\n\n\n\n\n\n\n\n\n\n\nPyTorch\nTensorFlow\n\n\n\n\nAdvantages\n- Intuitive, Pythonic Syntax: Feels like standard Python, reducing friction for experimentation  - Dynamic Graph Execution: Simplifies debugging and model design  - Research & Academia Favorite: widely used in cutting-edge papers\n- Static Graph Optimization: Graph-based execution can be highly optimized for speed and memory usage  - Extensive Production Ecosystem: Includes TensorFlow Serving, TensorFlow Lite, TFX for data pipelines  - Large Corporate Adoption: Backed by Google, widely used in enterprise settings\n\n\nDisadvantages\n- Deployment Maturity: Production tooling and ecosystem are improving but still behind TensorFlow  - Smaller Enterprise Adoption: Historically overshadowed by TensorFlow’s widespread adoption\n- Learning Curve: The graph-based approach can be challenging for newcomers  - Historically Less Intuitive: Older APIs and tutorials can be confusing, though Eager Mode improves usability"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#recommendations",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#recommendations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommendations",
    "text": "Recommendations\n\n\nChoose PyTorch if:\n\nYour focus is on rapid experimentation and academic research\nYou prioritize a Pythonic workflow and easy debugging\nYou prefer a dynamic graph approach (about it).\nYou are working on cutting-edge models with high flexibility\nYou value seamless interaction with Python libraries\n\n\nChoose TensorFlow if:\n\nYou need robust production and deployment pipelines\nYou plan to integrate with Google Cloud services\nYou require support for mobile/edge devices (e.g., TensorFlow Lite)\nYou benefit from static graph optimization for performance\nYou want an end-to-end ecosystem (TFX, TensorBoard, Serving)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#tensors-in-pytorch",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#tensors-in-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tensors in PyTorch",
    "text": "Tensors in PyTorch"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#datasets-dataloaders",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#datasets-dataloaders",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#datasets-dataloaders-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#datasets-dataloaders-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders\n\n\n\nThe code below extracts a single image‑tensor from the training_data used in the tutorial (you can use test_data the same way), prints its basic properties, and visualizes it.\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Choose the index of the image you wish to inspect\nidx = 0  # e.g., the first image; change as desired\n\n# Fetch the sample\nimage_tensor, label = training_data[idx]   # image_tensor is a 1×28×28 tensor\n\n# Inspect the raw tensor values\nprint(\"Shape :\", image_tensor.shape)  # torch.Size([1, 28, 28])\nprint(\"Label :\", label) # integer class id\nprint(\"Tensor (first 5 rows):\\n\", image_tensor[0, :5, :])\n\n# Visualize the image\nplt.imshow(image_tensor.squeeze(), cmap=\"gray\")\nplt.title(f\"Fashion‑MNIST class{label}\")\nplt.axis(\"off\")\nplt.show()\n\n\n\nHow it works\n\nIndex selection – set idx to any integer in range(len(training_data)).\n\nDataset access – indexing the dataset returns (image, label) with the transform already applied (here, ToTensor() scales to [0,1]).\n\nInspection – the printed tensor slice lets you verify pixel values, and plt.imshow renders the sample for visual confirmation.\n\n\nTo see a different image you just need to adjust the index."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#transforms",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#transforms",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Transforms",
    "text": "Transforms"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nWhat Are We Doing?\nWe are defining a neural network class using PyTorch. This network is designed to work with images, specifically 28×28 grayscale images like those from the FashionMNIST dataset. The network will output 10 values, one for each digit from 0 to 9.\nStep-by-Step Breakdown\n\nclass NeuralNetwork(nn.Module):\n\nWe create a new neural network class called NeuralNetwork. It inherits from PyTorch’s nn.Module, which is the base class for all neural network models.\n\ndef __init__(self): and super().__init__()\n\n__init__ is the constructor. It’s run when we create the model.\nsuper().__init__() tells Python to also run the initialization code from the parent class (nn.Module). This is required for PyTorch to keep track of everything inside the model.\n\nself.flatten = nn.Flatten():\n\nchanges the input from a 2D image (28×28) into a 1D vector (784 values), which is easier for linear layers to handle."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-2",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\nHere we build the main body of the neural network.\nself.linear_relu_stack = nn.Sequential(\n    nn.Linear(28*28, 512),\n    nn.ReLU(),\n    nn.Linear(512, 512),\n    nn.ReLU(),\n    nn.Linear(512, 10),\n)\nIn most contexts when we say “how many layers?” we refer to the learnable ones. So this network has three fully‑connected (Linear) layers, with ReLU activations in between.\n\nYou can think of the linear layer as a filter that projects the image into a new space with 512 dimensions. These new values are not pixels anymore, but rather abstract features learned by the network."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-3",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nFirst layer nn.Linear(28*28, 512): takes the 784 values from the image and transforms them into 512 values. A Linear(784, 512) layer performs:\n\nA matrix multiplication between the input vector (length 784) and a weight matrix of size [784 × 512], followed by adding a bias vector of length 512.\nMathematically: \\[\n\\text{output} = x \\cdot W + b\n\\]\nx is the input vector: shape [784]\nW is the weight matrix: shape [784 × 512]\nb is the bias vector: shape [512]\nThe result (output) is a new vector of shape [512]\n\n\n\nEach of the 512 output values is a linear combination of all 784 pixel values in the input image. By default, PyTorch initializes weights using Kaiming Uniform Initialization (a variant of He initialization), which works well with ReLU activation functions."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-4",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nnn.ReLU(): applies the ReLU activation function, which keeps positive numbers and turns negative numbers into zero. This adds non-linearity to the model.\nSecond layernn.Linear(512, 512): takes those 512 values and again outputs 512 values. This is a hidden layer, helping the model learn more complex patterns.\nnn.ReLU(): Another non-linear transformation.\nThird (Final) layer:nn.Linear(512, 10): takes the 512 values and produces 10 output values.\n\nThese are called logits, and each one corresponds to a digit class (0 to 9)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-5",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nforward(self, x): This is the forward pass, the function that runs when we send data through the model.\nStep-by-step:\n\n\nx = self.flatten(x): Convert the 28×28 image into a 1D tensor with 784 values.\nlogits = self.linear_relu_stack(x): Pass the input through the series of layers.\nreturn logits: Output the final predictions (raw scores for each class).\n\n\nIn summary this neural network:\n\nTakes an image (28×28) as input,\nFlattens it into a vector,\nPasses it through two fully connected layers with ReLU,\nOutputs a vector of size 10 (one for each digit)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-6",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\ntorch.rand(1, 28, 28, device=device): Creates a random image with shape [1, 28, 28]\n\n1 is the batch size (just one image)\n28×28 is the image dimension\ndevice=device ensures the tensor goes to CPU or GPU (wherever the model is)\n\n\n\n\n# To see tensor:\nprint(X)\n\n\n\nLet’s say the tensor shown is:\nX = torch.tensor([[\n    [0.1177, 0.2669, 0.6367, 0.6148, 0.3085, ...],  # row 0\n    [0.8672, 0.3645, 0.4822, 0.9566, 0.8999, ...],  # row 1\n    ...\n]])\n\n\nThis is a 3D tensor of shape [1, 28, 28]:\n\nThe first dimension 1 is the batch size,\nThe next two are height and width of the image.\n\nThe full index of 0.2669 in the 3D tensor is: X[0, 0, 1].\n\n0 → first (and only) image in the batch\n0 → first row of the image\n1 → second column in that row"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-7",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#build-the-neural-network-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\nlogits = model(X): This calls the model with input X.\n\nBehind the scenes, it runs model.forward(X)\nOutput: a vector of 10 values (called logits), one for each class (digits 0 through 9)\n\nNote: We do not call model.forward() directly — PyTorch manages hooks and gradients when we use model(X)\npred_probab = nn.Softmax(dim=1)(logits): Applies softmax to the raw output logits\n\nSoftmax turns logits into probabilities (values between 0 and 1 that sum to 1)\ndim=1 means we apply softmax across the 10 output class values (not across the batch)\n\ny_pred = pred_probab.argmax(1): Picks the index of the largest probability, i.e., the predicted class\n\nargmax(1) returns the class with the highest probability from each row (here we have just one row)\n\nprint(f\"Predicted class: {y_pred}\"): Prints the predicted digit class (0 through 9)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Automatic Differentiation with torch.autograd",
    "text": "Automatic Differentiation with torch.autograd"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#optimizing-model-parameters",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#optimizing-model-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Optimizing Model Parameters",
    "text": "Optimizing Model Parameters"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#save-and-load-the-model",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#save-and-load-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Save and Load the Model",
    "text": "Save and Load the Model"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#introduction-to-pytorch---youtube-series",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#introduction-to-pytorch---youtube-series",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Introduction to PyTorch - YouTube Series",
    "text": "Introduction to PyTorch - YouTube Series\n\n\n\nPro tip: Use Colab with a GPU runtime to speed up operations Runtime &gt; Change runtime type &gt; GPU"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#neural-networks---video",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#neural-networks---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Neural Networks - Video",
    "text": "Neural Networks - Video\n\n\n\n\n\n\nBut what is a neural network?"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\n\nNetwork Diagram of Single Layer Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Introduction and Layers Overview",
    "text": "Single Layer Neural Network: Introduction and Layers Overview\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nNeural networks are often displayed using network diagrams, as shown in the figure.\n\nInput Layer (Orange Circles):\n\n\\(X_1, X_2, X_3, X_4\\)\nThese are observed variables from the dataset.\n\nHidden Layer (Blue Circles):\n\n\\(A_1, A_2, A_3, A_4, A_5\\)\nThese are transformations (activations) computed from the inputs.\n\nOutput Layer (Pink Circle):\n\n\\(f(X) \\to Y\\)\n\\(Y\\) is also observed, e.g., a label or continuous response."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Observed vs. Latent",
    "text": "Single Layer Neural Network: Observed vs. Latent\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nWhere is the observed data?\n\n\\(X_j\\) are observed (the input features).\n\\(Y\\) is observed (the response or label).\nThe hidden units (\\(A_k\\)) are not observed; they’re learned transformations."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Hidden Layer as Transformations",
    "text": "Single Layer Neural Network: Hidden Layer as Transformations\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nIn the hidden layer, each activation \\(A_k\\) is computed as:\n\\[\nA_k = g\\Bigl(w_{k0} + \\sum_{j=1}^4 w_{kj} X_j\\Bigr),\n\\]\n\nIn the formula, these \\(h_k(X)\\) are the same as the activations \\(A_k\\).\n\\(h_k(X)\\) = \\(g(w_{k0} + \\sum_{j=1}^p w_{kj} X_j)\\).\n\\(g(\\cdot)\\) is a nonlinear function (e.g., ReLU, sigmoid, tanh).\n\\(w_{kj}\\) are the weights learned during training.\nEach hidden unit has a different set of weights, hence different transformations."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-training-the-network",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-training-the-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Training the Network",
    "text": "Single Layer Neural Network: Training the Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\n\nThe network learns all weights \\(w_{kj}, w_{k0}, \\beta_k, \\beta_0\\) during training.\nObjective: predict \\(Y\\) from \\(X\\) accurately.\nKey insight: Hidden layer learns useful transformations on the fly to help approximate the true function mapping \\(X\\) to \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-details",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#single-layer-neural-network-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Details",
    "text": "Single Layer Neural Network: Details\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A_k = h_k(X) = g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j)\\) are called the activations in the hidden layer. We can think of it as a non-linear tranformation of a linear function.\n\\(g(z)\\) is called the activation function. Two popular activation functions are: the sigmoid and rectified linear (ReLU).\nActivation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.\nSo the activations are like derived features — nonlinear transformations of linear combinations of the features.\nThe model is fit by minimizing \\(\\sum_{i=1}^{n} (y_i - f(x_i))^2\\) (e.g., for regression)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#nn-example-mnist-digits",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#nn-example-mnist-digits",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NN Example: MNIST Digits",
    "text": "NN Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights).\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#gradient-descent---video",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#gradient-descent---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent - Video",
    "text": "Gradient Descent - Video\n\n\n\n\n\n\nGradient descent, how neural networks learn"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#backpropagation-intuition---video",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#backpropagation-intuition---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Intuition - Video",
    "text": "Backpropagation Intuition - Video\n\n\n\n\n\n\nBackpropagation, intuitively"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#backpropagation-calculus---video",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#backpropagation-calculus---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Calculus - Video",
    "text": "Backpropagation Calculus - Video\n\n\n\n\n\n\nBackpropagation calculus"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#fitting-neural-networks-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#fitting-neural-networks-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Neural Networks",
    "text": "Fitting Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\min_{\\{w_k\\}_{1}^K, \\beta} \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2, \\quad \\text{where}\n\\]\n\\[\nf(x_i) = \\beta_0 + \\sum_{k=1}^K \\beta_k g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}\\right).\n\\]\nThis problem is difficult because the objective is non-convex.\nDespite this, effective algorithms have evolved that can optimize complex neural network problems efficiently."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#non-convex-functions-and-gradient-descent",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#non-convex-functions-and-gradient-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non Convex Functions and Gradient Descent",
    "text": "Non Convex Functions and Gradient Descent\nLet \\(R(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - f_\\theta(x_i))^2\\) with \\(\\theta = (\\{w_k\\}_{1}^K, \\beta)\\).\n\n\nStart with a guess \\(\\theta^0\\) for all the parameters in \\(\\theta\\), and set \\(t = 0\\).\nIterate until the objective \\(R(\\theta)\\) fails to decrease:\n\nFind a vector \\(\\delta\\) that reflects a small change in \\(\\theta\\), such that \\(\\theta^{t+1} = \\theta^t + \\delta\\) reduces the objective; i.e., \\(R(\\theta^{t+1}) &lt; R(\\theta^t)\\).\nSet \\(t \\gets t + 1\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#gradient-descent-continued",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#gradient-descent-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent Continued",
    "text": "Gradient Descent Continued\n\nIn this simple example, we reached the global minimum.\nIf we had started a little to the left of \\(\\theta^0\\), we would have gone in the other direction and ended up in a local minimum.\nAlthough \\(\\theta\\) is multi-dimensional, we have depicted the process as one-dimensional. It is much harder to identify whether one is in a local minimum in high dimensions.\nHow to find a direction \\(\\delta\\) that points downhill? We compute the gradient vector: \\[\n\\nabla R(\\theta^t) = \\frac{\\partial R(\\theta)}{\\partial \\theta} \\bigg|_{\\theta = \\theta^t}\n\\]\ni.e., the vector of partial derivatives at the current guess \\(\\theta^t\\).\nThe gradient points uphill, so our update is \\(\\delta = - \\rho \\nabla R(\\theta^t)\\) or \\[\n\\theta^{t+1} \\gets \\theta^t - \\rho \\nabla R(\\theta^t),\n\\] where \\(\\rho\\) is the learning rate (typically small, e.g., \\(\\rho = 0.001\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#gradients-and-backpropagation",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#gradients-and-backpropagation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradients and Backpropagation",
    "text": "Gradients and Backpropagation\n\n\\[\nR(\\theta) = \\sum_{i=1}^n R_i(\\theta) \\text{ is a sum, so gradient is sum of gradients.}\n\\]\n\\[\nR_i(\\theta) = \\frac{1}{2}(y_i - f_\\theta(x_i))^2 = \\frac{1}{2} \\left( y_i - \\beta_0 - \\sum_{k=1}^K \\beta_k g\\left( w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij} \\right) \\right)^2\n\\]\nFor ease of notation, let\n\\[\nz_{ik} = w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}.\n\\]\nBackpropagation uses the chain rule for differentiation:\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial \\beta_k} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\beta_k}\n= -(y_i - f_\\theta(x_i)) \\cdot g(z_{ik}).\n\\]\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial w_{kj}} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial g(z_{ik})} \\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}} \\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}}\n= -(y_i - f_\\theta(x_i)) \\cdot \\beta_k \\cdot g'(z_{ik}) \\cdot x_{ij}.\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#tricks-of-the-trade",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#tricks-of-the-trade",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tricks of the Trade",
    "text": "Tricks of the Trade\n\nSlow learning. Gradient descent is slow, and a small learning rate \\(\\rho\\) slows it even further. With early stopping, this is a form of regularization.\nStochastic gradient descent. Rather than compute the gradient using all the data, use a small minibatch drawn at random at each step. E.g. for MNIST data, with \\(n = 60K\\), we use minibatches of 128 observations.\nAn epoch is a count of iterations and amounts to the number of minibatch updates such that \\(n\\) samples in total have been processed; i.e. \\(60K/128 \\approx 469\\) for MNIST.\nRegularization. Ridge and lasso regularization can be used to shrink the weights at each layer. Two other popular forms of regularization are dropout and augmentation, discussed next."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#dropout-learning",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#dropout-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dropout Learning",
    "text": "Dropout Learning\n\n\nAt each Stochastic Gradient Descent (SGD) update, randomly remove units with probability \\(\\phi\\), and scale up the weights of those retained by \\(1/(1-\\phi)\\) to compensate.\nIn simple scenarios like linear regression, a version of this process can be shown to be equivalent to ridge regularization.\nAs in ridge, the other units stand in for those temporarily removed, and their weights are drawn closer together.\nSimilar to randomly omitting variables when growing trees in random forests."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#ridge-and-data-augmentation",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#ridge-and-data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge and Data Augmentation",
    "text": "Ridge and Data Augmentation\n\n\nMake many copies of each \\((x_i, y_i)\\) and add a small amount of Gaussian noise to the \\(x_i\\) — a little cloud around each observation — but leave the copies of \\(y_i\\) alone!\nThis makes the fit robust to small perturbations in \\(x_i\\), and is equivalent to ridge regularization in an OLS setting."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#data-augmentation-on-the-fly",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#data-augmentation-on-the-fly",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation on the Fly",
    "text": "Data Augmentation on the Fly\n\n\nData augmentation is especially effective with SGD, here demonstrated for a CNN and image classification.\nNatural transformations are made of each training image when it is sampled by SGD, thus ultimately making a cloud of images around each original training image.\nThe label is left unchanged — in each case still tiger.\nImproves performance of CNN and is similar to ridge."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#double-descent",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#double-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Double Descent",
    "text": "Double Descent\n\nWith neural networks, it seems better to have too many hidden units than too few.\nLikewise more hidden layers better than few.\nRunning stochastic gradient descent till zero training error often gives good out-of-sample error.\nIncreasing the number of units or layers and again training till zero error sometimes gives even better out-of-sample error.\nWhat happened to overfitting and the usual bias-variance trade-off?\n\nBelkin, Hsu, Ma, and Mandal (arXiv 2018) Reconciling Modern Machine Learning and the Bias-Variance Trade-off."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#the-double-descent-error-curve",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#the-double-descent-error-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Double-Descent Error Curve",
    "text": "The Double-Descent Error Curve\n\n\nWhen \\(d \\leq 20\\), model is OLS, and we see usual bias-variance trade-off.\nWhen \\(d &gt; 20\\), we revert to minimum-norm. As \\(d\\) increases above 20, \\(\\sum_{j=1}^d \\hat{\\beta}_j^2\\) decreases since it is easier to achieve zero error, and hence less wiggly solutions."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#less-wiggly-solutions",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#less-wiggly-solutions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Less Wiggly Solutions",
    "text": "Less Wiggly Solutions\n\n\nTo achieve a zero-residual solution with \\(d = 20\\) is a real stretch!\nEasier for larger \\(d\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#some-facts",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#some-facts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Facts",
    "text": "Some Facts\n\nIn a wide linear model (\\(p \\gg n\\)) fit by least squares, SGD with a small step size leads to a minimum norm zero-residual solution.\nStochastic gradient flow — i.e. the entire path of SGD solutions — is somewhat similar to ridge path.\nBy analogy, deep and wide neural networks fit by SGD down to zero training error often give good solutions that generalize well.\nIn particular cases with high signal-to-noise ratio — e.g. image recognition — are less prone to overfitting; the zero-error solution is mostly signal!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#cnn-introduction",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#cnn-introduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN: Introduction",
    "text": "CNN: Introduction\n\nNeural networks rebounded around 2010 with big successes in image classification.\nAround that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#the-cifar100-database",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#the-cifar100-database",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The CIFAR100 Database",
    "text": "The CIFAR100 Database\n\n\n\nThe figure shows 75 images drawn from the CIFAR100 database.\nThis database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale).\nEach image has a resolution of 32 × 32 pixels, with three eight-bit numbers per pixel representing red, green, and blue. The numbers for each image are organized in a three-dimensional array called a feature map.\nThe first two axes are spatial (both 32-dimensional), and the third is the channel axis, representing the three (blue, green or red) colors.\nThere is a designated training set of 50,000 images, and a test set of 10,000."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#the-convolutional-network-hierarchy",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#the-convolutional-network-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Convolutional Network Hierarchy",
    "text": "The Convolutional Network Hierarchy\n\n\n\nCNNs mimic, to some degree, how humans classify images, by recognizing specific features or patterns anywhere in the image that distinguish each particular object class.\nThe network first identifies low-level features in the input image, such as small edges or patches of color.\nThese low-level features are then combined to form higher-level features, such as parts of ears or eyes. Eventually, the presence or absence of these higher-level features contributes to the probability of any given output class.\nThis hierarchical construction is achieved by combining two specialized types of hidden layers: convolution layers and pooling layers:\nConvolution layers search for instances of small patterns in the image.\nPooling layers downsample these results to select a prominent subset.\nTo achieve state-of-the-art results, contemporary neural network architectures often use many convolution and pooling layers."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#convolution-layer",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#convolution-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Layer",
    "text": "Convolution Layer\n\n\nA convolution layer is made up of a large number of convolution filters, each of which is a template that determines whether a particular local feature is present in an image.\nA convolution filter relies on a very simple operation, called a convolution, which basically amounts to repeatedly multiplying matrix elements and then adding the results. \\[\n\\text{Input Image} =\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l\n\\end{bmatrix}\n\\quad \\text{Convolution Filter} =\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{bmatrix}.\n\\]\nWhen we convolve the image with the filter, we get the result: \\[\n\\text{Convolved Image} =\n\\begin{bmatrix}\na\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}.\n\\]\nthe convolution filter is applied to every 2 × 2 submatrix of the original image in order to obtain the convolved image.\nIf a 2 × 2 submatrix of the original image resembles the convolution filter, then it will have a large value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that resemble the convolution filter.\nThe filter is itself an image and represents a small shape, edge, etc.\nThe filters are learned during training."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#convolution-example",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#convolution-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Example",
    "text": "Convolution Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe idea of convolution with a filter is to find common patterns that occur in different parts of the image.\nThe two filters shown here highlight vertical and horizontal stripes.\nThe result of the convolution is a new feature map.\nSince images have three color channels, the filter does as well: one filter per channel, and dot-products are summed.\nThe weights in the filters are learned by the network."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#pooling-layer",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#pooling-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pooling Layer",
    "text": "Pooling Layer\nA pooling layer provides a way to condense a large image into a smaller summary image.\n\\[\n\\text{Max pool}\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]\n\nEach non-overlapping \\(2 \\times 2\\) block is replaced by its maximum.\nThis sharpens the feature identification.\nAllows for locational invariance.\nReduces the dimension by a factor of 4 — i.e., factor of 2 in each dimension."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#architecture-of-a-cnn",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#architecture-of-a-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Architecture of a CNN",
    "text": "Architecture of a CNN\n\n\nMany convolve + pool layers.\nFilters are typically small, e.g., each channel \\(3 \\times 3\\).\nEach filter creates a new channel in the convolution layer.\nAs pooling reduces size, the number of filters/channels is typically increased.\nNumber of layers can be very large.\nE.g., resnet50 trained on imagenet 1000-class image database has 50 layers!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#data-augmentation",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\n\n\nAn additional important trick used with image modeling is data augmentation.\nEssentially, each training image is replicated many times, with each replicate randomly distorted in a natural way such that human recognition is unaffected.\nTypical distortions are zoom, horizontal and vertical shift, shear, small rotations, and in this case horizontal flips.\nAt face value this is a way of increasing the training set considerably with somewhat different examples, and thus protects against overfitting.\nIn fact we can see this as a form of regularization: we build a cloud of images around each original image, all with the same label."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN Example: Pretrained Networks to Classify Images",
    "text": "CNN Example: Pretrained Networks to Classify Images\n\nHere we use the 50-layer resnet50 network trained on the 1000-class imagenet corpus to classify some photographs.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#document-classification-imdb-movie-reviews",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#document-classification-imdb-movie-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification: IMDB Movie Reviews",
    "text": "Document Classification: IMDB Movie Reviews\nThe IMDB corpus consists of user-supplied movie ratings for a large collection of movies. Each has been labeled for sentiment as positive or negative. Here is the beginning of a negative review:\n\nThis has to be one of the worst films of the 1990s. When my friends & I were watching this film (being the target audience it was aimed at) we just sat & watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn …\n\nWe have labeled training and test sets, each consisting of 25,000 reviews, and each balanced with regard to sentiment.\nGoal: We want to build a classifier to predict the sentiment of a review."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#featurization-bag-of-words",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#featurization-bag-of-words",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Featurization: Bag-of-Words",
    "text": "Featurization: Bag-of-Words\nDocuments have different lengths and consist of sequences of words. How do we create features \\(X\\) to characterize a document?\n\nFrom a dictionary, identify the 10K most frequently occurring words.\nCreate a binary vector of length \\(p = 10K\\) for each document, and score a 1 in every position that the corresponding word occurred.\nWith \\(n\\) documents, we now have an \\(n \\times p\\) sparse feature matrix \\(\\mathbf{X}\\).\nWe compare a lasso logistic regression model to a two-hidden-layer neural network on the next slide. (No convolutions here!)\nBag-of-words are unigrams. We can instead use bigrams (occurrences of adjacent word pairs) and, in general, m-grams."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews",
    "text": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews\n\n\n\nSimpler lasso logistic regression model works as well as neural network in this case.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#recurrent-neural-networks---rnn-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#recurrent-neural-networks---rnn-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks - RNN",
    "text": "Recurrent Neural Networks - RNN\n\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#simple-recurrent-neural-network-architecture",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#simple-recurrent-neural-network-architecture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Recurrent Neural Network Architecture",
    "text": "Simple Recurrent Neural Network Architecture\n\n\nThe hidden layer is a sequence of vectors \\(A_\\ell\\), receiving as input \\(X_\\ell\\) as well as \\(A_{\\ell-1}\\). \\(A_\\ell\\) produces an output \\(O_\\ell\\).\nThe same weights \\(\\mathbf{W}\\), \\(\\mathbf{U}\\), and \\(\\mathbf{B}\\) are used at each step in the sequence — hence the term recurrent.\nThe \\(A_\\ell\\) sequence represents an evolving model for the response that is updated as each element \\(X_\\ell\\) is processed."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-in-detail",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN in Detail",
    "text": "RNN in Detail\n\nSuppose \\(X_\\ell = (X_{\\ell1}, X_{\\ell2}, \\ldots, X_{\\ell p})\\) has \\(p\\) components, and \\(A_\\ell = (A_{\\ell1}, A_{\\ell2}, \\ldots, A_{\\ell K})\\) has \\(K\\) components. Then the computation at the \\(k\\)-th components of hidden unit \\(A_\\ell\\) is:\n\\[\nA_{\\ell k} = g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_{\\ell j} + \\sum_{s=1}^{K} u_{ks} A_{\\ell-1,s}\\right)\n\\]\n\\[\nO_\\ell = \\beta_0 + \\sum_{k=1}^{K} \\beta_k A_{\\ell k}\n\\]\nOften we are concerned only with the prediction \\(O_L\\) at the last unit. For squared error loss, and \\(n\\) sequence/response pairs, we would minimize:\n\\[\n\\sum_{i=1}^{n} (y_i - o_{iL})^2 = \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} x_{iL,j} + \\sum_{s=1}^{K} u_{ks} a_{i,L-1,s}\\right)\\right)\\right)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-for-document-classification-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-for-document-classification-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN for Document Classification",
    "text": "RNN for Document Classification\n\nThe document feature is a sequence of words \\(\\{\\mathcal{W}_\\ell\\}_{1}^{L}\\). We typically truncate/pad the documents to the same number \\(L\\) of words (we use \\(L = 500\\)).\nEach word \\(\\mathcal{W}_\\ell\\) is represented as a one-hot encoded binary vector \\(X_\\ell\\) (dummy variable) of length \\(10K\\), with all zeros and a single one in the position for that word in the dictionary.\nThis results in an extremely sparse feature representation and would not work well.\nInstead, we use a lower-dimensional pretrained word embedding matrix \\(\\mathbf{E}\\) (\\(m \\times 10K\\), next slide).\nThis reduces the binary feature vector of length \\(10K\\) to a real feature vector of dimension \\(m \\ll 10K\\) (e.g., \\(m\\) in the low hundreds)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Word Embedding - RNN Example: IMDB Reviews",
    "text": "Word Embedding - RNN Example: IMDB Reviews\n\nReview:\n\nthis is one of the best films actually the best I have ever seen the film starts one fall day…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings are pretrained on very large corpora of documents, using methods similar to principal components. word2vec and GloVe are popular.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-time-series-forecasting",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-time-series-forecasting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN: Time Series Forecasting",
    "text": "RNN: Time Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew-York Stock Exchange Data\nThree daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#autocorrelation",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#autocorrelation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nThe autocorrelation at lag \\(\\ell\\) is the correlation of all pairs \\((v_t, v_{t-\\ell})\\) that are \\(\\ell\\) trading days apart.\nThese sizable correlations give us confidence that past values will be helpful in predicting the future.\nThis is a curious prediction problem: the response \\(v_t\\) is also a feature \\(v_{t-\\ell}\\)!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-forecaster",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Forecaster",
    "text": "RNN Forecaster\nWe only have one series of data! How do we set up for an RNN?\nWe extract many short mini-series of input sequences \\(\\mathbf{X} = \\{ X_1, X_2, \\ldots, X_L \\}\\) with a predefined length \\(L\\) known as the lag:\n\\[\nX_1 = \\begin{pmatrix}\nv_{t-L} \\\\\nr_{t-L} \\\\\nz_{t-L}\n\\end{pmatrix}, \\quad\nX_2 = \\begin{pmatrix}\nv_{t-L+1} \\\\\nr_{t-L+1} \\\\\nz_{t-L+1}\n\\end{pmatrix}, \\quad\n\\cdots, \\quad\nX_L = \\begin{pmatrix}\nv_{t-1} \\\\\nr_{t-1} \\\\\nz_{t-1}\n\\end{pmatrix}, \\quad \\text{and} \\quad Y = v_t.\n\\]\nSince \\(T = 6,051\\), with \\(L = 5\\), we can create 6,046 such \\((X, Y)\\) pairs.\nWe use the first 4,281 as training data, and the following 1,770 as test data. We fit an RNN with 12 hidden units per lag step (i.e., per \\(A_\\ell\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#rnn-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Results for NYSE Data",
    "text": "RNN Results for NYSE Data\n\nThe figure shows predictions and truth for the test period.\n\\[\nR^2 = 0.42 \\text{ for RNN}\n\\]\n\\(R^2 = 0.18\\) for the naive approach — uses yesterday’s value of Log trading volume to predict that of today.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#autoregression-forecaster",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#autoregression-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Forecaster",
    "text": "Autoregression Forecaster\nThe RNN forecaster is similar in structure to a traditional autoregression procedure.\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\nv_{L+1} \\\\\nv_{L+2} \\\\\nv_{L+3} \\\\\n\\vdots \\\\\nv_T\n\\end{bmatrix}, \\quad\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & v_L & v_{L-1} & \\cdots & v_1 \\\\\n1 & v_{L+1} & v_L & \\cdots & v_2 \\\\\n1 & v_{L+2} & v_{L+1} & \\cdots & v_3 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & v_{T-1} & v_{T-2} & \\cdots & v_{T-L}\n\\end{bmatrix}.\n\\]\nFit an OLS regression of \\(\\mathbf{y}\\) on \\(\\mathbf{M}\\), giving:\n\\[\n\\hat{v}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 v_{t-1} + \\hat{\\beta}_2 v_{t-2} + \\cdots + \\hat{\\beta}_L v_{t-L}.\n\\]\nKnown as an order-\\(L\\) autoregression model or \\(AR(L)\\).\nFor the NYSE data, we can include lagged versions of DJ_return and log_volatility in matrix \\(\\mathbf{M}\\), resulting in \\(3L + 1\\) columns."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#autoregression-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#autoregression-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Results for NYSE Data",
    "text": "Autoregression Results for NYSE Data\n\n\\(R^2 = 0.41 \\text{ for } AR(5) \\text{ model (16 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for RNN model (205 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for } AR(5) \\text{ model fit by neural network.}\\)\n\\(R^2 = 0.46 \\text{ for all models if we include } \\textbf{day_of_week} \\text{ of day being predicted.}\\)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#summary-of-rnns",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#summary-of-rnns",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary of RNNs",
    "text": "Summary of RNNs\n\nWe have presented the simplest of RNNs. Many more complex variations exist.\nOne variation treats the sequence as a one-dimensional image, and uses CNNs for fitting. For example, a sequence of words using an embedding representation can be viewed as an image, and the CNN convolves by sliding a convolutional filter along the sequence.\nCan have additional hidden layers, where each hidden layer is a sequence, and treats the previous hidden layer as an input sequence.\nCan have output also be a sequence, and input and output share the hidden units. So called seq2seq learning are used for language translation."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#when-to-use-deep-learning-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#when-to-use-deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\n\nCNNs have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.\nRNNs have had big wins in speech modeling, language translation, and forecasting.\n\n\nShould we always use deep learning models?\n\nOften the big successes occur when the signal to noise ratio is high — e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.\nFor noisier data, simpler models can often work better:\n\nOn the NYSE data, the AR(5) model is much simpler than an RNN, and performed as well.\nOn the IMDB review data, a linear model fit (e.g. with glmnet) did as well as the neural network, and better than the RNN."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\nAs the authors suggest, I also endorse the Occam’s razor principle — we prefer simpler models if they work as well. More interpretable!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#additional-material",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Additional Material",
    "text": "Additional Material\n\n3Blue1Brown: Neural Networks\nDeep Learning, by Ian Goodfellow and Yoshua Bengio and Aaron Courvill\nWelch Labs: Neural Networks Demystified\nWelch Labs: Learning To See\nDistill: A Gentle Introduction to Graph Neural Networks\nNeural Networks and Deep Learning, by Michael Nielsen\nCITS4012 Natural Language Processing\nDeep Learning with PyTorch Step-by-Step"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#summary",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nDeep Learning Renaissance\n\nNeural networks first rose to prominence in the 1980s, waned in the 1990s, then surged again around 2010.\nAdvances in computing (GPUs) and availability of massive labeled datasets propelled deep learning success.\n\nFrameworks (PyTorch vs. TensorFlow)\n\nPyTorch is known for its dynamic graph and Pythonic syntax; widely used in research.\nTensorFlow has an extensive production ecosystem, ideal for enterprise and deployment.\n\nEssential Concepts\n\nAutomatic differentiation, gradient descent, and backpropagation are at the core of training neural networks.\n\n\n\n\n\n\nCNNs and RNNs\n\nCNNs excel in image classification by learning local patterns via convolution and pooling layers.\n\nRNNs (and variants like LSTM, GRU) handle sequential data for tasks like language modeling and time-series forecasting.\n\nWhen to Use Deep Learning\n\nWorks best on large datasets with high signal-to-noise ratio (e.g., image, text).\n\nSimpler models often perform well on noisier tasks or smaller datasets.\n\nOver-parameterization can still generalize due to “double-descent” effects.\n\nPractical Tips\n\nUse regularization (dropout, data augmentation, weight decay) to mitigate overfitting.\n\nMonitor convergence with appropriate learning rates and consider mini-batch stochastic gradient descent."
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, time series, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 1007\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required):\n\nGoogle Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\nMicrosoft Copilot: is an AI-powered assistant designed to enhance productivity and streamline workflows across various applications and services. It utilizes large language models and is integrated within Microsoft 365 apps like Word, Excel, PowerPoint, Outlook, and Teams, providing real-time, context-aware assistance for tasks such as drafting documents, analyzing data, managing projects, and communicating more efficiently. Users can leverage Copilot to automate repetitive tasks, generate ideas, summarize information, and access data across their work environment and the web, all within a secure and privacy-conscious framework.\n\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance\n3%\n\n\nParticipation\n7%\n\n\nQuizzes\n10%\n\n\nHomework\n30%\n\n\nCourse Case Competition\n20%\n\n\nFinal Project\n30%\n\n\n\n\nAttendance\nAttend your classes. If you do not attend class, you will likely not succeed. The instructor will take attendance and keep the attendance record.\n\n\nParticipation\nParticipate in activities, and complete any participatory exercises. Almost every class the instructor you will be required to take participatory activities. Your participation grade will be based on this record.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nCourse Case Competition\nWwe will have a semester-long, team-based predictive analytics competition hosted on Kaggle. Students must work in teams and will be allowed up to five submissions per day. The Kaggle platform will automatically evaluate submissions and maintain a leaderboard throughout the competition period.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (30%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPeer Review (10%): Each group will review and provide constructive feedback on other teams’ posters. This process encourages engagement, enhances critical analysis skills, and promotes a culture of constructive critique.\nPoster Presentation at the Purdue Undergraduate Research Conference (20%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\nInstructor/TA Evaluation (20%): After the Undergraduate Research Conference your instructor and the TA will evaluate your final submission based on a rubric that will be shared.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nReadings ISLP\nMaterial*\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction.\nCh. 1; Ch. 2;\nslidespodcast**video**book lab\n- Video: Statistical Learning: 2.1 Introduction to Regression Models- Video: Statistical Learning: 2.2 Dimensionality and Structured Models- Video: Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff- Video: Statistical Learning: 2.4 Classification- Video: Statistical Learning: 2.Py Data Types, Arrays, and Basics - 2023- Video: Statistical Learning: 2.Py.3 Graphics - 2023- Video: Statistical Learning: 2.Py Indexing and Dataframes - 2023\n\n\nWeek 2\nLinear Regression\nCh. 3.\nslidespodcastvideonotebookbook lab\n- Video: Statistical Learning: 3.1 Simple linear regression- Video: Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals- Video: Statistical Learning: 3.3 Multiple Linear Regression- Video: Statistical Learning: 3.4 Some important questions- Video: Statistical Learning: 3.5 Extensions of the Linear Model- Video: Statistical Learning: 3.Py Linear Regression and statsmodels Package - 2023- Video: Statistical Learning: 3.Py Multiple Linear Regression Package - 2023- Video: Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023\n\n\nWeek 3\nClassification\nCh. 4\nslidespodcastvideonotebookbook lab\n- Video: Statistical Learning: 4.1 Introduction to Classification Problems- Video: Statistical Learning: 4.2 Logistic Regression- Video: Statistical Learning: 4.3 Multivariate Logistic Regression- Video: Statistical Learning: 4.4 Logistic Regression Case Control Sampling and Multiclass- Video: Statistical Learning: 4.5 Discriminant Analysis- Video: Statistical Learning: 4.6 Gaussian Discriminant Analysis (One Variable)- Video: Statistical Learning: 4.7 Gaussian Discriminant Analysis (Many Variables)- Video: Statistical Learning: 4.8 Generalized Linear Models- Video: Statistical Learning: 4.9 Quadratic Discriminant Analysis and Naive Bayes- Video: Statistical Learning: 4.Py Logistic Regression I 2023- Video: Statistical Learning: 4.Py Linear Discriminant Analysis (LDA) I 2023- Video: Statistical Learning: 4.Py K-Nearest Neighbors (KNN) I 2023\n\n\nWeek 4\nResampling Methods\nCh. 5\nslidespodcastvideobook lab\n- Video: Statistical Learning: 5.1 Cross Validation- Video: Statistical Learning: 5.2 K-fold Cross Validation- Video: Statistical Learning: 5.3 Cross Validation the wrong and right way- Video: Statistical Learning: 5.4 The Bootstrap- Video: Statistical Learning: 5.5 More on the Bootstrap- Video: Statistical Learning: 5.Py Cross-Validation I 2023- Video: Statistical Learning: 5.Py Bootstrap I 2023- Book Chapter: Modern Dive -Bootstrapping and Confidence Intervals\n\n\nWeek 5\nLinear Model Selection & Regularization\nCh. 6\nslidespodcastvideobook lab\n- Video: Statistical Learning: 6.1 Introduction and Best Subset Selection- Video: Statistical Learning: 6.2 Stepwise Selection- Video: Statistical Learning: 6.3 Backward stepwise selection- Video: Statistical Learning: 6.4 Estimating test error- Video: Statistical Learning: 6.5 Validation and cross validation- Video: Statistical Learning: 6.6 Shrinkage methods and ridge regression- Video: Statistical Learning: 6.7 The Lasso- Video: Statistical Learning: 6.8 Tuning parameter selection- Video: Statistical Learning: 6.9 Dimension Reduction Methods- Video: Statistical Learning: 6.10 Principal Components Regression and Partial Least Squares- Video: Statistical Learning: 6.Py Stepwise Regression I 2023- Video: Statistical Learning: 6.Py Ridge Regression and the Lasso I 2023\n\n\nWeek 6\nBeyond Linearity\nCh. 7\nslidespodcastvideobook lab\n- Video: Statistical Learning: 7.1 Polynomials and Step Functions- Video: Statistical Learning: 7.2 Piecewise Polynomials and Splines- Video: Statistical Learning: 7.3 Smoothing Splines- Video: Statistical Learning: 7.4 Generalized Additive Models and Local Regression- Video: Statistical Learning: 7.Py Polynomial Regressions and Step Functions I 2023- Video: Statistical Learning: 7.Py Splines I 2023- Video: Statistical Learning: 7.Py Generalized Additive Models (GAMs) I 2023\n\n\nWeek 7\nTree-Based Methods\nCh. 8\nslidespodcastvideobook lab\n- Video: Statistical Learning: 8.1 Tree based methods- Video: Statistical Learning: 8.2 More details on Trees- Video: Statistical Learning: 8.3 Classification Trees- Video: Statistical Learning: 8.4 Bagging- Video: Statistical Learning: 8.5 Boosting- Video: Statistical Learning: 8.6 Bayesian Additive Regression Trees- Video: Statistical Learning: 8.Py Tree-Based Methods I 2023\n\n\nWeek 8\nTime Series\nLecture Material\nnotebookdatapodcastvideo\n- Book: Forecasting: Principles and Practice, the Pythonic Way- Videos: Forecasting: Principles and Practice - with R- Videos: Forecasting: Principles and Practice - with R- Book: Time series analysis with Python- PyTorch Forecasting Library- sktime Library- prophet Library- statsmodels Library- scikit-learn Library: Cross Calidation of Time Series Data- prophet Library\n\n\nWeek 9\nFinal Project\n.\n.\n.\n\n\nWeek 10\nElements of Data Communication\n.\nslidespodcastvideo\nPoster TemplateConference RubricVideo: Creating a Professional Poster\n\n\nWeek 11\nFinal Project\n.\n.\n.\n\n\nWeek 12\nFinal Project\n.\n.\n.\n\n\nWeek 13\nFinal Project\n.\n.\n.\n\n\nWeek 14\nDeep Learning\nCh. 10\nslidespodcastvideobook labcourse lab\n- Video: Statistical Learning: 10.1 Introduction to Neural Networks- Video: Statistical Learning: 10.2 Convolutional Neural Networks- Video: Statistical Learning: 10.3 Document Classification- Video: Statistical Learning: 10.4 Recurrent Neural Networks- Video: Statistical Learning: 10.6 Fitting Neural Networks- Video: Statistical Learning: 10.7 Interpolation and Double Descent- Video: Statistical Learning: 10.Py Single Layer Model: Hitters Data I 2023- Video: Statistical Learning: 10.Py Multilayer Model: MNIST Digit Data I 2023- Video: Statistical Learning: 10.Py Convolutional Neural Network: CIFAR Image Data I 2023- Video: Statistical Learning: 10.Py Document Classification and Recurrent Neural Networks I 2023\n\n\nWeek 15\nDeep Learning\nCh. 10\n.\n.\n\n\nWeek 16\nSpecial Topic: LLM\n.\nslides\n.\n\n\n\n* The majority of the course slides and labs are based on the ISLP book, “An Introduction to Statistical Learning with Applications in Python” by James, G., Witten, D., Hastie, T., and Tibshirani, R., and have been adapted to suit the specific needs of our course. ** This material was generated with Google NotebookLM based on the slides I prepared for the Predictive Analytics course.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#overview",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nSix General Principles\n\nContext Matters\nVisualization Derives From Data\nLess is More: Separate Signal From Noise\nHierarchy Among Data\nBeauty Counts: Not All Data Are Equally Important\nTelling Your Story\n\n\n\n\nPosters"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#the-forest-and-the-trees-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#the-forest-and-the-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Forest and the Trees",
    "text": "The Forest and the Trees\n\n\n\n\nForest\n\n\n\nWe have explored many implementation details in recent days, focusing on individual aspects of each analysis.\n\nToday, we want to take a step back to think less about the detail and more about the process.\nAfter all, every data analysis has a purpose. How can we achieve it more effectively?"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#essential-elements-of-data-communication",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#essential-elements-of-data-communication",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Essential Elements of Data Communication",
    "text": "Essential Elements of Data Communication\nLet’s break down the data communication process into six general principles:\n\nContext Matters\nVisualization Derives From Data\nLess is More: Separate Signal From Noise\nHierarchy Among Data\nBeauty Counts: Not All Data Are Equally Important\nTelling Your Story"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#context",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#context",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Context",
    "text": "Context\nEvery analysis has a goal and an audience.\n\nIt’s important to separate data exploration from the final analysis. Don’t fall into the temptation of showing everything you did.\nAdapt the report to your audience. Decision-makers aren’t always interested in execution details.\nSo what? Keep a specific learning objective in mind. It will guide which information is relevant for your report.\n\n\nIsolated numbers don’t tell us much. To make evidence-based decisions, it’s necessary to establish an appropriate basis for comparison for the goal of your report."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#context-can-come-from-new-information",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#context-can-come-from-new-information",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Context Can Come from New Information…",
    "text": "Context Can Come from New Information…"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#or-reinforce-existing-information",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#or-reinforce-existing-information",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "…or Reinforce Existing Information",
    "text": "…or Reinforce Existing Information"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#choosing-the-chart",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#choosing-the-chart",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Chart",
    "text": "Choosing the Chart\n\nWhat type of data?\nHow many dimensions?\nMost reports are consumed in 2D media. Showing more than that can confuse the reader.\nBe careful with scales!"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#use-graphs-instead-of-tables",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#use-graphs-instead-of-tables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Use graphs instead of tables!",
    "text": "Use graphs instead of tables!\n\n\n\n\nTable vs Plot"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#use-graphs-instead-of-tables-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#use-graphs-instead-of-tables-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Use graphs instead of tables!",
    "text": "Use graphs instead of tables!\n\n\n\n\nTable vs Plot"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#scales-can-be-misleading",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#scales-can-be-misleading",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Scales Can Be Misleading",
    "text": "Scales Can Be Misleading\n\n\n\n\nScale Fail"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#avoid-dual-axes",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#avoid-dual-axes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Avoid Dual Axes",
    "text": "Avoid Dual Axes\n\n\n\n\nDual Axis"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#or-triple-axes",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#or-triple-axes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Or Triple Axes!",
    "text": "Or Triple Axes!\n\n\n\n\nTriple Axis"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#fewer-pie-charts",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#fewer-pie-charts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fewer Pie Charts…",
    "text": "Fewer Pie Charts…\n\n\n\n\nPie vs Bar"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#what",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#what",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What?!",
    "text": "What?!\n\n\nAbout Pie Charts"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#oof",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#oof",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Oof",
    "text": "Oof\n\n\n\n\nChart Fail"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#check-potential-ideas-the-python-or-r-graph-galleries",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#check-potential-ideas-the-python-or-r-graph-galleries",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Check Potential Ideas! The Python or R Graph Galleries",
    "text": "Check Potential Ideas! The Python or R Graph Galleries\n\n\n\n\n\n\n\nThe Python Graph Gallery\n\n\n\n\n\n\n\n\nThe R Graph Gallery"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#eliminating-noise",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#eliminating-noise",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Eliminating Noise",
    "text": "Eliminating Noise\n\n\nThe more information in your visualization, the greater the cognitive load.\nYour objective must be to reduce your audience cognitive costs.\n\n\n\nData-Ink Ratio Formula\n\\[\n\\text{Data-Ink Ratio} = \\frac{\\text{Data-Ink}}{\\text{Total ink used to print the graphic}}\n\\]"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#data-ink-ratio",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#data-ink-ratio",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data-Ink Ratio",
    "text": "Data-Ink Ratio\n\nYour objective must be to reduce your audience cognitive costs."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#step-by-step-cleanup",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#step-by-step-cleanup",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step-by-Step Cleanup",
    "text": "Step-by-Step Cleanup"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#eliminating-the-border",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#eliminating-the-border",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Eliminating the Border",
    "text": "Eliminating the Border"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#cleaning-the-grids",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#cleaning-the-grids",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cleaning the Grids",
    "text": "Cleaning the Grids"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#removing-the-points",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#removing-the-points",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Removing the Points",
    "text": "Removing the Points"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#processing-the-axes",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#processing-the-axes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Processing the Axes",
    "text": "Processing the Axes"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#adjusting-the-label",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#adjusting-the-label",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusting the Label",
    "text": "Adjusting the Label"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#adjusting-colors",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#adjusting-colors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusting Colors",
    "text": "Adjusting Colors"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#before-and-after",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#before-and-after",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Before and After",
    "text": "Before and After\n\n\n\n\n\nBefore and After"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#count-the-number-3s",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#count-the-number-3s",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Count the Number 3s",
    "text": "Count the Number 3s"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#count-the-number-3s-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#count-the-number-3s-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Count the Number 3s",
    "text": "Count the Number 3s"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#ways-to-draw-attention",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#ways-to-draw-attention",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ways to Draw Attention",
    "text": "Ways to Draw Attention"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#highlighting-with-colors",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#highlighting-with-colors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Highlighting with Colors",
    "text": "Highlighting with Colors"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#returning-to-our-example",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#returning-to-our-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Returning to Our Example",
    "text": "Returning to Our Example"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#returning-to-our-example-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#returning-to-our-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Returning to Our Example",
    "text": "Returning to Our Example"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#use-colors-strategically",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#use-colors-strategically",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Use Colors Strategically",
    "text": "Use Colors Strategically"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#emphasizing-the-main-point",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#emphasizing-the-main-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Emphasizing the Main Point",
    "text": "Emphasizing the Main Point"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#emphasizing-the-main-point-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#emphasizing-the-main-point-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Emphasizing the Main Point",
    "text": "Emphasizing the Main Point"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together\nLet’s tell a story starting from the chart below, making step-by-step adaptations we’ve discussed. What is it telling you?"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-2",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-3",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-4",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#bringing-it-all-together-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-2",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-3",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-4",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-5",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-6",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-7",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-narrative-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#before-and-after-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#before-and-after-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Before and After",
    "text": "Before and After"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#covid-19-evolution",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#covid-19-evolution",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "COVID-19 Evolution",
    "text": "COVID-19 Evolution"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#moving-average",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#moving-average",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Moving Average",
    "text": "Moving Average"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#deaths-in-new-york",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#deaths-in-new-york",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deaths in New York",
    "text": "Deaths in New York"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#additional-material",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Additional Material",
    "text": "Additional Material\n\nFlowing Data\nInformation is Beautiful\nThe Functional Art\nCOVID in the Financial Times"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#why-a-poster-presentation",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#why-a-poster-presentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why a Poster Presentation?",
    "text": "Why a Poster Presentation?\n\n\n\n\n\n\n\n\n\nShowcases your work\nOrganizes your ideas and results\nIs visually appealing\nEncourages interactive discussion\nDemonstrates your mastery of predictive analytics concepts"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#designing-a-predictive-analytics-poster-objectives",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#designing-a-predictive-analytics-poster-objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Designing a Predictive Analytics Poster: Objectives",
    "text": "Designing a Predictive Analytics Poster: Objectives\n\n\n\n\n\n\n\n\n\nCommunicate key findings and impact of your project\nHighlight the predictive approach, methodology, and novel insights\nHighlight the business implications and insights"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-template-and-rubric",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-template-and-rubric",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Template and Rubric",
    "text": "Planning Your Poster: Template and Rubric\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\n\n\n\nCheck my personal webpage to see other posters and projects I have mentored. It is highly recommended!\nThanks to Professor Matthew A. Lanham for sharing the original version of this template.\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-visual-hierarchy",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-visual-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Visual Hierarchy",
    "text": "Planning Your Poster: Visual Hierarchy\n\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\n\nProvide a visually engaging, easy-to-navigate summary\nAlways in columns!\nHeadings: Large and bold to guide the reader\nColor & Contrast: Choose a simple palette that highlights main points\nFont Size: Text should be legible from ~3 feet away\nFlow: Logical reading order from top-left to bottom-right\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-layout-design",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-layout-design",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Layout & Design",
    "text": "Planning Your Poster: Layout & Design\n\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\n\nKeep It Simple\n\nLimit text and rely on figures, charts, and bullet points\nUse consistent color schemes, fonts, and alignment\nUse a QR Code to drive your audience to a dashboard or the project webpage\n\nUse of Space\n\nWhite space is your friend — avoid clutter\nGroup related sections in boxes or areas for clarity\n60% Graphics / 40% Text\n\nKey Text Guidelines\n\nTitle: ~85–100 pt\nHeadings/Subheadings: ~46–55 pt\nBody Text: ~34–38 pt\nCaptions: ~26–28 pt\nShould be readble from 3 feet away\n\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-organizing-your-content",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#planning-your-poster-organizing-your-content",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Organizing Your Content",
    "text": "Planning Your Poster: Organizing Your Content\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\n\nTitle & Authors: Clear, concise, and visible\nAbstract/Introduction: One clear and concise paragraph to show why your problem is important and what are the main results.\nBusiness Problem: What is your project goal?\nAnalytics Problem Framing: What you aim to predict or explain and why\nResearch Question: Summarise your project in a question you will answer\nData and EDA: Key variables, data collection.\nMethodology: Modeling approach focusing on the response variable.\nModel Building and Evaluation: Model results clear and appropriately evaluated\nBusiness Implication: Business validation of model/solution demonstrated. Main findings, performance metrics, interpretation\nConclusion & Future Work: Wrap-up and potential next steps\nAcknowledgments & References: Recognize collaborators and sources\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#designing-for-predictive-analytics",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#designing-for-predictive-analytics",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Designing for Predictive Analytics",
    "text": "Designing for Predictive Analytics\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\n\nEmphasize the Predictive Component\n\nModel Choice: Clearly state if you used linear regression, random forests, neural networks, etc.\nMetrics: Include accuracy, RMSE, AUC, or other relevant metrics\nModel Interpretation: Highlight key features or coefficients that drove the predictions\n\nData Visualization Tips\n\nUse labeled graphs (e.g., bar charts, scatter plots, confusion matrices)\nShow before-and-after comparisons if you performed feature engineering\nExplain the importance of training vs. testing sets (or cross-validation)\n\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#crafting-a-clear-narrative",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#crafting-a-clear-narrative",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Crafting a Clear Narrative",
    "text": "Crafting a Clear Narrative"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#crafting-a-clear-narrative-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#crafting-a-clear-narrative-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Crafting a Clear Narrative",
    "text": "Crafting a Clear Narrative\n\n\n\n\nWord Count"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#crafting-a-clear-narrative-2",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#crafting-a-clear-narrative-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Crafting a Clear Narrative",
    "text": "Crafting a Clear Narrative\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\nIntroduction & Problem Statement\n\nPresent the real-world problem or question\nBriefly mention why it matters (e.g., business impact, social relevance)\n\nThe “So What?” Factor\n\nEmphasize the value of your predictive findings\nIllustrate how stakeholders can use predictions or insights\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#research-design-flow",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#research-design-flow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Research Design Flow",
    "text": "Research Design Flow\n\n\nShow each step as a section in your poster or as bullet points under Methodology"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#effective-figures-and-tables",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#effective-figures-and-tables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Effective Figures and Tables",
    "text": "Effective Figures and Tables\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\nCharts and Graphs\n\nLabel axes, include legends only if necessary\nHighlight the data that supports your main argument\nProvide short, descriptive captions beneath each figure\n\nTables\n\nKeep tables simple, emphasize key results\nConsider whether a chart might be more impactful than a table\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#results-interpretation",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#results-interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results & Interpretation",
    "text": "Results & Interpretation\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\n\n\nShowcase Predictive Performance\n\nProvide confusion matrices for classification models\nCompare model performance with baseline or benchmark\nDiscuss practical implications of the performance metrics\n\nCritical Thinking\n\nExplain why the model performed well or poorly\nReflect on limitations (e.g., small dataset, missing variables, etc.)\nSuggest ways to improve performance or replicate your findings\n\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#conclusion-future-work",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#conclusion-future-work",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusion & Future Work",
    "text": "Conclusion & Future Work\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\nMain Takeaways\n\nRecap key predictions and findings in plain language\nEmphasize the impact and potential applications\n\nNext Steps\n\nPossible avenues for further research or new data\nEncouraging further validation, deployment, or real-world testing\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-touches",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#final-touches",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Touches",
    "text": "Final Touches\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\nAcknowledgments & References\n\nRecognize supporting faculty, funding sources, and collaborators\nCite any references (including data sources and libraries) in a consistent format\n\nProofreading & Practice\nSpell-check all text, verify data accuracy, ensure images are clear\nPractice explaining your poster to a non-expert\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#presenting-your-poster",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#presenting-your-poster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Presenting your Poster",
    "text": "Presenting your Poster\n\n\n\n\n\n\n\n\nTemplate, Rubric*, and Video\n\n\n\n\n\nTips for Presenting Well\n\nArrive early to set up\nStay close and off to the side\nPrepare a 30-second, 90-second, and 2-minute elevator pitches using your poster as a visual guide\nInvite questions to spark in-depth discussions\nActively ask questions to your audience\nUse your hands to direct your listener to your poster\nPrevent you or someone else blocking the poster\nFollow the dress code: Business professional!\n\nNeed help with the dress code? Check the CCO Career Closet.\n\n\n\n\n\n\n\n\n*This rubric is used by URC conference judges to evaluate live presentations. For our course, poster submissions will be assessed using a separate rubric. Please refer to the Poster First Draft and Final Poster Submission Milestones for the specific evaluation criteria."
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#summary-1",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nMain Takeaways from this lecture:\n\n\n\nData Communication Principles:\n\nContext matters: Tailor your analysis to the audience and goal.\nFocus on the story: Highlight insights, not the process.\nBeauty and clarity: Simplify visuals, use appropriate colors, and remove unnecessary elements.\n\nVisualization Best Practices:\n\nUse graphs instead of tables where possible.\nAvoid misleading scales and excessive dimensions.\nPrioritize hierarchy and emphasize key data points.\n\n\n\n\n\nPoster:\n\nSimplicity and clarity win over clutter\nEmphasize the predictive analytics approach and metrics\nEnsure visual impact and logical flow\nPractice delivering a concise overview of your work\n\nFinal Message:\n\nLess is more. Reduce complexity to communicate data effectively.\nAlways keep your audience’s decision-making needs at the forefront."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-large-language-models-llm-1",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-large-language-models-llm-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: Large Language Models (LLM)",
    "text": "Special Topic: Large Language Models (LLM)\n\n\n\n\n\n\nLarge Language Models explained briefly"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-llm---transformers",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-llm---transformers",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: LLM - Transformers",
    "text": "Special Topic: LLM - Transformers\n\n\n\n\n\n\nTransformers, the tech behind LLMs"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-llm---attention",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-llm---attention",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: LLM - Attention",
    "text": "Special Topic: LLM - Attention\n\n\n\n\n\n\nAttention in transformers, step-by-step"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-llm---memory",
    "href": "lecture_slides/10_deep_learning/11_deep_learning.html#special-topic-llm---memory",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: LLM - Memory",
    "text": "Special Topic: LLM - Memory\n\n\n\n\n\n\nHow might LLMs store facts"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#overview",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nDeep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\nEarly Rise (1980s)\n\nNeural networks first gained popularity.\n\nHigh levels of excitement, with dedicated conferences (e.g., NeurIPS, Snowbird).\n\n1990s Shift\n\nEmergence of other methods (SVMs, Random Forests, Boosting).\n\nNeural networks receded into the background.\n\nResurgence (2010)\n\nRebranded and refined under the banner of Deep Learning.\n\nBy the 2020s, became extremely successful and widely adopted.\n\nKey Drivers of Success\n\nRapid increases in computing power (GPUs, parallel computing).\n\nAvailability of large-scale datasets.\n\nUser-friendly deep learning libraries (e.g., TensorFlow, PyTorch).\n\n\n\n\nMuch of the credit goes to three pioneers and their students:\n\nYann LeCun, Geoffrey Hinton, and Yoshua Bengio,\nwho received the 2019 ACM Turing Award for their work in Neural Networks."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-interviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-interviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AI Visionaries: Interviews",
    "text": "AI Visionaries: Interviews\n\n\n\n\n\n\n\n\n Yann LeCunThe Future of AIDec 16, 2023 \n\n\n\n\n\n\n\n Geoffrey Hinton60 Minutes InterviewOct 9, 2023 \n\n\n\n\n\n\n\n Yoshua BengioPath to Human-Level AIApr 24, 2024"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#what-are-deep-learning-frameworks",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#what-are-deep-learning-frameworks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What Are Deep Learning Frameworks?",
    "text": "What Are Deep Learning Frameworks?\n\nDeep learning frameworks reduce boilerplate code, handle tensor operations efficiently, and make it easier to prototype and iterate on new architectures.\nSoftware libraries designed to streamline the creation, training, and deployment of neural networks.\n\nProvide pre-built functions, automatic differentiation, and GPU/TPU support.\n\nNecessity: They allow researchers and developers to focus on model design rather than low-level implementation details."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-and-tensor-flow",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-and-tensor-flow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PyTorch and Tensor Flow",
    "text": "PyTorch and Tensor Flow\n\n\n\nWhat is PyTorch?\n\nDeveloped primarily by Facebook (Meta) and released on September 2016.\nEmphasizes a dynamic computation graph (eager execution).\nHighly “Pythonic”: feels natural for Python developers.\nStrong community presence in academia and research.\n\n\nWhy is PyTorch Necessary?\n\nEase of Use & Debugging\n\nEvaluate expressions immediately without building a separate graph.\n\nMore intuitive for experimenting with complex, innovative models.\n\nResearch Focus\n\nQuickly prototype new ideas and iterate.\n\nActive Ecosystem\n\nLibraries like torchvision, torchaudio, and others for specialized tasks.\n\n\n\n\nHow to begin\n\nhttps://pytorch.org/tutorials/beginner/basics/intro.html.\nThere is also a YouTube Series (PyTorch Beginner Series) also here (Introduction to PyTorch)\n\n\n\n\nWhat is TensorFlow?\n\nDeveloped primarily by Google and released in November 2015.\nHistorically used a static graph approach (with an “eager mode” added later).\nComes with extensive tools for deployment (mobile, web, and production).\nLarge ecosystem with well-integrated components (e.g., TensorBoard, TFX, TensorFlow Lite).\n\n\n\nWhy is TensorFlow Necessary?\n\nProduction-Ready\n\nStrong support for model serving at scale in enterprise environments.\n\nComprehensive Ecosystem\n\nVisualization (TensorBoard), data processing (TFX), and model deployment pipelines.\n\n\nCross-Platform & Hardware Support\n\nEasily deploy models to cloud infrastructures, mobile devices, and specialized hardware (TPUs).\n\n\n\n\nHow to begin\n\nhttps://www.tensorflow.org/tutorials. There is also a Quick Start!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#key-differences",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#key-differences",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Key Differences",
    "text": "Key Differences\n\n\n\n\n\n\n\n\nAspect\nPyTorch\nTensorFlow\n\n\n\n\nComputation Graph\nDynamic graph (eager execution by default).\nHistorically static graph with a build-and-execute phase (now supports eager execution).\n\n\nDebugging & Development Style\nMore straightforward for Python developers, immediate error feedback.\nCan be trickier to debug in graph mode; eager mode helps but is relatively newer.\n\n\nDeployment & Production\nTorchServe and growing enterprise support, but historically overshadowed by TensorFlow’s tools.\nTensorFlow Serving, TensorFlow Lite, and easy Google Cloud integration.\n\n\n\n\nWhile the fundamental math and building blocks are similar, the biggest difference typically lies in how you prototype, debug, and deploy models."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#similarities",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#similarities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Similarities",
    "text": "Similarities\n\n\n\n\n\n\n\nSimilarity\nDescription\n\n\n\n\nWide Range of Neural Network Layers\nConvolutional, Recurrent, Transformers, etc. Both frameworks maintain robust libraries for standard and advanced layers.\n\n\nAuto-Differentiation\nNo need to manually compute gradients; backpropagation is handled automatically.\n\n\nGPU Acceleration\nBoth leverage CUDA (NVIDIA GPUs) or other backends to speed up training.\n\n\nRich Communities\nAbundant tutorials, example code, pretrained models, and Q&A forums.\n\n\n\n\nDespite differing philosophies, PyTorch and TensorFlow share many core functionalities and have large, supportive user communities."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Advantages and Disadvantages",
    "text": "Comparison of Advantages and Disadvantages\n\n\n\n\n\n\n\n\n\n\nPyTorch\nTensorFlow\n\n\n\n\nAdvantages\n- Intuitive, Pythonic Syntax: Feels like standard Python, reducing friction for experimentation  - Dynamic Graph Execution: Simplifies debugging and model design  - Research & Academia Favorite: widely used in cutting-edge papers\n- Static Graph Optimization: Graph-based execution can be highly optimized for speed and memory usage  - Extensive Production Ecosystem: Includes TensorFlow Serving, TensorFlow Lite, TFX for data pipelines  - Large Corporate Adoption: Backed by Google, widely used in enterprise settings\n\n\nDisadvantages\n- Deployment Maturity: Production tooling and ecosystem are improving but still behind TensorFlow  - Smaller Enterprise Adoption: Historically overshadowed by TensorFlow’s widespread adoption\n- Learning Curve: The graph-based approach can be challenging for newcomers  - Historically Less Intuitive: Older APIs and tutorials can be confusing, though Eager Mode improves usability"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recommendations",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recommendations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommendations",
    "text": "Recommendations\n\n\nChoose PyTorch if:\n\nYour focus is on rapid experimentation and academic research\nYou prioritize a Pythonic workflow and easy debugging\nYou prefer a dynamic graph approach (about it).\nYou are working on cutting-edge models with high flexibility\nYou value seamless interaction with Python libraries\n\n\nChoose TensorFlow if:\n\nYou need robust production and deployment pipelines\nYou plan to integrate with Google Cloud services\nYou require support for mobile/edge devices (e.g., TensorFlow Lite)\nYou benefit from static graph optimization for performance\nYou want an end-to-end ecosystem (TFX, TensorBoard, Serving)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#tensors-in-pytorch",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#tensors-in-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tensors in PyTorch",
    "text": "Tensors in PyTorch"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders\n\n\n\nThe code below extracts a single image‑tensor from the training_data used in the tutorial (you can use test_data the same way), prints its basic properties, and visualizes it.\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Choose the index of the image you wish to inspect\nidx = 0  # e.g., the first image; change as desired\n\n# Fetch the sample\nimage_tensor, label = training_data[idx]   # image_tensor is a 1×28×28 tensor\n\n# Inspect the raw tensor values\nprint(\"Shape :\", image_tensor.shape)  # torch.Size([1, 28, 28])\nprint(\"Label :\", label) # integer class id\nprint(\"Tensor (first 5 rows):\\n\", image_tensor[0, :5, :])\n\n# Visualize the image\nplt.imshow(image_tensor.squeeze(), cmap=\"gray\")\nplt.title(f\"Fashion‑MNIST class{label}\")\nplt.axis(\"off\")\nplt.show()\n\n\n\nHow it works\n\nIndex selection – set idx to any integer in range(len(training_data)).\n\nDataset access – indexing the dataset returns (image, label) with the transform already applied (here, ToTensor() scales to [0,1]).\n\nInspection – the printed tensor slice lets you verify pixel values, and plt.imshow renders the sample for visual confirmation.\n\n\nTo see a different image you just need to adjust the index."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#transforms",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#transforms",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Transforms",
    "text": "Transforms"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nWhat Are We Doing?\nWe are defining a neural network class using PyTorch. This network is designed to work with images, specifically 28×28 grayscale images like those from the FashionMNIST dataset. The network will output 10 values, one for each digit from 0 to 9.\nStep-by-Step Breakdown\n\nclass NeuralNetwork(nn.Module):\n\nWe create a new neural network class called NeuralNetwork. It inherits from PyTorch’s nn.Module, which is the base class for all neural network models.\n\ndef __init__(self): and super().__init__()\n\n__init__ is the constructor. It’s run when we create the model.\nsuper().__init__() tells Python to also run the initialization code from the parent class (nn.Module). This is required for PyTorch to keep track of everything inside the model.\n\nself.flatten = nn.Flatten():\n\nchanges the input from a 2D image (28×28) into a 1D vector (784 values), which is easier for linear layers to handle."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\nHere we build the main body of the neural network.\nself.linear_relu_stack = nn.Sequential(\n    nn.Linear(28*28, 512),\n    nn.ReLU(),\n    nn.Linear(512, 512),\n    nn.ReLU(),\n    nn.Linear(512, 10),\n)\nIn most contexts when we say “how many layers?” we refer to the learnable ones. So this network has three fully‑connected (Linear) layers, with ReLU activations in between.\n\nYou can think of the linear layer as a filter that projects the image into a new space with 512 dimensions. These new values are not pixels anymore, but rather abstract features learned by the network."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-3",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nFirst layer nn.Linear(28*28, 512): takes the 784 values from the image and transforms them into 512 values. A Linear(784, 512) layer performs:\n\nA matrix multiplication between the input vector (length 784) and a weight matrix of size [784 × 512], followed by adding a bias vector of length 512.\nMathematically: \\[\n\\text{output} = x \\cdot W + b\n\\]\nx is the input vector: shape [784]\nW is the weight matrix: shape [784 × 512]\nb is the bias vector: shape [512]\nThe result (output) is a new vector of shape [512]\n\n\n\nEach of the 512 output values is a linear combination of all 784 pixel values in the input image. By default, PyTorch initializes weights using Kaiming Uniform Initialization (a variant of He initialization), which works well with ReLU activation functions."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-4",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nnn.ReLU(): applies the ReLU activation function, which keeps positive numbers and turns negative numbers into zero. This adds non-linearity to the model.\nSecond layernn.Linear(512, 512): takes those 512 values and again outputs 512 values. This is a hidden layer, helping the model learn more complex patterns.\nnn.ReLU(): Another non-linear transformation.\nThird (Final) layer:nn.Linear(512, 10): takes the 512 values and produces 10 output values.\n\nThese are called logits, and each one corresponds to a digit class (0 to 9)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-5",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nforward(self, x): This is the forward pass, the function that runs when we send data through the model.\nStep-by-step:\n\n\nx = self.flatten(x): Convert the 28×28 image into a 1D tensor with 784 values.\nlogits = self.linear_relu_stack(x): Pass the input through the series of layers.\nreturn logits: Output the final predictions (raw scores for each class).\n\n\nIn summary this neural network:\n\nTakes an image (28×28) as input,\nFlattens it into a vector,\nPasses it through two fully connected layers with ReLU,\nOutputs a vector of size 10 (one for each digit)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-6",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\ntorch.rand(1, 28, 28, device=device): Creates a random image with shape [1, 28, 28]\n\n1 is the batch size (just one image)\n28×28 is the image dimension\ndevice=device ensures the tensor goes to CPU or GPU (wherever the model is)\n\n\n\n\n# To see tensor:\nprint(X)\n\n\n\nLet’s say the tensor shown is:\nX = torch.tensor([[\n    [0.1177, 0.2669, 0.6367, 0.6148, 0.3085, ...],  # row 0\n    [0.8672, 0.3645, 0.4822, 0.9566, 0.8999, ...],  # row 1\n    ...\n]])\n\n\nThis is a 3D tensor of shape [1, 28, 28]:\n\nThe first dimension 1 is the batch size,\nThe next two are height and width of the image.\n\nThe full index of 0.2669 in the 3D tensor is: X[0, 0, 1].\n\n0 → first (and only) image in the batch\n0 → first row of the image\n1 → second column in that row"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-7",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\nlogits = model(X): This calls the model with input X.\n\nBehind the scenes, it runs model.forward(X)\nOutput: a vector of 10 values (called logits), one for each class (digits 0 through 9)\n\nNote: We do not call model.forward() directly — PyTorch manages hooks and gradients when we use model(X)\npred_probab = nn.Softmax(dim=1)(logits): Applies softmax to the raw output logits\n\nSoftmax turns logits into probabilities (values between 0 and 1 that sum to 1)\ndim=1 means we apply softmax across the 10 output class values (not across the batch)\n\ny_pred = pred_probab.argmax(1): Picks the index of the largest probability, i.e., the predicted class\n\nargmax(1) returns the class with the highest probability from each row (here we have just one row)\n\nprint(f\"Predicted class: {y_pred}\"): Prints the predicted digit class (0 through 9)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Automatic Differentiation with torch.autograd",
    "text": "Automatic Differentiation with torch.autograd"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#optimizing-model-parameters",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#optimizing-model-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Optimizing Model Parameters",
    "text": "Optimizing Model Parameters"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#save-and-load-the-model",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#save-and-load-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Save and Load the Model",
    "text": "Save and Load the Model"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#introduction-to-pytorch---youtube-series",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#introduction-to-pytorch---youtube-series",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Introduction to PyTorch - YouTube Series",
    "text": "Introduction to PyTorch - YouTube Series\n\n\n\nPro tip: Use Colab with a GPU runtime to speed up operations Runtime &gt; Change runtime type &gt; GPU"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#neural-networks---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#neural-networks---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Neural Networks - Video",
    "text": "Neural Networks - Video\n\n\n\n\n\n\nBut what is a neural network?"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\n\nNetwork Diagram of Single Layer Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Introduction and Layers Overview",
    "text": "Single Layer Neural Network: Introduction and Layers Overview\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nNeural networks are often displayed using network diagrams, as shown in the figure.\n\nInput Layer (Orange Circles):\n\n\\(X_1, X_2, X_3, X_4\\)\nThese are observed variables from the dataset.\n\nHidden Layer (Blue Circles):\n\n\\(A_1, A_2, A_3, A_4, A_5\\)\nThese are transformations (activations) computed from the inputs.\n\nOutput Layer (Pink Circle):\n\n\\(f(X) \\to Y\\)\n\\(Y\\) is also observed, e.g., a label or continuous response."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Observed vs. Latent",
    "text": "Single Layer Neural Network: Observed vs. Latent\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nWhere is the observed data?\n\n\\(X_j\\) are observed (the input features).\n\\(Y\\) is observed (the response or label).\nThe hidden units (\\(A_k\\)) are not observed; they’re learned transformations."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Hidden Layer as Transformations",
    "text": "Single Layer Neural Network: Hidden Layer as Transformations\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nIn the hidden layer, each activation \\(A_k\\) is computed as:\n\\[\nA_k = g\\Bigl(w_{k0} + \\sum_{j=1}^4 w_{kj} X_j\\Bigr),\n\\]\n\nIn the formula, these \\(h_k(X)\\) are the same as the activations \\(A_k\\).\n\\(h_k(X)\\) = \\(g(w_{k0} + \\sum_{j=1}^p w_{kj} X_j)\\).\n\\(g(\\cdot)\\) is a nonlinear function (e.g., ReLU, sigmoid, tanh).\n\\(w_{kj}\\) are the weights learned during training.\nEach hidden unit has a different set of weights, hence different transformations."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-training-the-network",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-training-the-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Training the Network",
    "text": "Single Layer Neural Network: Training the Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\n\nThe network learns all weights \\(w_{kj}, w_{k0}, \\beta_k, \\beta_0\\) during training.\nObjective: predict \\(Y\\) from \\(X\\) accurately.\nKey insight: Hidden layer learns useful transformations on the fly to help approximate the true function mapping \\(X\\) to \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-details",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Details",
    "text": "Single Layer Neural Network: Details\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A_k = h_k(X) = g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j)\\) are called the activations in the hidden layer. We can think of it as a non-linear tranformation of a linear function.\n\\(g(z)\\) is called the activation function. Two popular activation functions are: the sigmoid and rectified linear (ReLU).\nActivation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.\nSo the activations are like derived features — nonlinear transformations of linear combinations of the features.\nThe model is fit by minimizing \\(\\sum_{i=1}^{n} (y_i - f(x_i))^2\\) (e.g., for regression)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#nn-example-mnist-digits",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#nn-example-mnist-digits",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NN Example: MNIST Digits",
    "text": "NN Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights).\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent - Video",
    "text": "Gradient Descent - Video\n\n\n\n\n\n\nGradient descent, how neural networks learn"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-intuition---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-intuition---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Intuition - Video",
    "text": "Backpropagation Intuition - Video\n\n\n\n\n\n\nBackpropagation, intuitively"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-calculus---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-calculus---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Calculus - Video",
    "text": "Backpropagation Calculus - Video\n\n\n\n\n\n\nBackpropagation calculus"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Neural Networks",
    "text": "Fitting Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\min_{\\{w_k\\}_{1}^K, \\beta} \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2, \\quad \\text{where}\n\\]\n\\[\nf(x_i) = \\beta_0 + \\sum_{k=1}^K \\beta_k g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}\\right).\n\\]\nThis problem is difficult because the objective is non-convex.\nDespite this, effective algorithms have evolved that can optimize complex neural network problems efficiently."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#non-convex-functions-and-gradient-descent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#non-convex-functions-and-gradient-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non Convex Functions and Gradient Descent",
    "text": "Non Convex Functions and Gradient Descent\nLet \\(R(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - f_\\theta(x_i))^2\\) with \\(\\theta = (\\{w_k\\}_{1}^K, \\beta)\\).\n\n\nStart with a guess \\(\\theta^0\\) for all the parameters in \\(\\theta\\), and set \\(t = 0\\).\nIterate until the objective \\(R(\\theta)\\) fails to decrease:\n\nFind a vector \\(\\delta\\) that reflects a small change in \\(\\theta\\), such that \\(\\theta^{t+1} = \\theta^t + \\delta\\) reduces the objective; i.e., \\(R(\\theta^{t+1}) &lt; R(\\theta^t)\\).\nSet \\(t \\gets t + 1\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent-continued",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent Continued",
    "text": "Gradient Descent Continued\n\nIn this simple example, we reached the global minimum.\nIf we had started a little to the left of \\(\\theta^0\\), we would have gone in the other direction and ended up in a local minimum.\nAlthough \\(\\theta\\) is multi-dimensional, we have depicted the process as one-dimensional. It is much harder to identify whether one is in a local minimum in high dimensions.\nHow to find a direction \\(\\delta\\) that points downhill? We compute the gradient vector: \\[\n\\nabla R(\\theta^t) = \\frac{\\partial R(\\theta)}{\\partial \\theta} \\bigg|_{\\theta = \\theta^t}\n\\]\ni.e., the vector of partial derivatives at the current guess \\(\\theta^t\\).\nThe gradient points uphill, so our update is \\(\\delta = - \\rho \\nabla R(\\theta^t)\\) or \\[\n\\theta^{t+1} \\gets \\theta^t - \\rho \\nabla R(\\theta^t),\n\\] where \\(\\rho\\) is the learning rate (typically small, e.g., \\(\\rho = 0.001\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradients-and-backpropagation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradients-and-backpropagation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradients and Backpropagation",
    "text": "Gradients and Backpropagation\n\n\\[\nR(\\theta) = \\sum_{i=1}^n R_i(\\theta) \\text{ is a sum, so gradient is sum of gradients.}\n\\]\n\\[\nR_i(\\theta) = \\frac{1}{2}(y_i - f_\\theta(x_i))^2 = \\frac{1}{2} \\left( y_i - \\beta_0 - \\sum_{k=1}^K \\beta_k g\\left( w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij} \\right) \\right)^2\n\\]\nFor ease of notation, let\n\\[\nz_{ik} = w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}.\n\\]\nBackpropagation uses the chain rule for differentiation:\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial \\beta_k} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\beta_k}\n= -(y_i - f_\\theta(x_i)) \\cdot g(z_{ik}).\n\\]\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial w_{kj}} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial g(z_{ik})} \\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}} \\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}}\n= -(y_i - f_\\theta(x_i)) \\cdot \\beta_k \\cdot g'(z_{ik}) \\cdot x_{ij}.\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#tricks-of-the-trade",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#tricks-of-the-trade",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tricks of the Trade",
    "text": "Tricks of the Trade\n\nSlow learning. Gradient descent is slow, and a small learning rate \\(\\rho\\) slows it even further. With early stopping, this is a form of regularization.\nStochastic gradient descent. Rather than compute the gradient using all the data, use a small minibatch drawn at random at each step. E.g. for MNIST data, with \\(n = 60K\\), we use minibatches of 128 observations.\nAn epoch is a count of iterations and amounts to the number of minibatch updates such that \\(n\\) samples in total have been processed; i.e. \\(60K/128 \\approx 469\\) for MNIST.\nRegularization. Ridge and lasso regularization can be used to shrink the weights at each layer. Two other popular forms of regularization are dropout and augmentation, discussed next."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#dropout-learning",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#dropout-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dropout Learning",
    "text": "Dropout Learning\n\n\nAt each Stochastic Gradient Descent (SGD) update, randomly remove units with probability \\(\\phi\\), and scale up the weights of those retained by \\(1/(1-\\phi)\\) to compensate.\nIn simple scenarios like linear regression, a version of this process can be shown to be equivalent to ridge regularization.\nAs in ridge, the other units stand in for those temporarily removed, and their weights are drawn closer together.\nSimilar to randomly omitting variables when growing trees in random forests."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#ridge-and-data-augmentation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#ridge-and-data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge and Data Augmentation",
    "text": "Ridge and Data Augmentation\n\n\nMake many copies of each \\((x_i, y_i)\\) and add a small amount of Gaussian noise to the \\(x_i\\) — a little cloud around each observation — but leave the copies of \\(y_i\\) alone!\nThis makes the fit robust to small perturbations in \\(x_i\\), and is equivalent to ridge regularization in an OLS setting."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation-on-the-fly",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation-on-the-fly",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation on the Fly",
    "text": "Data Augmentation on the Fly\n\n\nData augmentation is especially effective with SGD, here demonstrated for a CNN and image classification.\nNatural transformations are made of each training image when it is sampled by SGD, thus ultimately making a cloud of images around each original training image.\nThe label is left unchanged — in each case still tiger.\nImproves performance of CNN and is similar to ridge."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#double-descent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#double-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Double Descent",
    "text": "Double Descent\n\nWith neural networks, it seems better to have too many hidden units than too few.\nLikewise more hidden layers better than few.\nRunning stochastic gradient descent till zero training error often gives good out-of-sample error.\nIncreasing the number of units or layers and again training till zero error sometimes gives even better out-of-sample error.\nWhat happened to overfitting and the usual bias-variance trade-off?\n\nBelkin, Hsu, Ma, and Mandal (arXiv 2018) Reconciling Modern Machine Learning and the Bias-Variance Trade-off."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#the-double-descent-error-curve",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#the-double-descent-error-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Double-Descent Error Curve",
    "text": "The Double-Descent Error Curve\n\n\nWhen \\(d \\leq 20\\), model is OLS, and we see usual bias-variance trade-off.\nWhen \\(d &gt; 20\\), we revert to minimum-norm. As \\(d\\) increases above 20, \\(\\sum_{j=1}^d \\hat{\\beta}_j^2\\) decreases since it is easier to achieve zero error, and hence less wiggly solutions."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#less-wiggly-solutions",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#less-wiggly-solutions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Less Wiggly Solutions",
    "text": "Less Wiggly Solutions\n\n\nTo achieve a zero-residual solution with \\(d = 20\\) is a real stretch!\nEasier for larger \\(d\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#some-facts",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#some-facts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Facts",
    "text": "Some Facts\n\nIn a wide linear model (\\(p \\gg n\\)) fit by least squares, SGD with a small step size leads to a minimum norm zero-residual solution.\nStochastic gradient flow — i.e. the entire path of SGD solutions — is somewhat similar to ridge path.\nBy analogy, deep and wide neural networks fit by SGD down to zero training error often give good solutions that generalize well.\nIn particular cases with high signal-to-noise ratio — e.g. image recognition — are less prone to overfitting; the zero-error solution is mostly signal!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-introduction",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-introduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN: Introduction",
    "text": "CNN: Introduction\n\nNeural networks rebounded around 2010 with big successes in image classification.\nAround that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#the-cifar100-database",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#the-cifar100-database",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The CIFAR100 Database",
    "text": "The CIFAR100 Database\n\n\n\nThe figure shows 75 images drawn from the CIFAR100 database.\nThis database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale).\nEach image has a resolution of 32 × 32 pixels, with three eight-bit numbers per pixel representing red, green, and blue. The numbers for each image are organized in a three-dimensional array called a feature map.\nThe first two axes are spatial (both 32-dimensional), and the third is the channel axis, representing the three (blue, green or red) colors.\nThere is a designated training set of 50,000 images, and a test set of 10,000."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#the-convolutional-network-hierarchy",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#the-convolutional-network-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Convolutional Network Hierarchy",
    "text": "The Convolutional Network Hierarchy\n\n\n\nCNNs mimic, to some degree, how humans classify images, by recognizing specific features or patterns anywhere in the image that distinguish each particular object class.\nThe network first identifies low-level features in the input image, such as small edges or patches of color.\nThese low-level features are then combined to form higher-level features, such as parts of ears or eyes. Eventually, the presence or absence of these higher-level features contributes to the probability of any given output class.\nThis hierarchical construction is achieved by combining two specialized types of hidden layers: convolution layers and pooling layers:\nConvolution layers search for instances of small patterns in the image.\nPooling layers downsample these results to select a prominent subset.\nTo achieve state-of-the-art results, contemporary neural network architectures often use many convolution and pooling layers."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-layer",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Layer",
    "text": "Convolution Layer\n\n\nA convolution layer is made up of a large number of convolution filters, each of which is a template that determines whether a particular local feature is present in an image.\nA convolution filter relies on a very simple operation, called a convolution, which basically amounts to repeatedly multiplying matrix elements and then adding the results. \\[\n\\text{Input Image} =\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l\n\\end{bmatrix}\n\\quad \\text{Convolution Filter} =\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{bmatrix}.\n\\]\nWhen we convolve the image with the filter, we get the result: \\[\n\\text{Convolved Image} =\n\\begin{bmatrix}\na\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}.\n\\]\nthe convolution filter is applied to every 2 × 2 submatrix of the original image in order to obtain the convolved image.\nIf a 2 × 2 submatrix of the original image resembles the convolution filter, then it will have a large value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that resemble the convolution filter.\nThe filter is itself an image and represents a small shape, edge, etc.\nThe filters are learned during training."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-example",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Example",
    "text": "Convolution Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe idea of convolution with a filter is to find common patterns that occur in different parts of the image.\nThe two filters shown here highlight vertical and horizontal stripes.\nThe result of the convolution is a new feature map.\nSince images have three color channels, the filter does as well: one filter per channel, and dot-products are summed.\nThe weights in the filters are learned by the network."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling-layer",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pooling Layer",
    "text": "Pooling Layer\nA pooling layer provides a way to condense a large image into a smaller summary image.\n\\[\n\\text{Max pool}\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]\n\nEach non-overlapping \\(2 \\times 2\\) block is replaced by its maximum.\nThis sharpens the feature identification.\nAllows for locational invariance.\nReduces the dimension by a factor of 4 — i.e., factor of 2 in each dimension."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#architecture-of-a-cnn",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#architecture-of-a-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Architecture of a CNN",
    "text": "Architecture of a CNN\n\n\nMany convolve + pool layers.\nFilters are typically small, e.g., each channel \\(3 \\times 3\\).\nEach filter creates a new channel in the convolution layer.\nAs pooling reduces size, the number of filters/channels is typically increased.\nNumber of layers can be very large.\nE.g., resnet50 trained on imagenet 1000-class image database has 50 layers!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\n\n\nAn additional important trick used with image modeling is data augmentation.\nEssentially, each training image is replicated many times, with each replicate randomly distorted in a natural way such that human recognition is unaffected.\nTypical distortions are zoom, horizontal and vertical shift, shear, small rotations, and in this case horizontal flips.\nAt face value this is a way of increasing the training set considerably with somewhat different examples, and thus protects against overfitting.\nIn fact we can see this as a form of regularization: we build a cloud of images around each original image, all with the same label."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN Example: Pretrained Networks to Classify Images",
    "text": "CNN Example: Pretrained Networks to Classify Images\n\nHere we use the 50-layer resnet50 network trained on the 1000-class imagenet corpus to classify some photographs.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-imdb-movie-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-imdb-movie-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification: IMDB Movie Reviews",
    "text": "Document Classification: IMDB Movie Reviews\nThe IMDB corpus consists of user-supplied movie ratings for a large collection of movies. Each has been labeled for sentiment as positive or negative. Here is the beginning of a negative review:\n\nThis has to be one of the worst films of the 1990s. When my friends & I were watching this film (being the target audience it was aimed at) we just sat & watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn …\n\nWe have labeled training and test sets, each consisting of 25,000 reviews, and each balanced with regard to sentiment.\nGoal: We want to build a classifier to predict the sentiment of a review."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#featurization-bag-of-words",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#featurization-bag-of-words",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Featurization: Bag-of-Words",
    "text": "Featurization: Bag-of-Words\nDocuments have different lengths and consist of sequences of words. How do we create features \\(X\\) to characterize a document?\n\nFrom a dictionary, identify the 10K most frequently occurring words.\nCreate a binary vector of length \\(p = 10K\\) for each document, and score a 1 in every position that the corresponding word occurred.\nWith \\(n\\) documents, we now have an \\(n \\times p\\) sparse feature matrix \\(\\mathbf{X}\\).\nWe compare a lasso logistic regression model to a two-hidden-layer neural network on the next slide. (No convolutions here!)\nBag-of-words are unigrams. We can instead use bigrams (occurrences of adjacent word pairs) and, in general, m-grams."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews",
    "text": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews\n\n\n\nSimpler lasso logistic regression model works as well as neural network in this case.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks---rnn-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks---rnn-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks - RNN",
    "text": "Recurrent Neural Networks - RNN\n\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#simple-recurrent-neural-network-architecture",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#simple-recurrent-neural-network-architecture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Recurrent Neural Network Architecture",
    "text": "Simple Recurrent Neural Network Architecture\n\n\nThe hidden layer is a sequence of vectors \\(A_\\ell\\), receiving as input \\(X_\\ell\\) as well as \\(A_{\\ell-1}\\). \\(A_\\ell\\) produces an output \\(O_\\ell\\).\nThe same weights \\(\\mathbf{W}\\), \\(\\mathbf{U}\\), and \\(\\mathbf{B}\\) are used at each step in the sequence — hence the term recurrent.\nThe \\(A_\\ell\\) sequence represents an evolving model for the response that is updated as each element \\(X_\\ell\\) is processed."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-in-detail",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN in Detail",
    "text": "RNN in Detail\n\nSuppose \\(X_\\ell = (X_{\\ell1}, X_{\\ell2}, \\ldots, X_{\\ell p})\\) has \\(p\\) components, and \\(A_\\ell = (A_{\\ell1}, A_{\\ell2}, \\ldots, A_{\\ell K})\\) has \\(K\\) components. Then the computation at the \\(k\\)-th components of hidden unit \\(A_\\ell\\) is:\n\\[\nA_{\\ell k} = g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_{\\ell j} + \\sum_{s=1}^{K} u_{ks} A_{\\ell-1,s}\\right)\n\\]\n\\[\nO_\\ell = \\beta_0 + \\sum_{k=1}^{K} \\beta_k A_{\\ell k}\n\\]\nOften we are concerned only with the prediction \\(O_L\\) at the last unit. For squared error loss, and \\(n\\) sequence/response pairs, we would minimize:\n\\[\n\\sum_{i=1}^{n} (y_i - o_{iL})^2 = \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} x_{iL,j} + \\sum_{s=1}^{K} u_{ks} a_{i,L-1,s}\\right)\\right)\\right)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-for-document-classification-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-for-document-classification-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN for Document Classification",
    "text": "RNN for Document Classification\n\nThe document feature is a sequence of words \\(\\{\\mathcal{W}_\\ell\\}_{1}^{L}\\). We typically truncate/pad the documents to the same number \\(L\\) of words (we use \\(L = 500\\)).\nEach word \\(\\mathcal{W}_\\ell\\) is represented as a one-hot encoded binary vector \\(X_\\ell\\) (dummy variable) of length \\(10K\\), with all zeros and a single one in the position for that word in the dictionary.\nThis results in an extremely sparse feature representation and would not work well.\nInstead, we use a lower-dimensional pretrained word embedding matrix \\(\\mathbf{E}\\) (\\(m \\times 10K\\), next slide).\nThis reduces the binary feature vector of length \\(10K\\) to a real feature vector of dimension \\(m \\ll 10K\\) (e.g., \\(m\\) in the low hundreds)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Word Embedding - RNN Example: IMDB Reviews",
    "text": "Word Embedding - RNN Example: IMDB Reviews\n\nReview:\n\nthis is one of the best films actually the best I have ever seen the film starts one fall day…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings are pretrained on very large corpora of documents, using methods similar to principal components. word2vec and GloVe are popular.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-time-series-forecasting",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-time-series-forecasting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN: Time Series Forecasting",
    "text": "RNN: Time Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew-York Stock Exchange Data\nThree daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autocorrelation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autocorrelation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nThe autocorrelation at lag \\(\\ell\\) is the correlation of all pairs \\((v_t, v_{t-\\ell})\\) that are \\(\\ell\\) trading days apart.\nThese sizable correlations give us confidence that past values will be helpful in predicting the future.\nThis is a curious prediction problem: the response \\(v_t\\) is also a feature \\(v_{t-\\ell}\\)!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-forecaster",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Forecaster",
    "text": "RNN Forecaster\nWe only have one series of data! How do we set up for an RNN?\nWe extract many short mini-series of input sequences \\(\\mathbf{X} = \\{ X_1, X_2, \\ldots, X_L \\}\\) with a predefined length \\(L\\) known as the lag:\n\\[\nX_1 = \\begin{pmatrix}\nv_{t-L} \\\\\nr_{t-L} \\\\\nz_{t-L}\n\\end{pmatrix}, \\quad\nX_2 = \\begin{pmatrix}\nv_{t-L+1} \\\\\nr_{t-L+1} \\\\\nz_{t-L+1}\n\\end{pmatrix}, \\quad\n\\cdots, \\quad\nX_L = \\begin{pmatrix}\nv_{t-1} \\\\\nr_{t-1} \\\\\nz_{t-1}\n\\end{pmatrix}, \\quad \\text{and} \\quad Y = v_t.\n\\]\nSince \\(T = 6,051\\), with \\(L = 5\\), we can create 6,046 such \\((X, Y)\\) pairs.\nWe use the first 4,281 as training data, and the following 1,770 as test data. We fit an RNN with 12 hidden units per lag step (i.e., per \\(A_\\ell\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Results for NYSE Data",
    "text": "RNN Results for NYSE Data\n\nThe figure shows predictions and truth for the test period.\n\\[\nR^2 = 0.42 \\text{ for RNN}\n\\]\n\\(R^2 = 0.18\\) for the naive approach — uses yesterday’s value of Log trading volume to predict that of today.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-forecaster",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Forecaster",
    "text": "Autoregression Forecaster\nThe RNN forecaster is similar in structure to a traditional autoregression procedure.\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\nv_{L+1} \\\\\nv_{L+2} \\\\\nv_{L+3} \\\\\n\\vdots \\\\\nv_T\n\\end{bmatrix}, \\quad\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & v_L & v_{L-1} & \\cdots & v_1 \\\\\n1 & v_{L+1} & v_L & \\cdots & v_2 \\\\\n1 & v_{L+2} & v_{L+1} & \\cdots & v_3 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & v_{T-1} & v_{T-2} & \\cdots & v_{T-L}\n\\end{bmatrix}.\n\\]\nFit an OLS regression of \\(\\mathbf{y}\\) on \\(\\mathbf{M}\\), giving:\n\\[\n\\hat{v}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 v_{t-1} + \\hat{\\beta}_2 v_{t-2} + \\cdots + \\hat{\\beta}_L v_{t-L}.\n\\]\nKnown as an order-\\(L\\) autoregression model or \\(AR(L)\\).\nFor the NYSE data, we can include lagged versions of DJ_return and log_volatility in matrix \\(\\mathbf{M}\\), resulting in \\(3L + 1\\) columns."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Results for NYSE Data",
    "text": "Autoregression Results for NYSE Data\n\n\\(R^2 = 0.41 \\text{ for } AR(5) \\text{ model (16 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for RNN model (205 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for } AR(5) \\text{ model fit by neural network.}\\)\n\\(R^2 = 0.46 \\text{ for all models if we include } \\textbf{day_of_week} \\text{ of day being predicted.}\\)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#summary-of-rnns",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#summary-of-rnns",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary of RNNs",
    "text": "Summary of RNNs\n\nWe have presented the simplest of RNNs. Many more complex variations exist.\nOne variation treats the sequence as a one-dimensional image, and uses CNNs for fitting. For example, a sequence of words using an embedding representation can be viewed as an image, and the CNN convolves by sliding a convolutional filter along the sequence.\nCan have additional hidden layers, where each hidden layer is a sequence, and treats the previous hidden layer as an input sequence.\nCan have output also be a sequence, and input and output share the hidden units. So called seq2seq learning are used for language translation."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\n\nCNNs have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.\nRNNs have had big wins in speech modeling, language translation, and forecasting.\n\n\nShould we always use deep learning models?\n\nOften the big successes occur when the signal to noise ratio is high — e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.\nFor noisier data, simpler models can often work better:\n\nOn the NYSE data, the AR(5) model is much simpler than an RNN, and performed as well.\nOn the IMDB review data, a linear model fit (e.g. with glmnet) did as well as the neural network, and better than the RNN."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\nAs the authors suggest, I also endorse the Occam’s razor principle — we prefer simpler models if they work as well. More interpretable!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-large-language-models-llm-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-large-language-models-llm-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: Large Language Models (LLM)",
    "text": "Special Topic: Large Language Models (LLM)\n\n\n\n\n\n\nLarge Language Models explained briefly"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-llm---transformers",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-llm---transformers",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: LLM - Transformers",
    "text": "Special Topic: LLM - Transformers\n\n\n\n\n\n\nTransformers, the tech behind LLMs"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-llm---attention",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-llm---attention",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: LLM - Attention",
    "text": "Special Topic: LLM - Attention\n\n\n\n\n\n\nAttention in transformers, step-by-step"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-llm---memory",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#special-topic-llm---memory",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Special Topic: LLM - Memory",
    "text": "Special Topic: LLM - Memory\n\n\n\n\n\n\nHow might LLMs store facts"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#additional-material",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Additional Material",
    "text": "Additional Material\n\n3Blue1Brown: Neural Networks\nDeep Learning, by Ian Goodfellow and Yoshua Bengio and Aaron Courvill\nWelch Labs: Neural Networks Demystified\nWelch Labs: Learning To See\nDistill: A Gentle Introduction to Graph Neural Networks\nNeural Networks and Deep Learning, by Michael Nielsen\nCITS4012 Natural Language Processing\nDeep Learning with PyTorch Step-by-Step"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#summary",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nDeep Learning Renaissance\n\nNeural networks first rose to prominence in the 1980s, waned in the 1990s, then surged again around 2010.\nAdvances in computing (GPUs) and availability of massive labeled datasets propelled deep learning success.\n\nFrameworks (PyTorch vs. TensorFlow)\n\nPyTorch is known for its dynamic graph and Pythonic syntax; widely used in research.\nTensorFlow has an extensive production ecosystem, ideal for enterprise and deployment.\n\nEssential Concepts\n\nAutomatic differentiation, gradient descent, and backpropagation are at the core of training neural networks.\n\n\n\n\n\n\nCNNs and RNNs\n\nCNNs excel in image classification by learning local patterns via convolution and pooling layers.\n\nRNNs (and variants like LSTM, GRU) handle sequential data for tasks like language modeling and time-series forecasting.\n\nWhen to Use Deep Learning\n\nWorks best on large datasets with high signal-to-noise ratio (e.g., image, text).\n\nSimpler models often perform well on noisier tasks or smaller datasets.\n\nOver-parameterization can still generalize due to “double-descent” effects.\n\nPractical Tips\n\nUse regularization (dropout, data augmentation, weight decay) to mitigate overfitting.\n\nMonitor convergence with appropriate learning rates and consider mini-batch stochastic gradient descent."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#overview",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nMoving Beyond Linearity\nPolynomial Regression\nStep Functions\nRegression Splines\n\n\n\nSmoothing Splines\nLocal Regression\nGeneralized Additive Models\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#moving-beyond-linearity-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#moving-beyond-linearity-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Moving Beyond Linearity",
    "text": "Moving Beyond Linearity\n\nThe truth is never linear!\nOr almost never!\nBut often the linearity assumption is good enough.\nWhen it’s not…\n\npolynomials,\n\nstep functions,\n\nsplines,\n\nlocal regression, and\n\ngeneralized additive models\n\noffer a lot of flexibility, without losing the ease and interpretability of linear models."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-wage-data",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-wage-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Wage Data",
    "text": "Polynomial Regression: Wage Data\n\nPolynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. It provides a simple way to provide a non-linear fit to data.\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nLeft Plot: We fit a fourth-degree polynomial model to predict Wage using Age as the predictor. The data become sparse toward the higher end of Age, so there is relatively little information to guide the model’s fit in that region. As a result, the standard errors increase toward the tail—a phenomenon often referred to as “leverage.”\n\n\n\nRight Plot: We fit a fourth-degree polynomial model for a logistic regression. Similar to the left plot, the data diminish at the tail end of the predictor range, leaving fewer observations to inform the fit and leading to wider confidence intervals in that region."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Details",
    "text": "Polynomial Regression: Details\n\n\nCreate new variables \\(X_1 = X, \\, X_2 = X^2\\), etc., and then treat as multiple linear regression.\nIt is linear in the coefficients, but it is a non linear function of \\(x\\).\nNot really interested in the coefficients; more interested in the fitted function values at any value \\(x_0\\):\n\n\n\\[\n    \\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 + \\hat{\\beta}_2 x_0^2 + \\hat{\\beta}_3 x_0^3 + \\hat{\\beta}_4 x_0^4.\n\\]\n\n\nSince \\(\\hat{f}(x_0)\\) is a linear function of the \\(\\hat{\\beta}_\\ell\\), can get a simple expression for pointwise-variances (\\(\\text{Var}[\\hat{f}(x_0)]\\)) at any value \\(x_0\\).\n\nIn the figure, we have computed the fit and pointwise standard errors on a grid of values for \\(x_0\\). We show \\(\\hat{f}(x_0) \\pm 2 \\cdot \\text{se}[\\hat{f}(x_0)]\\).\n\nHow to choose \\(d\\), the polynomial degree? We either fix the degree \\(d\\) at some reasonably low value or use cross-validation to choose \\(d\\)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Details",
    "text": "Polynomial Regression: Details\n\n\nLogistic regression follows naturally. For example, in the figure we model\n\n\n\\[\n    \\text{Pr}(y_i &gt; 250 \\mid x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}{1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}.\n\\]\n\n\nTo get confidence intervals, compute upper and lower bounds on on the logit scale, and then invert to get on the probability scale.\nCan do separately on several variables—just stack the variables into one matrix, and separate out the pieces afterwards (see GAMs later).\nCaveat: Polynomials have notorious tail behavior — very bad for extrapolation. So, it is not recommended to trust predictions near the end of the data."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\n\nStep functions cut the range of a variable into \\(K\\) distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.\n\\[\nC_1(X) = I(X &lt; 35), \\quad C_2(X) = I(35 \\leq X &lt; 50), \\dots, C_3(X) = I(X \\geq 65)\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-2",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\n\nA step function model partitions the range of a predictor variable \\(X\\) into multiple intervals and creates a set of dummy (0/1) indicators—one for each interval. By fitting a standard linear model with these dummy variables, the resulting function is piecewise constant: within each interval, the fitted value remains the same.\n\nLocal (Step Functions): Because each interval in a step function acts independently, changes in one interval have minimal or no effect on the fitted values in other intervals. Consequently, step functions allow a locally controlled fit, where data in a specific region of \\(X\\) only affect the parameters corresponding to that interval.\nGlobal (Polynomials): In contrast, polynomial models rely on parameters that apply across the entire range of \\(X\\). Consequently, if you alter a single data point or a small set of points in one region, those changes can influence the fitted function everywhere else. This global dependence can lead to dramatic shifts in the estimated curve.\nSimplicity: Step functions are conceptually straightforward and easy to implement, requiring only the definition of intervals and fitting dummy variables.\nLocal Control: Their piecewise nature can be beneficial when the true relationship changes abruptly or when you want to minimize the effect of outliers in one region on the fit elsewhere.\nBut there are some Drawbacks:\n\nBlocky Appearance: The fitted function may appear abrupt or “blocky,” as it is piecewise constant rather than smooth.\n\nBoundary Sensitivity: Choosing the number and location of breakpoints is somewhat subjective and can significantly impact the model’s fit.\n\nOverall, step functions provide an intuitive, locally controlled alternative to global polynomial models. However, the abrupt transitions and the need to specify breakpoints can limit their practical appeal—particularly for applications where a smooth or continuous functional form is desired."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-3",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step functions",
    "text": "Step functions\n\nEasy to work with. Creates a series of dummy variables representing each group.\nUseful way of creating interactions that are easy to interpret. For example, interaction effect of \\(\\text{Year}\\) and \\(\\text{Age}\\):\n\n\n\\[\n    I(\\text{Year} &lt; 2005) \\cdot \\text{Age}, \\quad I(\\text{Year} \\geq 2005) \\cdot \\text{Age}\n\\]\nwould allow for different linear functions in each age category.\n\n\nChoice of cutpoints or knots can be problematic. For creating nonlinearities, smoother alternatives such as splines are available."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nWhile linear models are simple and often effective, real-world data frequently exhibit non-linear relationships, necessitating more flexible modeling approaches.\n\nNon-Linearity is Common: Most real-world data are not well-represented by simple linear trends, underscoring the importance of non-linear modeling techniques.\n\nPolynomial Regression: Extends linear models by including polynomial terms, enabling the capture of curved relationships. This method offers flexibility but may exhibit instability, especially at the tails, where standard error bands tend to widen due to sparse data.\n\nStep Functions: Provide an alternative approach by dividing continuous variables into discrete intervals, resulting in piecewise constant fits. This makes them particularly useful when natural breakpoints exist in the data.\n\nInteractions with Dummy Variables: Step functions are intuitive to work with and allow for straightforward inclusion of interaction effects through the creation of dummy variables.\n\nChallenges of Step Functions: The performance of step functions heavily depends on the selection of cut points. Poorly chosen boundaries can obscure important patterns or lead to overfitting, highlighting the need for domain expertise when applying this method.\n\nTakeaway: Both polynomial regression and step functions are valuable tools for modeling non-linear relationships. However, their effectiveness depends on thoughtful implementation, particularly in managing standard errors and selecting appropriate cut points."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#regression-splines-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#regression-splines-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nRegression splines are an extension and more flexible than polynomials and step functions.\nThey involve dividing the range of \\(X\\) into \\(K\\) distinct regions. Within each region, a polynomial function is fit to the data.\nHowever, these polynomials are constrained so that they join smoothly at the region boundaries, or knots. Provided that the interval is divided into enough regions, this can produce an extremely flexible fit."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#piecewise-polynomials",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#piecewise-polynomials",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Piecewise Polynomials",
    "text": "Piecewise Polynomials\n\nInstead of a single polynomial in \\(X\\) over its whole domain, we can rather use different polynomials in regions defined by knots.\n\n\n\\[\n    y_i =\n    \\begin{cases}\n    \\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i^2 + \\beta_{31}x_i^3 + \\epsilon_i & \\text{if } x_i &lt; c; \\\\\n    \\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_i^3 + \\epsilon_i & \\text{if } x_i \\geq c.\n    \\end{cases}\n\\]\n\n\nBetter to add constraints to the polynomials, e.g., continuity.\nSplines have the “maximum” amount of continuity."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#splines-visualization",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Splines Visualization",
    "text": "Splines Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop-Left Panel: A third-degree polynomial is fitted to the data on the left side of the knot at \\(X = 50\\), and another (separate) third-degree polynomial is fitted on the right side. There is no continuity constraint imposed at the knot, meaning the two polynomials may not meet at the same function value at \\(X = 50\\).\nTop-Right Panel: Again, a third-degree polynomial is fitted on each side of \\(X = 50\\). However, in this case, the polynomials are forced to be continuous at the knot. In other words, they must share the same function value at \\(X = 50\\).\nBottom-Left Panel: As in the top-right panel, a third-degree polynomial is fitted on each side of \\(X = 50\\), but with an additional constraint that enforces continuity of the first and second derivatives at \\(X = 50\\). This ensures a smoother transition between the left and right segments of the piecewise function.\nBottom-Right Panel: A linear regression model is fitted on each side of \\(X = 50\\). The model is constrained to be continuous at the knot, so both linear segments meet at the same value at \\(X = 50\\)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines",
    "text": "Linear Splines\n\nThe predictor space is partitioned at a set of specified points called knots \\(\\{\\xi_k\\}\\). A linear spline is a piecewise linear polynomial that remains continuous at each knot.\nWe construct linear spline models by augmenting a linear predictor with piecewise components that activate past designated knot locations, yielding a flexible yet interpretable approach to modeling relationships. Specifically:\n\n\n\nBasis-Function Representation: The model is written as\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+1} b_{K+1}(x_i) + \\epsilon_i,\n\\]\nwhere each \\(b_k(\\cdot)\\) is a basis function.\nOne basis function, \\(b_1(x_i)\\), is simply \\(x_i\\). The others, \\(b_{k+1}(x_i) = (x_i - \\xi_k)_+\\), capture local deviations after each knot \\(\\xi_k\\).\nThe notation \\((\\cdot)_+\\) denotes the “positive part,” meaning \\(\\max\\{0, \\cdot\\}\\). Therefore, \\((x_i - \\xi_k)_+ = x_i - \\xi_k\\) if \\(x_i &gt; \\xi_k\\), and 0 otherwise.\n\n\nPiecewise Linear Behavior: Because \\(b_{k+1}(x_i)\\) only becomes non-zero when \\(x_i\\) exceeds the knot \\(\\xi_k\\), the spline behaves like a linear function with additional slopes kicking in after each knot.\n\n\nEssentially, below the smallest knot, the model is a simple linear function of \\(x_i\\). Once \\(x_i\\) passes a knot \\(\\xi_k\\), the corresponding term \\((x_i - \\xi_k)_+\\) begins to contribute, allowing the slope to change. This creates segments of potentially different slopes while maintaining continuity at the knots.\n\n\n\nContinuity at Knots: Despite having distinct linear segments, the spline remains continuous at each \\(\\xi_k\\). The continuity follows naturally from how \\((\\cdot)_+\\) is defined. At a knot, \\((x_i - \\xi_k)_+\\) transitions from 0 to a linear increase, ensuring no jumps in the fitted function.\nRelevance\n\n\nFlexibility: Linear splines allow for piecewise changes in slope rather than forcing a single global linear relationship. This can capture more nuanced relationships between predictors and responses.\nInterpretability: Each knot \\(\\xi_k\\) marks a point where the slope can adjust, making it straightforward to interpret how the effect of \\(x_i\\) differs below and above that knot.\nComparison to Polynomials: Unlike higher-order polynomials, splines can avoid the global distortion that arises from polynomial terms. A single outlier or a data pattern in one region does not overly influence the fit across the entire range of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines-example",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines: Example",
    "text": "Linear Splines: Example\n\nTo illustrate a linear spline with one knot at \\(x = 50\\), suppose we have a response variable \\(y\\) (e.g., a person’s yearly wage) and a single predictor \\(x\\) (e.g., age):\n\n\n\nDefine the Basis Functions: We choose to place a single knot at \\(x = 50\\). According to the slide’s notation, the basis functions are:\n\n\n\\[\n   b_1(x_i) = x_i, \\quad\n   b_2(x_i) = (x_i - 50)_{+} \\,=\\,\n   \\begin{cases}\n     x_i - 50, & \\text{if } x_i &gt; 50,\\\\\n     0, & \\text{if } x_i \\le 50.\n   \\end{cases}\n\\]\n\n\nSpecify the Model: The corresponding linear spline model is:\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 \\, b_1(x_i) + \\beta_2 \\, b_2(x_i) + \\epsilon_i,\n\\]\n\n\nor more explicitly:\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 \\, x_i + \\beta_2 \\, (x_i - 50)_{+} + \\epsilon_i.\n\\]\n\n\n\n3.For \\(x_i \\le 50\\):\n\n\n\\((x_i - 50)_+ = 0\\). Hence, the model reduces to\n\n\n\\[\n     y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\n\nwhich is a simple linear relationship with slope \\(\\beta_1\\) for ages up to 50.\n\n\n4.For \\(x_i &gt; 50\\):\n\n\n\\((x_i - 50)_+ = x_i - 50\\). Thus, the model becomes\n\n\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - 50) + \\epsilon_i \\\\\n    &= [\\beta_0 - 50 \\beta_2] + (\\beta_1 + \\beta_2) x_i + \\epsilon_i\n\\end{aligned}\n\\]\n\n\nmeaning the slope for ages beyond 50 is \\(\\beta_1 + \\beta_2\\). The intercept adjusts accordingly to ensure the function remains continuous at \\(x=50\\).\n\n\n\n\nLocal Flexibility: Below 50, the effect of age on wage is governed by \\(\\beta_1\\). Above 50, the slope can change to \\(\\beta_1 + \\beta_2\\).\nContinuity at 50: Because the spline is forced to match up at \\(x = 50\\), there is no abrupt jump in the fitted curve.\nSimplicity of Implementation: We only introduced one additional term \\((x_i - 50)_+\\) to capture the potential change in slope after age 50."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines-visualization",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines Visualization",
    "text": "Linear Splines Visualization\n\n\n\n\n\n\n\n\n\n\n\nTop plot: shows two linear fits over the domain \\(0 \\le x \\le 1\\). The blue line represents a single global linear function (extending as the dashed line beyond the knot at \\(x = 0.6\\)), whereas the orange line demonstrates how adding a spline basis function allows the slope to change precisely at \\(x = 0.6\\).\nBottom plot: displays the corresponding basis function \\(b(x) = (x - 0.6)_{+}\\), which is defined to be zero for \\(x \\le 0.6\\) and increases linearly for \\(x &gt; 0.6\\). Because \\(b(x)\\) starts at zero at the knot, it does not introduce a jump—thus ensuring continuity—but it permits the slope to differ on either side of \\(x = 0.6\\).\n\nBy including this basis function (and a coefficient for it) in a linear model, one can capture a “bend” or change in slope at the specified knot. More generally, introducing additional such functions at different knots yields a piecewise linear model that remains continuous but adapts its slope in each region."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nA cubic spline with knots at \\(\\xi_k, \\, k = 1, \\dots, K\\) is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.\nAgain we can represent this model with truncated power basis functions:\n\\[\ny_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+3} b_{K+3}(x_i) + \\epsilon_i,\n\\]\n\\(b_1(x_i) = x_i,\\)\n\\(b_2(x_i) = x_i^2,\\)\n\\(b_3(x_i) = x_i^3,\\)\n\\(b_{k+3}(x_i) = (x_i - \\xi_k)_+^3, \\quad k = 1, \\dots, K\\)\n\nwhere\n\\[\n(x_i - \\xi_k)_+^3 =\n\\begin{cases}\n(x_i - \\xi_k)^3 & \\text{if } x_i &gt; \\xi_k, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines-visualization",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines Visualization",
    "text": "Cubic Splines Visualization"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-splines",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\nA natural cubic spline extrapolates linearly beyond the boundary knots. This adds \\(4 = 2 \\times 2\\) extra constraints, and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-spline",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-spline",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Natural Cubic Spline",
    "text": "Natural Cubic Spline"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#knot-placement",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#knot-placement",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Knot Placement",
    "text": "Knot Placement\n\n\nOne strategy is to decide \\(K\\), the number of knots, and then place them at appropriate quantiles of the observed \\(X\\).\nA cubic spline with \\(K\\) knots has \\(K + 4\\) parameters or degrees of freedom.\nA natural spline with \\(K\\) knots has \\(K\\) degrees of freedom.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure: Comparison of a degree-14 polynomial and a natural cubic spline, each with 15df."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nSplines and piecewise polynomials build on the concept of dividing the predictor domain into segments, offering enhanced flexibility and precision over traditional polynomial models. By using different polynomial functions in various regions and ensuring smooth transitions at knots, these methods provide a powerful approach to capturing non-linear relationships in data.\n\nPiecewise Polynomials: Replace a single global polynomial with multiple polynomials fitted across segments, resulting in a more tailored and adaptable fit.\n\nContinuity Constraints: Enforcing continuity at knots ensures smooth transitions between polynomial segments, avoiding abrupt changes and better representing underlying trends.\n\nNatural Cubic Splines: Add further constraints by requiring smoothness in the function’s second derivative at knots and controlling boundary behavior, ensuring robustness and reducing overfitting at extremes.\n\nFlexibility and Local Adaptation: Piecewise polynomials adapt to local data behavior, making them particularly effective for capturing non-linear trends and abrupt changes in data. This adaptability is essential in real-world datasets that exhibit complex patterns.\n\nStrategic Knot Placement: Properly placing knots, such as at quantiles of the predictor variable, ensures that each segment has adequate data coverage, resulting in more reliable and stable estimates.\n\nImportance of Boundary Constraints: Natural cubic splines excel at controlling the function’s behavior near boundaries, mitigating overfitting and improving interpretability in regions with sparse data.\n\nAdvantages Over Traditional Polynomials: Splines overcome the limitations of global polynomial fitting, such as excessive wiggliness or global influence of outliers, providing smoother, more interpretable fits that align closely with the data’s natural structure.\nConclusion: Splines offer a superior method for non-linear modeling, combining flexibility, local adaptation, and smoothness, making them an indispensable tool in modern statistical analysis."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-adding-mathematical-rigor",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-adding-mathematical-rigor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: Adding Mathematical Rigor",
    "text": "Smoothing Splines: Adding Mathematical Rigor\n\nSmoothing splines are an extension of regression splines but arise in a different context. They are derived by minimizing a residual sum of squares criterion subject to a penalty for roughness, balancing data fidelity and smoothness.\nThe optimization criterion for fitting a smooth function \\(g(x)\\) to data is given by:\n\\[\n\\text{minimize}_{g \\in S}  \\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2 + \\lambda \\int \\left( g''(t) \\right)^2 dt\n\\]\nKey Components:\n\nResidual Sum of Squares (RSS): The first term, \\(\\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2\\), ensures that the function \\(g(x)\\) fits the observed data points \\((x_i, y_i)\\) closely.\nRoughness Penalty: The second term, \\(\\lambda \\int \\left( g''(t) \\right)^2 dt\\), penalizes the curvature of \\(g(x)\\) by integrating the square of its second derivative. This term controls how “wiggly” the function is and enforces smoothness.\nTuning Parameter \\(\\lambda\\): The parameter \\(\\lambda \\geq 0\\) determines the tradeoff between fit and smoothness:\n\nWhen \\(\\lambda = 0\\): The penalty vanishes, and the function \\(g(x)\\) becomes fully flexible, interpolating all data points.\nAs \\(\\lambda \\to \\infty\\): The penalty dominates, and \\(g(x)\\) becomes increasingly smooth, eventually reducing to a simple linear function.\n\n\n\nIntuition: Smoothing splines allow for flexible, smooth fits that adapt to the structure of the data while avoiding excessive overfitting. By tuning \\(\\lambda\\), analysts can strike a balance between capturing meaningful patterns and maintaining a smooth, interpretable curve."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\n\nSmoothing splines yield a natural cubic spline solution with a knot at every unique value of \\(x_i\\). Rather than choosing specific knot locations, practitioners select a single tuning parameter \\(\\lambda\\), which balances data fidelity and smoothness through the roughness penalty.\n\nAutomatic Knot Placement: By placing a knot at each unique \\(x_i\\), smoothing splines avoid the need to manually choose knot positions. The smoothness parameter \\(\\lambda\\) becomes the key lever for controlling model complexity.\nSmoother Matrix: The fitted values can be written as\n\n\n\\[\n    \\hat{g}_\\lambda = \\mathbf{S}_\\lambda\\,\\mathbf{y},\n\\]\nwhere \\(\\mathbf{S}_\\lambda\\) is an \\(n \\times n\\) matrix determined by the locations of the \\(x_i\\) and the penalty \\(\\lambda\\).\n\n\nEffective Degrees of Freedom: The complexity of the smoothing spline is measured by the effective degrees of freedom, computed as\n\n\n\\[\n    \\text{df}_\\lambda = \\sum_{i=1}^n \\{\\mathbf{S}_\\lambda\\}_{ii},\n\\]\ni.e., the trace of the smoother matrix. A larger trace indicates a more flexible fit, while a smaller trace corresponds to a smoother, less flexible curve."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-choosing-lambda",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-choosing-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines — Choosing \\(\\lambda\\)",
    "text": "Smoothing Splines — Choosing \\(\\lambda\\)\n\nSpecifying \\(\\text{df}\\) Instead of \\(\\lambda\\)\n\nA key advantage of smoothing splines is the option to specify the effective degrees of freedom (\\(\\text{df}\\)) directly, rather than choosing \\(\\lambda\\) outright. This allows you to control the spline’s flexibility (or smoothness) in more intuitive terms.\n\n\nLOOCV\n\nAlternatively, \\(\\lambda\\) can be selected to minimize the leave-one-out (LOO) cross-validation error, given by\n\n\n\n\\[\n\\text{RSS}_{\\text{cv}}(\\lambda)\n      = \\sum_{i=1}^n \\Bigl( y_i - \\hat{g}_\\lambda^{(-i)}(x_i) \\Bigr)^2\n      = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2,\n\\]\nwhere \\(\\hat{g}_\\lambda^{(-i)}\\) denotes the fitted function obtained by leaving out the \\(i\\)-th observation, and \\(\\mathbf{S}_\\lambda\\) is the smoother matrix.\nWhether you fix the degrees of freedom directly or optimize \\(\\lambda\\) via cross-validation, the goal is to strike a balance between fidelity to the data and smoothness of the spline."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-specifying-degrees-of-freedom",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-specifying-degrees-of-freedom",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: Specifying Degrees of Freedom",
    "text": "Smoothing Splines: Specifying Degrees of Freedom\n\n\nEffective Degrees of Freedom (df)\n\n\nInstead of selecting \\(\\lambda\\) directly, we choose an equivalent “degrees of freedom” value, \\(\\text{df}_\\lambda\\).\nThe relationship between \\(\\lambda\\) and \\(\\text{df}\\) is such that increasing \\(\\lambda\\) (stronger penalty) lowers \\(\\text{df}\\), leading to a smoother (less flexible) spline.\n\n\nAdvantages\n\n\nIntuitive Interpretation: \\(\\text{df}\\) tells us how many “parameters” (roughly) our model is using, making it easier to grasp complexity.\nDirect Control: Analysts may already have an idea of how flexible their model needs to be, and can set \\(\\text{df}\\) accordingly.\n\n\nPractical Tip\n\n\nEmpirically, one might try a range of df values (e.g., 4, 5, 6, …) and pick the one that balances interpretability and fit (potentially guided by an information criterion or cross-validation)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-loocv-to-specify-lambda",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-loocv-to-specify-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: LOOCV to Specify \\(\\lambda\\)",
    "text": "Smoothing Splines: LOOCV to Specify \\(\\lambda\\)\n\n\n\n\nWe can use Cross-validation to choose \\(\\lambda\\).\n\n\nFor each candidate \\(\\lambda\\), we leave out one data point at a time, fit the spline, and measure the prediction error on the held-out point.\n\n\nMathematical Form\n\n\n\\[\n     \\text{RSS}_{\\text{cv}}(\\lambda)\n       = \\sum_{i=1}^n \\left(y_i - \\hat{g}_\\lambda^{(-i)}(x_i)\\right)^2,\n\\]\n\n\nwhere \\(\\hat{g}_\\lambda^{(-i)}\\) denotes the spline fitted without the \\(i\\)-th data point.\nIn practice, this can be computed more efficiently via the influence matrix \\(\\mathbf{S}_\\lambda\\):\n\n\n\\[\n       \\text{RSS}_{\\text{cv}}(\\lambda)\n         = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2.\n\\]\n\n\n\nAdvantages\n\n\nData-Driven: No need to guess \\(\\text{df}\\) or \\(\\lambda\\); we let cross-validation find the best trade-off.\nRobust: Tends to choose a value that generalizes well to new data.\n\n\nPractical Tip\n\n\nEvaluate \\(\\text{RSS}_{\\text{cv}}\\) for a grid of \\(\\lambda\\) values (e.g., \\(\\lambda \\in \\{0.01, 0.1, 1, 10\\}\\)) and pick the one that minimizes the cross-validation error."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-spline-degrees-of-freedom",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-spline-degrees-of-freedom",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Spline: Degrees of Freedom",
    "text": "Smoothing Spline: Degrees of Freedom"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#local-regression-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#local-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Local Regression",
    "text": "Local Regression\n\nLocal regression is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.\n\n\n\n\n\n\n\n\n\nWith a sliding weight function, we fit separate linear fits over the range of \\(X\\) by weighted least squares.\n\n\nIt is a smart way to fit non-linear functions by fitting local linear functions on the data!"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#generalized-additive-models-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#generalized-additive-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\nGeneralized additive models (GAMs) allow us to extend the methods covered in this lecture to deal with multiple predictors.\nGAMs allows for flexible nonlinearities in several variables, but retains the additive structure of linear models.\n\\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\cdots + f_p(x_{ip}) + \\epsilon_i.\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gam-details",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gam-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAM Details",
    "text": "GAM Details\n\nCan fit a GAM simply using, e.g., natural splines:\nCoefficients are not that interesting; fitted functions are.\n\n\n\nCan mix terms — some linear, some nonlinear — and compare models with ANOVA.\nCan use smoothing splines or local regression as well:\n\nGAMs are additive, although low-order interactions can be included in a natural way using, e.g., bivariate smoothers."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gams-for-classification",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gams-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAMs for Classification",
    "text": "GAMs for Classification\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p).\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-2",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nPolynomial Regression\n\nExpands a linear model by adding powers of the predictor(s).\n\nSimple to implement but can behave poorly at boundaries.\n\nStep Functions\n\nPartitions the predictor space into intervals; fits piecewise constant segments.\n\nHighly interpretable but can appear “blocky” and requires choosing cutpoints.\n\nRegression Splines\n\nUses piecewise polynomials, constrained to meet smoothly at knots.\n\nGreater flexibility than polynomials or step functions, with continuity at knots.\n\nNatural Cubic Splines\n\nEnforce additional constraints for boundary behavior.\n\nAllow more robust modeling without overfitting at the extremes.\n\n\n\n\n\n\nSmoothing Splines\n\nAutomates knot selection (a knot at each data point); regularizes via a penalty on curvature.\n\nBalances fidelity to data (\\(\\mathrm{RSS}\\)) and smoothness (\\(\\lambda\\)).\n\n\\(\\lambda\\) (or effective degrees of freedom) is chosen via cross-validation or by specifying df directly.\n\nLocal Regression\n\nFits a series of local linear models weighted by proximity to each target \\(x\\).\nUseful for highly variable, nonlinear relationships.\n\nGeneralized Additive Models (GAMs)\n\nAllows non-linear fits in multiple predictors yet maintains an additive structure.\n\nSupports smooth terms (splines, local regression) combined with linear terms.\n\n\nBottom Line\n\nBy moving beyond a strictly linear framework—using polynomials, splines, or GAMs—analysts can more accurately capture complex patterns while retaining interpretability and control over model complexity."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overview",
    "href": "lecture_slides/01_introduction/01_introduction.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nSupervised Learning\nUnsupervised Learning\nStatistical Learning Overview\n\nWhat is Statistical Learning?\nParametric and Structured Models\nAssessing Model Accuracy\nClassification Problems\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Quantitative Methods Department at the Mitch. Daniels School of Business at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#students",
    "href": "lecture_slides/01_introduction/01_introduction.html#students",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Students",
    "text": "Students\n\n\nWhat do you expect to gain from this Predictive Analytics course?\n\n\n\nIt is your turn! - 10 minutes\nPresent yourself to your left/right colleague and ask:\nCollect her/his answer and submit your first Participation Assignment!"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\n\n\n\nCourse Info:\n\nBrightspace - Official.\nCourse Webpage - Supplementary.\n\nSyllabus\n\nClass Times & Location: check the course syllabus.\n\nOffice Hours: check the course syllabus for group and individual appointments.\n\nSchedule and Materials:\n\nPodcast (before class)\n\nRequired Readings (before class)\n\nLecture Slides (before class)\n\nLecture Video (during class)\n\nBook labs (during/after class)\n\nSupplementary Material (after class)\n\n\n\n\nCourse Tracks\n\nStandard Track\n\nExternal Case Competition (bonus)\n\nAssessments\n\nAttendance\n\nParticipation\n\nQuizzes\n\nHomework\n\nCourse Case Competition\n\nFinal Project"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "href": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Spam Detection",
    "text": "Spam Detection\n\n\nData from 4601 emails sent to an individual (named George, at HP Labs, before 2000). Each is labeled as spam or email.\nGoal: build a customized spam filter.\nInput features: relative frequencies of 57 of the most commonly occurring words and punctuation marks in these email messages.\n\n\n\n\nWord\nSpam\nEmail\n\n\n\n\ngeorge\n0.00\n1.27\n\n\nyou\n2.26\n1.27\n\n\nhp\n0.02\n0.90\n\n\nfree\n0.52\n0.07\n\n\n!\n0.51\n0.11\n\n\nedu\n0.01\n0.29\n\n\nremove\n0.28\n0.01\n\n\n\nAverage percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "href": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Zip Code",
    "text": "Zip Code\n\nIdentify the numbers in a handwritten zip code."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Netflix Prize",
    "text": "Netflix Prize\n\n\n\n\n\n\n\n\n\n\n\nVideo: Winning the Netflix Prize\nNetflix Prize - Wiki"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\n\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#notation",
    "href": "lecture_slides/01_introduction/01_introduction.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\n\n\n\n\n\n\n\n\n\n\nThere can be many \\(Y\\) values at \\(X=4\\). A good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\nIs also defined for a vector \\(\\mathbf{X}\\).\n\n\\[\nf(\\mathbf{x}) = f(x_1, x_2, x_3) = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\n\n\nIs the ideal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{is the function that minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error.\n\nEven if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is a distribution of possible \\(Y\\) values.\n\n\n\n\n\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#supervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#supervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\n\n\nDefinition:\nLearning a mapping from inputs \\(X\\) to an output \\(Y\\) using labeled data.\nThe algorithm is supervised because the correct answers are known during training.\nGoal:\n\nPredict \\(Y\\) for new unseen \\(X\\).\n\nMinimize prediction error.\n\n\n\n\nExamples of Methods:\n\nLinear Regression, Logistic Regression\n\nDecision Trees, Random Forests, SVMs\n\nNeural Networks\n\nApplications:\n\nPredicting stock prices\n\nDiagnosing diseases\n\nSpam email detection"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\nDefinition:\nLearning the structure or patterns in data without labeled outputs.\nThe algorithm is unsupervised because no outcome variable guides the learning.\nGoal:\n\nDiscover hidden structures in \\(X\\).\n\nGroup, reduce, or represent data meaningfully.\n\n\n\n\nExamples of Methods:\n\nClustering: k-means, hierarchical clustering\n\nDimensionality Reduction: PCA, t-SNE\n\nApplications:\n\nCustomer segmentation\n\nMarket basket analysis\n\nReducing dimensionality of large datasets\n\nCharacteristics:\n\nDifficult to know how well we are doing.\n\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "href": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large.\n\nReason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point \\((0, 0)\\).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point \\((0, 0)\\).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBottom panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\nRed points are simulated values for income from the model:\n\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\nLinear regression model fit to the simulated data:\n\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\n\nHere we use a technique called a thin-plate spline to fit a flexible surface. We control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "href": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. We tuned the parameter all the way down to zero and this surface actually goes through every single data point.\n\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "href": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso.\n\nIntermediate: Least squares, Generalized Additive Models, Trees.\n\nHigh flexibility: Support Vector Machines, Deep Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\), the Mean Squared Error (MSE):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\nBias (tends to underfit)\n\nDefinition: The error introduced by approximating a real-world problem with a simplified model. High bias implies underfitting.\n\nImplication: High bias ⇒ the model misses important patterns (systematic error).\n\nTypical causes: Too-simple model (e.g., overly rigid assumptions), insufficient features, heavy regularization.\n\n\nVariance (tends to overfit)\n\nDefinition: The amount by which the model’s prediction would change if trained on a different training dataset. High variance implies overfitting.\n\nImplication: High variance ⇒ the model is overly sensitive to noise (unstable across samples).\n\nTypical causes: Overly flexible model, too many features vs. observations, weak regularization.\n\n\n\n\n\n\n\n\nThe trade-off we manage\n\n\n\nIncreasing flexibility ↓ bias but ↑ variance.\n\nGoal: choose complexity that minimizes expected test error.\n\n\n\n\nDecomposition of expected test MSE\n\\[\n\\mathbb{E}\\big[(Y-\\hat f(X))^2\\big]\n   \\;=\\; \\underbrace{\\big(\\mathrm{Bias}[\\hat f(X)]\\big)^2}_{\\text{misspecification}}\n   \\;+\\; \\underbrace{\\mathrm{Var}[\\hat f(X)]}_{\\text{sensitivity}}\n   \\;+\\; \\underbrace{\\sigma^2}_{\\text{irreducible noise}}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Panel: Model Fits\n\nBlack Curve: The true generating function, representing the underlying relationship we want to estimate.\nData Points: Observations generated from the black curve, with added noise (error).\nFitted Models:\n\nOrange Line: A simple linear model (low flexibility).\nBlue Line: A moderately flexible model, likely a spline or thin plate spline.\nGreen Line: A highly flexible model that closely fits the data points but may overfit.\n\n\nKey Insight:\nThe green model captures the data points well but risks overfitting, while the orange model is too rigid and misses the underlying structure. The blue model strikes a balance.\n\nBottom Panel: Mean Squared Error (MSE)\n\nGray Curve: Training data MSE.\n\nDecreases consistently as flexibility increases.\nFlexible models fit the training data well, but this does not generalize to test data.\n\nRed Curve: Test data MSE across models of increasing flexibility.\n\nStarts high for rigid models (orange line).\nDecreases to a minimum (optimal model complexity, blue line).\nIncreases again for overly flexible models (green line), due to overfitting.\n\n\nKey Takeaway:\nThere is an optimal model complexity (the “magic point”) where test data MSE is minimized. Beyond this point, models become overly complex and generalization performance deteriorates."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off: Other Examples",
    "text": "Bias-Variance Trade-off: Other Examples\n\n\n\n\nHere, the truth is smoother, so smoother fits and linear models perform well.\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off of the Examples",
    "text": "Bias-Variance Trade-off of the Examples\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\nStatistical Learning and Predictive Analytics\n\nGoal: Build models to predict outcomes and understand relationships between inputs (predictors) and responses.\nSupervised Learning: Focuses on predicting \\(Y\\) (response) using \\(X\\) (predictors) via models like regression and classification.\nUnsupervised Learning: Focuses on finding patterns in data without predefined responses (e.g., clustering).\n\nBias-Variance Trade-off\n\nKey Trade-off: Model flexibility affects bias and variance:\n\nHigh flexibility → Low bias but high variance (overfitting).\nLow flexibility → High bias but low variance (underfitting).\n\nGoal: Find the optimal flexibility that minimizes test error.\n\n\nTechniques and Applications\n\nParametric Models:\n\nSimpler and interpretable (e.g., linear regression).\nOften used as approximations.\n\nFlexible Models:\n\nHandle complex patterns (e.g., splines, SVMs, deep learning).\nRequire careful tuning to avoid overfitting.\n\n\nPractical Considerations\n\nAssessing Model Accuracy:\n\nUse test data to calculate MSE.\nBalance between training performance and generalizability.\n\n\nKey Challenges\n\nCurse of Dimensionality:\n\nHigh-dimensional data affects distance-based methods like KNN.\nLarger neighborhoods needed, losing “locality.”"
  },
  {
    "objectID": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#cleaning-the-gridlines",
    "href": "lecture_slides/09_data_communication_poster/09_data_communication_poster.html#cleaning-the-gridlines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cleaning the Gridlines",
    "text": "Cleaning the Gridlines"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#overview",
    "href": "lecture_slides/03_classification/03_classification.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nIntroduction to Classification\nLinear versus Logistic Regression\nMaking Predictions\nMultinomial Logistic Regression\n\n\n\nDiscriminant Analysis\nLinear Discriminant Analysis when \\(p &gt; 1\\)\nTypes of errors\nOther Forms of Discriminant Analysis\nNaive Bayes\nGeneralized Linear Models\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#recap",
    "href": "lecture_slides/03_classification/03_classification.html#recap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\nWhat is the fundamental difference between a regression problem and a classification problem in supervised learning, based on the nature of the outcome variable?\n\n\nRegression predicts a continuous numeric outcome (quantitative response). Classification predicts a discrete category/label (qualitative response, e.g., binary or multiclass).\n\n\n\n\nLoad the Advertising.csv and obtain descriptive statistics:\n\n\ndf = pd.read_csv(Advertising.csv)df.describe\n\n\n\n\nWhat is a possible way to check if there is a relationship between predictors and an outcome variable? performance?\n\n\nRegression Models.\n\n\n\n\n\nIn a regression model, how can we determine the strength of the relationship between predictors and our response variable?\n\n\nCan be assessed by examining the size and statistical significance of the predictors coefficients, the model’s goodness-of-fit, and its predictive performance. Large and statistically significant coefficients (low p-values with narrow confidence intervals) indicate that changes in predictors are strongly associated with changes in the outcome variable. R-squared and adjusted R-squared values quantify how much of the variance in the outcome variable is explained by the predictors.\n\n\n\n\nHow to discover which predictor is associated with the outcome variable?\n\n\nCheck coefficients, p-values, and confidence intervals for statistical significance. In multiple regression, standardized coefficients and model fit (R², adjusted R²) indicate relative importance.\n\n\n\n\nHow can you determine if there is synergy among predictors in terms of the outcome variable?\n\n\nSynergy among predictors can be determined by testing for interaction effects in a regression model, where you add product terms (e.g., TV × Radio) alongside the main predictors. A significant interaction coefficient indicates that the effect of one predictor on the outcome depends on the level of another, revealing synergy (positive or negative)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#what-is-a-classification-problem",
    "href": "lecture_slides/03_classification/03_classification.html#what-is-a-classification-problem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is a classification problem?",
    "text": "What is a classification problem?\n\n\n\n\nClassification involves categorizing data into predefined classes or groups based on their features."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#classification",
    "href": "lecture_slides/03_classification/03_classification.html#classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification",
    "text": "Classification\n\nQualitative variables take values in an unordered set \\(C\\), such as:\n\n\\(\\text{eye color} \\in \\{\\text{brown}, \\text{blue}, \\text{green}\\}\\)\n\\(\\text{email} \\in \\{\\text{spam}, \\text{ham}\\}\\)\n\nGiven a feature vector \\(X\\) and a qualitative response \\(Y\\) taking values in the set \\(C\\), the classification task is to build a function \\(C(X)\\) that takes as input the feature vector \\(X\\) and predicts its value for \\(Y\\); i.e. \\(C(X) \\in C\\).\nOften, we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(C\\).\n\nFor example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification as fraudulent or not."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-credit-card-default",
    "href": "lecture_slides/03_classification/03_classification.html#example-credit-card-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Card Default",
    "text": "Example: Credit Card Default\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot of income vs. balance with markers indicating whether a person defaulted (e.g., “+” for defaulted, “o” for not defaulted).\n\n\n\n\n\n\n\n\n\n\n\nBoxplots comparing balance and income for default (“Yes”) vs. no default (“No”)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#can-we-use-linear-regression",
    "href": "lecture_slides/03_classification/03_classification.html#can-we-use-linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can we use Linear Regression?",
    "text": "Can we use Linear Regression?\nSuppose for the Default classification task that we code:\n\\[\nY =\n\\begin{cases}\n0 & \\text{if No} \\\\\n1 & \\text{if Yes.}\n\\end{cases}\n\\]\nCan we simply perform a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y} &gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression does a good job as a classifier and is equivalent to linear discriminant analysis, which we discuss later.\nSince in the population \\(E(Y|X = x) = \\Pr(Y = 1|X = x)\\), we might think that regression is perfect for this task.\nHowever, linear regression might produce probabilities less than zero or greater than one. Logistic regression is more appropriate."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-probability-of-default",
    "href": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-probability-of-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression: Probability of Default",
    "text": "Linear versus Logistic Regression: Probability of Default\n\n\nThe orange marks indicate the response \\(Y\\), either 0 or 1.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression does not estimate \\(\\Pr(Y = 1|X)\\) well.\n\n\n\nLogistic regression seems well-suited to the task."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-regression-continued",
    "href": "lecture_slides/03_classification/03_classification.html#linear-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression continued",
    "text": "Linear Regression continued\n\n\nNow suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke;} \\\\\n2 & \\text{if drug overdose;} \\\\\n3 & \\text{if epileptic seizure.}\n\\end{cases}\n\\]\nThis coding suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure.\nLinear regression is not appropriate here. Multiclass Logistic Regression or Discriminant Analysis are more appropriate."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet’s write \\(p(X) = \\Pr(Y = 1|X)\\) for short and consider using balance to predict default. Logistic regression uses the form:\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\n\\((e \\approx 2.71828)\\) is a mathematical constant Euler’s number.\nIt is easy to see that no matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take, \\(p(X)\\) will have values between 0 and 1.\n\nA bit of rearrangement gives:\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X.\n\\]\nThis monotone transformation is called the log odds or logit transformation of \\(p(X)\\). (By log, we mean natural log: \\(\\ln\\).)"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\nStep 1: Express \\(1 - p(X)\\)\n\nSince \\(p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\), we can write:\n\\[\n1 - p(X) = 1 - \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\nSimplify:\n\\[\n1 - p(X) = \\frac{1 + e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-1",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\nStep 2: Compute the Odds\n\nThe odds are defined as:\n\\[\n\\frac{p(X)}{1 - p(X)}\n\\]\nSubstitute \\(p(X)\\) and \\(1 - p(X)\\):\n\\[\n\\frac{p(X)}{1 - p(X)} =\n\\frac{\\dfrac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}}\n{\\dfrac{1}{1 + e^{\\beta_0 + \\beta_1 X}}}\n\\]\nSimplify:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-2",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\nStep 3: Take the Log of the Odds\n\nTaking the natural logarithm:\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\log\\!\\Bigl(e^{\\beta_0 + \\beta_1 X}\\Bigr)\n\\]\nSimplify using the log property \\(\\log(e^x) = x\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-3",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\nConclusion\n\nThe final transformation shows that the log-odds (logit) of \\(p(X)\\) is a linear function of \\(X\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-1",
    "href": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression",
    "text": "Linear versus Logistic Regression\n\n\n\n\n\n\n\n\n\n\nLogistic regression ensures that our estimate for \\(p(X)\\) lies between 0 and 1."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nWe use maximum likelihood to estimate the parameters.\n\\[\n\\ell(\\beta_0, \\beta) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0} (1 - p(x_i)).\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a model by maximizing the likelihood function, which measures how likely the observed data is given the parameters.\n\nThe likelihood function is based on the probability distribution of the data. If you assume that the data points are independent, the likelihood function is the product of the probabilities of each observation.\n\nConsidering a data series of observed zeros and ones, and a model for the probabilities involving parameters (e.g., \\(\\beta_0\\) and \\(\\beta_1\\)), for any specific parameter values, we can compute the probability of observing the data.\nSince the observations are assumed to be independent, the joint probability of the observed sequence is the product of the probabilities for each observation. For each “1,” we use the model’s predicted probability, \\(p(x_i)\\), and for each “0,” we use \\(1 - p(x_i)\\).\nThe goal of MLE is to find the parameter values that maximize this joint probability, as they make the observed data most likely to have occurred."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSuppose you are flipping a coin, and you observe 5 heads out of 10 flips. The coin’s bias (the probability of heads) is \\(p\\), and you want to estimate \\(p\\).\nThe probability of observing a single outcome (heads or tails) follows the Bernoulli distribution:\n\\[\nP(\\text{Heads or Tails}) = p^x (1-p)^{1-x}, \\quad \\text{where } x = 1 \\text{ for heads, } x = 0 \\text{ for tails.}\n\\]\nFor 10 independent flips, the likelihood function is:\n\\[\nL(p) = P(\\text{data} \\mid p) = \\prod_{i=1}^{10} p^{x_i}(1-p)^{1-x_i}.\n\\]\nIf there are 5 heads (\\(x=1\\)) and 5 tails (\\(x=0\\)):\n\\[\nL(p) = p^5 (1-p)^5.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSimplify with the Log-Likelihood\nSince multiplying probabilities can result in very small numbers, we take the logarithm of the likelihood (log-likelihood). The logarithm simplifies the product into a sum:\n\\[\n\\ell(p) = \\log L(p) = \\log \\left(p^5 (1-p)^5\\right) = 5\\log(p) + 5\\log(1-p).\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nMaximize the Log-Likelihood\nTo find the value of \\(p\\) that maximizes \\(\\ell(p)\\), take the derivative of the log-likelihood with respect to \\(p\\) and set it to zero:\n\\[\n\\frac{\\partial\\ell(p)}{\\partial p} = \\frac{5}{p} - \\frac{5}{1-p} = 0.\n\\]\nSimplify:\n\\[\n\\frac{5}{p} = \\frac{5}{1-p}.\n\\]\nSolve for \\(p\\):\n\\[\n1 - p = p \\quad \\Rightarrow \\quad 1 = 2p \\quad \\Rightarrow \\quad p = 0.5.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nTo confirm that \\(p = 0.5\\) is the maximum, you can check the second derivative of the log-likelihood (concavity) or use numerical methods.\nIn our example, \\(p = 0.5\\) makes sense intuitively because the data (5 heads out of 10 flips) suggests the coin is unbiased.\nThe maximum likelihood estimate of \\(p\\) is \\(0.5\\). The MLE method finds the parameter values that make the observed data most likely, given the assumed probability model."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\nAssumptions:\n\nData \\(x_1, x_2, \\dots, x_n\\) are drawn from a normal distribution with:\n\n\\[\n  f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\]\n\nAssume \\(\\sigma\\) is known (say, \\(\\sigma = 1\\)) and we want to estimate \\(\\mu\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\nThe likelihood for \\(n\\) independent observations is:\n\\[\nL(\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x_i - \\mu)^2}{2}}\n\\]\nTaking the natural log:\n\\[\n\\ell(\\mu) = \\log L(\\mu) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi) - \\frac{(x_i - \\mu)^2}{2} \\right]\n\\]\nSimplify (since \\(-\\frac{1}{2} \\log(2\\pi)\\) is constant):\n\\[\n\\ell(\\mu) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\nDifferentiate with respect to \\(\\mu\\):\n\\[\n\\frac{\\partial \\ell(\\mu)}{\\partial \\mu} = -\\sum_{i=1}^n (x_i - \\mu)\n\\] Set this to zero:\n\\[\n\\sum_{i=1}^n (x_i - \\mu) = 0\n\\]\nSolve for \\(\\mu\\):\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nThe MLE for the mean \\(\\mu\\) is simply the sample mean:\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#making-predictions-1",
    "href": "lecture_slides/03_classification/03_classification.html#making-predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Making Predictions",
    "text": "Making Predictions\nMost statistical packages can fit linear logistic regression models by maximum likelihood.\nLogistic Regression Coefficients\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.6513\n0.3612\n-29.5\n&lt; 0.0001\n\n\nbalance\n0.0055\n0.0002\n24.9\n&lt; 0.0001\n\n\n\n\nWhat is our estimated probability of default for someone with a credit card balance of $1000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}} = 0.006\n\\]\nWith a a credit card balance of $2000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 2000}}{1 + e^{-10.6513 + 0.0055 \\times 2000}} = 0.586\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-student-predictor",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-student-predictor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Student Predictor",
    "text": "Logistic Regression with Student Predictor\nLet’s do it again, using student as the predictor.\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-3.5041\n0.0707\n-49.55\n&lt; 0.0001\n\n\nstudent \\(Yes\\)\n0.4049\n0.1150\n3.52\n0.0004\n\n\n\n\nPredicted Probabilities\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{Yes}) = \\frac{e^{-3.5041 + 0.4049 \\times 1}}{1 + e^{-3.5041 + 0.4049 \\times 1}} = 0.0431,\n\\]\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{No}) = \\frac{e^{-3.5041 + 0.4049 \\times 0}}{1 + e^{-3.5041 + 0.4049 \\times 0}} = 0.0292.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-several-variables",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-several-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Several Variables",
    "text": "Logistic Regression with Several Variables\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\]\n\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.8690\n0.4923\n-22.08\n&lt; 0.0001\n\n\nbalance\n0.0057\n0.0002\n24.74\n&lt; 0.0001\n\n\nincome\n0.0030\n0.0082\n0.37\n0.7115\n\n\nstudent Yes\n-0.6468\n0.2362\n-2.74\n0.0062\n\n\n\nWhy is the coefficient for student negative, while it was positive before?"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confounding",
    "href": "lecture_slides/03_classification/03_classification.html#confounding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\nRelationship between Y and X controlled for W\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confounding-1",
    "href": "lecture_slides/03_classification/03_classification.html#confounding-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.\nBut for each level of balance, students default less than non-students.\nMultiple logistic regression can tease this out."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#multinomial-logistic-regression-1",
    "href": "lecture_slides/03_classification/03_classification.html#multinomial-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nLogistic regression is frequently used when the response is binary, or \\(K = 2\\) classes. We need a modification when there are \\(K &gt; 2\\) classes. E.g. stroke, drug overdose, and epileptic seizure for the emergency room example.\nThe simplest representation uses different linear functions for each class, combined with the softmax function to form probabilities:\n\\[\n\\Pr(Y = k | X = x) = \\text{Softmax}(z_k) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^{K} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.\n\\]\n\nWe really only need \\(K - 1\\) functions (see the book for details).\nWe fit by maximizing the multinomial log-likelihood (cross-entropy) — a generalization of the binomial.\nAn example will given later in the course, when we fit the 10-class model to the MNIST digit dataset."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#what-is-the-softmax-function",
    "href": "lecture_slides/03_classification/03_classification.html#what-is-the-softmax-function",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the Softmax Function?",
    "text": "What is the Softmax Function?\n\nThe softmax function is used in multinomial logistic regression to convert raw scores (logits) into probabilities for multiple classes.\n\n\nLogits are the raw, untransformed output of the linear component in logistic regression. For a given class \\(k\\), the logit is defined as:\n\\[\nz_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\n\\]\nWhere:\n\n\\(z_k\\): The logit for class \\(k\\).\n\\(\\beta_{k0}\\): Intercept term.\n\\(\\beta_{kj}\\): Coefficients for predictor \\(x_j\\).\n\n\n\nSoftmax Definition:\nFor \\(K\\) classes and input \\(x\\), the softmax function is defined as:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{l=1}^K e^{z_l}}\n\\]\nWhere:\n\n\\(z_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\\): The linear score (logit) for class \\(k\\).\n\\(\\beta_{k0}, \\beta_{k1}, \\dots, \\beta_{kp}\\): Coefficients for class \\(k\\).\n\\(e^{z_k}\\): Exponentiated score for class \\(k\\), ensuring all values are positive.\n\n\n\n\nKey Features of the Softmax Function\n\nProbability Distribution: Outputs probabilities that sum to 1 across all \\(K\\) classes. \\(\\text{Pr}(Y = k \\mid X = x) = \\text{Softmax}(z_k)\\).\nNormalization: Normalizes logits by dividing each exponentiated logit by the sum of all exponentiated logits.\nHandles Multiclass Classification: Extends binary logistic regression to \\(K &gt; 2\\) classes."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-of-softmax-in-action",
    "href": "lecture_slides/03_classification/03_classification.html#example-of-softmax-in-action",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example of Softmax in Action",
    "text": "Example of Softmax in Action\n\n\n\nImagine classifying three emergency room conditions: Stroke, Drug Overdose, and Epileptic Seizure.\nSuppose the logits are: \\(z_{\\text{stroke}} = 2.5, \\quad z_{\\text{drug overdose}} = 1.0, \\quad z_{\\text{epileptic seizure}} = 0.5\\)\nThe probabilities are:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{e^{2.5} + e^{1.0} + e^{0.5}}\n\\]\n\nStep 1: Exponentiate the Logits\n\\(e^{z_{\\text{stroke}}} = e^{2.5} \\approx 12.182\\)\n\\(e^{z_{\\text{drug overdose}}} = e^{1.0} \\approx 2.718\\)\n\\(e^{z_{\\text{epileptic seizure}}} = e^{0.5} \\approx 1.649\\)\n\n\nStep 2: Compute the Denominator\n\\(\\sum_{l=1}^K e^{z_l} = e^{2.5} + e^{1.0} + e^{0.5}\\)\n\\(\\sum_{l=1}^K e^{z_l} \\approx 12.182 + 2.718 + 1.649 = 16.549\\)\n\n\n\nStep 3: Calculate the Probabilities\n\\(\\text{Pr}(\\text{stroke}) = \\frac{e^{z_{\\text{stroke}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{12.182}{16.549} \\approx 0.7366\\)\n\\(\\text{Pr}(\\text{drug overdose}) = \\frac{e^{z_{\\text{drug overdose}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{2.718}{16.549} \\approx 0.1642\\)\n\\(\\text{Pr}(\\text{epileptic seizure}) = \\frac{e^{z_{\\text{epileptic seizure}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{1.649}{16.549} \\approx 0.0996\\)\nThe output probabilities represent the likelihood of each condition, ensuring:\n\\[\n\\sum_{k=1}^3 \\text{Pr}(Y = k) = 1\n\\]\nWe have:\n\\[\n   0.7366 + 0.1642 + 0.0996 \\approx 1.000\n\\]\n\n\nConclusion\n\nThe softmax function translates raw scores into probabilities, making it essential for multiclass classification.\nIt ensures a probabilistic interpretation while maintaining normalization across all classes."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#discriminative-vs.-generative",
    "href": "lecture_slides/03_classification/03_classification.html#discriminative-vs.-generative",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminative vs. Generative",
    "text": "Discriminative vs. Generative\n\n\n\n\nDiscriminative (Logistic Regression)\n\nModel the conditional distribution of the response\n\\[\n\\Pr(Y=k\\mid X=x)\n\\]\nLearn parameters by maximizing the (multinomial) log-likelihood (cross-entropy).\nDirectly outputs class posterior probabilities.\n\n\nGenerative (LDA, QDA, NB)\n\nModel the class-conditional distribution of the predictors\n\\[\nf_k(x)\\equiv \\Pr(X=x\\mid Y=k)\n\\] and the class prior \\(\\pi_k=\\Pr(Y=k)\\).\nApply Bayes’ theorem to “flip” into posteriors: \\[\np_k(x)=\\Pr(Y=k\\mid X=x)=\n\\frac{\\pi_k\\,f_k(x)}{\\sum_{l=1}^{K}\\pi_l\\,f_l(x)}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#three-generative-classifiers-at-a-glance",
    "href": "lecture_slides/03_classification/03_classification.html#three-generative-classifiers-at-a-glance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Three Generative Classifiers (at a glance)",
    "text": "Three Generative Classifiers (at a glance)\n\n\n\nLinear Discriminant Analysis (LDA)\n\nAssumption: \\(X\\mid Y=k \\sim \\mathcal{N}(\\mu_k,\\Sigma)\\) with common \\(\\Sigma\\).\nLeads to linear decision boundaries in \\(x\\).\n\n\nQuadratic Discriminant Analysis (QDA)\n\nAssumption: \\(X\\mid Y=k \\sim \\mathcal{N}(\\mu_k,\\Sigma_k)\\) with class-specific \\(\\Sigma_k\\).\nLeads to quadratic decision boundaries; more flexible, higher variance."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes",
    "text": "Naïve Bayes\n\nAssumption: features are conditionally independent given \\(Y\\).\nDensity factorization: \\[\nf_k(x)=\\prod_{j=1}^p f_{kj}(x_j\\mid Y=k).\n\\]\nScales well in high dimensions; often competitive even when independence is only approximately true."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#summary",
    "href": "lecture_slides/03_classification/03_classification.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nLogistic regression: model \\(\\Pr(Y\\mid X)\\) directly (discriminative).\nGenerative route: model \\(\\Pr(X\\mid Y)\\) and \\(\\Pr(Y)\\), then use Bayes Theorem to get posteriors.\nBayes classifier picks the class with the largest posterior and is optimal if the model is correct.\nLDA/QDA/Naïve Bayes are practical estimators of the Bayes rule under different assumptions on \\(f_k(x)\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#why-discriminant-analysis",
    "href": "lecture_slides/03_classification/03_classification.html#why-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Discriminant Analysis?",
    "text": "Why Discriminant Analysis?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#discriminant-analysis-1",
    "href": "lecture_slides/03_classification/03_classification.html#discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of \\(X\\) in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(\\Pr(Y \\mid X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions as input for \\(f_k(x)\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#bayes-theorem-for-classification",
    "href": "lecture_slides/03_classification/03_classification.html#bayes-theorem-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes Theorem for Classification",
    "text": "Bayes Theorem for Classification\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\Pr(X = x \\mid Y = k) \\cdot \\Pr(Y = k)}{\\Pr(X = x)}\n\\]\nOne writes this slightly differently for discriminant analysis:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{\\ell=1}^K \\pi_\\ell f_\\ell(x)}, \\quad \\text{where}\n\\]\n\n\\(f_k(x) = \\Pr(X = x \\mid Y = k)\\) is the density for \\(X\\) in class \\(k\\). Here we will use normal densities for these, separately in each class.\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#bayes-theorem-explanation",
    "href": "lecture_slides/03_classification/03_classification.html#bayes-theorem-explanation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes’ Theorem: Explanation",
    "text": "Bayes’ Theorem: Explanation\nIt describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A|B)\\): Posterior probability - Probability of event \\(A\\) occurring given that \\(B\\) is true — updated probability after the evidence is considered.\n\\(P(A)\\): Prior probability - Initial probability of event \\(A\\) — the probability before the evidence is considered.\n\\(P(B|A)\\): Likelihood - Probability of observing event \\(B\\) given that \\(A\\) is true.\n\\(P(B)\\): Marginal probability - Total probability of the evidence, event \\(B\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#understanding-conditional-probability",
    "href": "lecture_slides/03_classification/03_classification.html#understanding-conditional-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Understanding Conditional Probability",
    "text": "Understanding Conditional Probability\nConditional probability is the probability of an event occurring given that another event has already occurred.\nDefinition:\n\\[\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nis the probability of event \\(A\\) occurring given that \\(B\\) is true.\n\nInterpretation: How likely is \\(A\\) if we know that \\(B\\) happens?"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#what-is-joint-probability",
    "href": "lecture_slides/03_classification/03_classification.html#what-is-joint-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Joint Probability?",
    "text": "What is Joint Probability?\nJoint probability refers to the probability of two events occurring together.\nDefinition: \\(P(A \\cap B)\\) is the probability that both \\(A\\) and \\(B\\) occur.\n\nConnection to Conditional Probability:\n\\[\n  P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n  P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\nThis formula is crucial for understanding Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#symmetry-in-joint-events",
    "href": "lecture_slides/03_classification/03_classification.html#symmetry-in-joint-events",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Symmetry in Joint Events",
    "text": "Symmetry in Joint Events\nJoint probability is symmetric, meaning:\n\\[\nP(A \\cap B) = P(B \\cap A)\n\\]\nThus, we can also express it as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\nThis symmetry is the key to deriving Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#deriving-bayes-theorem",
    "href": "lecture_slides/03_classification/03_classification.html#deriving-bayes-theorem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deriving Bayes’ Theorem",
    "text": "Deriving Bayes’ Theorem\n\nGiven that the definition of Conditional Probability is:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n\n\nUsing the Definition of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n   P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\n\n\n\nSymmetry of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(B \\cap A)\n\\]\n\n\nThus, we can express the joint probability as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\n\nThe Bayes’ Theorem!\n\nSubstitute this back into the conditional probability definition:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#why-bayes-theorem-matters",
    "href": "lecture_slides/03_classification/03_classification.html#why-bayes-theorem-matters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Bayes’ Theorem Matters?",
    "text": "Why Bayes’ Theorem Matters?\n\nBayes’ Theorem is a foundational principle in probability theory and statistics, enabling:\n\nIncorporation of Prior Knowledge:\nIt allows for the integration of prior knowledge or beliefs when making statistical inferences.\nBeliefs Update:\nIt provides a systematic way to update the probability estimates as new evidence or data becomes available.\nProbabilistic Thinking:\nEncourages a probabilistic approach to decision-making, quantifying uncertainty, and reasoning under uncertainty.\nVersatility in Applications:\nFrom medical diagnosis to spam filtering, Bayes’ Theorem is pivotal in areas requiring probabilistic assessment.\n\nBayes’ Theorem is a paradigm that shapes the way we interpret and interact with data, offering a powerful tool for learning from information and making decisions in an uncertain world."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#classify-to-the-highest-density",
    "href": "lecture_slides/03_classification/03_classification.html#classify-to-the-highest-density",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classify to the Highest Density",
    "text": "Classify to the Highest Density\n\n\nLeft-hand plot: single variable X and \\(\\pi_k f_k(x)\\) in the vertical axis for both classes \\(k\\) equals 1 and \\(k\\) equals 2. In this case the the pies are the same for both, so anything to the left of zero we classify as as green and anything to the right we classify as as purple.\nRight-hand plot: here we have different priors. The probability of \\(k = 2\\) is 0.7 and and of of \\(k= 1\\) is 0.3. The decision boundary moved slightly to the left. On the right, we favor the pink class."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p = 1\\)",
    "text": "Linear Discriminant Analysis when \\(p = 1\\)\nThe Gaussian density has the form:\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2}\n\\]\nHere \\(\\mu_k\\) is the mean, and \\(\\sigma_k^2\\) the variance (in class \\(k\\)). We will assume that all the \\(\\sigma_k = \\sigma\\) are the same.\n\nPlugging this into Bayes formula, we get a rather complex expression for \\(p_k(x) = \\Pr(Y = k \\mid X = x)\\):\n\\[\np_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2}}{\\sum_{\\ell=1}^K \\pi_\\ell \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2}}\n\\]\nHappily, there are simplifications and cancellations."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#discriminant-functions",
    "href": "lecture_slides/03_classification/03_classification.html#discriminant-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Functions",
    "text": "Discriminant Functions\nTo classify one observation at the value \\(X = x\\) into a class, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\n\nIf there are \\(K = 2\\) classes and \\(\\pi_1 = \\pi_2 = 0.5\\), then one can see that the decision boundary is at:\n\\[\nx = \\frac{\\mu_1 + \\mu_2}{2}.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "href": "lecture_slides/03_classification/03_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Estimating Parameters for Discriminant Analysis",
    "text": "Example: Estimating Parameters for Discriminant Analysis\n\n\nLeft-Panel: Synthetic population data with \\(\\mu_1 = -1.5\\), \\(\\mu_2 = 1.5\\), \\(\\pi_1 = \\pi_2 = 0.5\\), and \\(\\sigma^2 = 1\\).\nTypically, we don’t know these parameters; we just have the training data. In that case, we simply estimate the parameters and plug them into the rule.\nRight-Panel: histograms of the sample. We see that the estimation provided a decision boundary (black solid line) pretty close to the correct one, the one of the population."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#estimating-the-parameters",
    "href": "lecture_slides/03_classification/03_classification.html#estimating-the-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\n\nThe prior is the number in each class divided by the total number:\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nThe means in each class is the sample mean:\n\\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n\\]\nWe assume that the variance is the same in each of the classes and so we assume a pooled variance estimate:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n\\]\n\\[\n= \\sum_{k=1}^K \\frac{n_k - 1}{n - K} \\cdot \\hat{\\sigma}_k^2\n\\]\nwhere \\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\\) is the usual formula for the estimated variance in the \\(k\\)-th class."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-2",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\nGaussian density in two Dimensions, two variables \\(x_1\\) and \\(x_2\\). On the Left-panel, we have a bell function and this is the case when the two variables are uncorrelated. On the Right-panel, there is correlation between the two predictors and it is like a stretched bell.\nDensity:\n\\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\\] where \\(\\Sigma\\) is the covariance matrix."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#covariance-matrix",
    "href": "lecture_slides/03_classification/03_classification.html#covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\n\nThe covariance matrix is a square matrix that summarizes the covariance (a measure of how much two random variables vary together) between multiple variables in a dataset.\nDefinition:\nFor a random vector \\(X = [X_1, X_2, \\dots, X_p]^\\top\\) with \\(p\\) variables, the covariance matrix \\(\\Sigma\\) is defined as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_p) \\\\\n\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_p) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(X_p, X_1) & \\text{Cov}(X_p, X_2) & \\cdots & \\text{Var}(X_p)\n\\end{bmatrix}\n\\]\n\nKey Properties:\n\n\\(\\text{Var}(X_i)\\): Variance of variable \\(X_i\\).\n\\(\\text{Cov}(X_i, X_j)\\): Covariance between variables \\(X_i\\) and \\(X_j\\).\n\\(\\Sigma\\) is symmetric: \\(\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)\\).\nDiagonal elements represent variances, and off-diagonal elements represent covariances."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-3",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\nDiscriminant function: after simplifying the density function we can find\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\]\nNote that it is a linear function where the first component, \\(x^T \\Sigma^{-1} \\mu_k\\), has the \\(x\\) variable multiplied by a coefficient vector and, the second component, \\(\\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\), is a constant."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-4",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\nThe Discriminant function can be written as\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\]\na linear function. That is a function for class \\(k\\), where \\(c_{k0}\\) represents the constant we find in the second component of the Discriminant function and \\(c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\) come from the first component of the Discriminant function. We compute \\(\\delta_k(x)\\) for each of the classes and then you classify to the class for which it is largest."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#illustration-p-2-and-k-3-classes",
    "href": "lecture_slides/03_classification/03_classification.html#illustration-p-2-and-k-3-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes",
    "text": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes\n\n\nLeft-panel: The circle presents the countor of the density of a particular level of probability for the blue, green, and the orange class. Here \\(\\pi_1 = \\pi_2 = \\pi_3 = \\frac{1}{3}\\). The dashed lines are known as the Bayes decision boundaries. They are the “True” decision boundaries, were they known, they would yield the fewest misclassification errors, among all possible classifiers.\nRight-panel: We compute the mean for \\(x_1\\) and \\(x_2\\) for the each blue, green, and orange class. After plugging them into the formula, instead of getting the the dotted lines we get the solid black lines."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#from-delta_kx-to-probabilities",
    "href": "lecture_slides/03_classification/03_classification.html#from-delta_kx-to-probabilities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "From \\(\\delta_k(x)\\) to Probabilities",
    "text": "From \\(\\delta_k(x)\\) to Probabilities\n\n\nOnce we have estimates of the Discriminant Functions, \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\n\\[\n\\hat{\\Pr}(Y = k | X = x) = \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^K e^{\\hat{\\delta}_l(x)}}.\n\\]\n\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{\\Pr}(Y = k | X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{\\Pr}(Y = 2 | X = x) \\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#lda-on-credit-data",
    "href": "lecture_slides/03_classification/03_classification.html#lda-on-credit-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "LDA on Credit Data",
    "text": "LDA on Credit Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Default Status\n\n\n\nPredicted Default Status\nNo\nYes\nTotal\n\n\n\n\nNo\n9644\n252\n9896\n\n\nYes\n23\n81\n104\n\n\nTotal\n9667\n333\n10000\n\n\n\n\n\n\n\\(\\frac{23 + 252}{10000}\\) errors — a 2.75% misclassification rate!\n\n\nSome caveats:\n\nThis is training error, and we may be overfitting.\nIf we classified to the prior, the proportion of cases in the classes (e.g. always assuming the class No default). We would make \\(\\frac{333}{10000}\\) errors, or only 3.33%. This is what we call the null rate.\nWe can break the errors into different kinds: of the true No’s, we make \\(\\frac{23}{9667} = 0.2\\%\\) errors; of the true Yes’s, we make \\(\\frac{252}{333} = 75.7\\%\\) errors!"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#types-of-errors-1",
    "href": "lecture_slides/03_classification/03_classification.html#types-of-errors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of errors",
    "text": "Types of errors\n\nFalse positive rate: The fraction of negative examples that are classified as positive — 0.2% in example.\nFalse negative rate: The fraction of positive examples that are classified as negative — 75.7% in example.\nWe produced this table by classifying to class Yes if:\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq 0.5\n\\]\nWe can change the two error rates by changing the threshold from \\(0.5\\) to some other value in \\([0, 1]\\):\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq \\text{threshold},\n\\]\nand vary \\(\\text{threshold}\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#varying-the-threshold",
    "href": "lecture_slides/03_classification/03_classification.html#varying-the-threshold",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to reduce the false negative rate, we may want to reduce the threshold to 0.1 or less."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#other-forms-of-discriminant-analysis-1",
    "href": "lecture_slides/03_classification/03_classification.html#other-forms-of-discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Forms of Discriminant Analysis",
    "text": "Other Forms of Discriminant Analysis\n\nWhen \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\Sigma\\) in each class, this leads to linear discriminant analysis.\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n\\]\nBy altering the forms for \\(f_k(x)\\), we get different classifiers:\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get quadratic discriminant analysis.\nWith \\(f_k(x) = \\prod_{j=1}^{p} f_{jk}(x_j)\\) (conditional independence model) in each class, we get naive Bayes. For Gaussians, this means \\(\\Sigma_k\\) are diagonal.\nMany other forms, by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#quadratic-discriminant-analysis",
    "href": "lecture_slides/03_classification/03_classification.html#quadratic-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Quadratic Discriminant Analysis",
    "text": "Quadratic Discriminant Analysis\n\n\n\n\n\n\n\n\n\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1}(x - \\mu_k) + \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k|\n\\]\nIn the Left-plot we see a case when the true boundary should be linear. In the Right-plot, covariances were different in the true data. It is possible to see that the bayes decision boundary is curved and the quadratic discriminant analysis is also curved whereas the linear discriminant analysis gives a different boundary.\nWhether each class has the same or different covariance matrices significantly impacts how boundaries between the classes are defined. The covariance matrix describes the spread or variability of data points within each class and how the features in that class relate to each other.\n\nKey Insight: If \\(\\Sigma_k\\) are different for each class, the quadratic terms matter significantly.\nQDA allows for non-linear decision boundaries due to unique covariance matrices for each class.\nExample: Suppose we are classifying plants based on two features (e.g., height and leaf width). If one type of plant has a tall and narrow spread of data, while another type has a short and wide spread, QDA can handle these differences and draw curved boundaries to separate the groups."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#assess-the-covariance-matrices",
    "href": "lecture_slides/03_classification/03_classification.html#assess-the-covariance-matrices",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assess the Covariance Matrices",
    "text": "Assess the Covariance Matrices\n\nLDA assumes the covariance matrices of all classes are the same, while QDA allows each class to have its own. To determine which assumption is better:\n\nHypothesis test: we can perform a Test for Equality of Covariance Matrices (e.g. Box’s M Test). If the covariance matrices are similar (test is not significant): LDA is appropriate. If the covariance matrices differ (test is significant): QDA may be better.\nVisual Inspection: Plot the data in two dimensions (e.g., using scatterplots). Check if the spread, shape, or orientation of data points differs significantly between classes. If they are similar, LDA might work well. If they are visibly different, QDA is likely better.\nCompare Model Performance: run both models and choose the model that performs better on unseen data (test set).\nConsider the Number of Features and Data Size: LDA performs well with smaller datasets because it estimates a single covariance matrix across all classes (fewer parameters). QDA requires a larger dataset because it estimates a separate covariance matrix for each class (more parameters).\nDomain Knowledge: Use your understanding of the data to decide."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-versus-lda",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-versus-lda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression versus LDA",
    "text": "Logistic Regression versus LDA\n\nFor a two-class problem, one can show that for LDA:\n\\[\n\\log \\left( \\frac{p_1(x)}{1 - p_1(x)} \\right) = \\log \\left( \\frac{p_1(x)}{p_2(x)} \\right) = c_0 + c_1 x_1 + \\dots + c_p x_p\n\\]\nif we take the log odds, \\(\\log \\left( \\frac{p_1(x)}{1 - p_1(x)}\\right)\\), which is the log of the probability for class 1 versus the probability for class two, we endup with a linear function of \\(x\\), \\(c_0 + c_1 x_1 + \\dots + c_p x_p\\). So it has the same form as logistic regression.\nThe difference lies in how the parameters are estimated.\n\nLogistic regression uses the conditional likelihood based on \\(\\text{Pr}(Y|X)\\). In Machine Learning, it is known as discriminative learning.\nLDA uses the full likelihood based on the joint distributions of \\(x's\\) and \\(y's\\), \\(\\text{Pr}(X, Y)\\), whereas logistic regression was only using the distribution of \\(y's\\). It is known as generative learning.\nDespite these differences, in practice, the results are often very similar.\n\nLogistic regression can also fit quadratic boundaries like QDA by explicitly including quadratic terms in the model."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naive-bayes-1",
    "href": "lecture_slides/03_classification/03_classification.html#naive-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\nAssumes features are independent in each class.\nUseful when \\(p\\) is large, and so multivariate methods like QDA and even LDA break down.\n\nGaussian Naive Bayes assumes each \\(\\Sigma_k\\) is diagonal:\n\\[\n\\begin{aligned}\n\\delta_k(x) &\\propto \\log \\left[ \\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\right] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\left[ \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\sigma_{kj}^2 \\right] + \\log \\pi_k\n\\end{aligned}\n\\]\n\nCan be used for mixed feature vectors (qualitative and quantitative). If \\(X_j\\) is qualitative, replace \\(f_{kj}(x_j)\\) with the probability mass function (histogram) over discrete categories.\nKey Point: Despite strong assumptions, naive Bayes often produces good classification results.\n\nExplanation:\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_{kj}(x_j)\\): Density function for feature \\(j\\) in class \\(k\\).\n\\(\\mu_{kj}\\): Mean of feature \\(j\\) in class \\(k\\).\n\\(\\sigma_{kj}^2\\): Variance of feature \\(j\\) in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#diagonal-covariance-matrix",
    "href": "lecture_slides/03_classification/03_classification.html#diagonal-covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Diagonal Covariance Matrix",
    "text": "Diagonal Covariance Matrix\n\nA diagonal covariance matrix is a special case of the covariance matrix where all off-diagonal elements are zero. This implies that the variables are uncorrelated.\nGeneral Form:\nFor \\(p\\) variables, a diagonal covariance matrix \\(\\Sigma\\) is represented as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_p^2\n\\end{bmatrix}\n\\]\nProperties:\n\nDiagonal Elements (\\(\\sigma_i^2\\)): Represent the variance of each variable \\(X_i\\).\nOff-Diagonal Elements: All equal to zero (\\(\\text{Cov}(X_i, X_j) = 0\\) for \\(i \\neq j\\)), indicating no linear relationship between variables.\nA diagonal covariance matrix assumes independence between variables. Each variable varies independently without influencing the others.\nCommonly used in simpler models, such as Naive Bayes, where independence is assumed."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes",
    "href": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\nLogistic regression models \\(\\Pr(Y = k | X = x)\\) directly, via the logistic function. Similarly, the multinomial logistic regression uses the softmax function. These all model the conditional distribution of \\(Y\\) given \\(X\\).\nBy contrast, generative models start with the conditional distribution of \\(X\\) given \\(Y\\), and then use Bayes formula to turn things around:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}.\n\\]\n\n\\(f_k(x)\\) is the density of \\(X\\) given \\(Y = k\\);\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal probability that \\(Y\\) is in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes-1",
    "href": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\nLinear and quadratic discriminant analysis derive from generative models, where \\(f_k(x)\\) are Gaussian.\nUseful if some classes are well separated. A situation where logistic regression is unstable.\nNaïve Bayes assumes that the densities \\(f_k(x)\\) in each class factor:\n\n\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\n\\]\n\nEquivalently, this assumes that the features are independent within each class.\nThen using Bayes formula:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)}{\\sum_{l=1}^{K} \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\cdots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-details",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Details",
    "text": "Naïve Bayes — Details\nWhy the independence assumption?\n\nDifficult to specify and model high-dimensional densities.\nMuch easier to specify one-dimensional densities.\nCan handle mixed features:\n\nIf feature \\(j\\) is quantitative, can model as univariate Gaussian, for example: \\(X_j \\mid Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2).\\) We estimate \\(\\mu_{jk}\\) and \\(\\sigma_{jk}^2\\) from the data, and then plug into Gaussian density formula for \\(f_{jk}(x_j)\\).\nAlternatively, can use a histogram estimate of the density, and directly estimate \\(f_{jk}(x_j)\\) by the proportion of observations in the bin into which \\(x_j\\) falls.\nIf feature \\(j\\) is qualitative, can simply model the proportion in each category.\n\nSomewhat unrealistic but extremely useful in many cases.\nDespite its simplicity, often shows good classification performance due to reduced variance."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis toy example demonstrates the working of the Naïve Bayes classifier for two classes (\\(k = 1\\) and \\(k = 2\\)) and three features (\\(X_1, X_2, X_3\\)). The goal is to compute the posterior probabilities \\(\\Pr(Y = 1 \\mid X = x^*)\\) and \\(\\Pr(Y = 2 \\mid X = x^*)\\) for a given observation \\(x^* = (0.4, 1.5, 1)\\).\n\nThe prior probabilities for each class are:\n\\(\\hat{\\pi}_1 = \\hat{\\pi}_2 = 0.5\\)\n\n\nFor each feature (\\(X_1, X_2, X_3\\)), we estimate the class-conditional density functions:\n\n\\(\\hat{f}_{11}, \\hat{f}_{12}, \\hat{f}_{13}\\): Densities for \\(k = 1\\) (class 1).\n\n\\(\\hat{f}_{11}(0.4) = 0.368 \\\\\\)\n\\(\\hat{f}_{12}(1.5) = 0.484 \\\\\\)\n\\(\\hat{f}_{13}(1) = 0.226 \\\\\\)\n\n\\(\\hat{f}_{21}, \\hat{f}_{22}, \\hat{f}_{23}\\): Densities for \\(k = 2\\) (class 2).\n\n\\(\\hat{f}_{21}(0.4) = 0.030 \\\\\\)\n\\(\\hat{f}_{22}(1.5) = 0.130 \\\\\\)\n\\(\\hat{f}_{23}(1) = 0.616 \\\\\\)"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example-1",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\nCompute Class-Conditional Likelihoods for each class \\(k\\), the likelihood is computed as the product of the conditional densities for each feature:\n\n\\[\n   \\hat{f}_k(x^*) = \\prod_{j=1}^3 \\hat{f}_{kj}(x_j^*)\n\\]\n\nFor \\(k = 1\\):\n\n\\[\n     \\hat{f}_{11}(0.4) = 0.368, \\quad \\hat{f}_{12}(1.5) = 0.484, \\quad \\hat{f}_{13}(1) = 0.226\n\\]\n\\[\n     \\hat{f}_1(x^*) = 0.368 \\times 0.484 \\times 0.226 \\approx 0.0402\n\\]\n\nFor \\(k = 2\\):\n\n\\[\n     \\hat{f}_{21}(0.4) = 0.030, \\quad \\hat{f}_{22}(1.5) = 0.130, \\quad \\hat{f}_{23}(1) = 0.616\n\\]\n\\[\n     \\hat{f}_2(x^*) = 0.030 \\times 0.130 \\times 0.616 \\approx 0.0024\n\\]\n\n\n\nCompute Posterior Probabilities using Bayes’ theorem:\n\n\\[\n   \\Pr(Y = k \\mid X = x^*) = \\frac{\\hat{\\pi}_k \\hat{f}_k(x^*)}{\\sum_{k=1}^2 \\hat{\\pi}_k \\hat{f}_k(x^*)}\n\\]\n\nFor \\(k = 1\\): \\[\n\\Pr(Y = 1 \\mid X = x^*) = \\frac{0.5 \\times 0.0402}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.944\n\\]\nFor \\(k = 2\\): \\[\n\\Pr(Y = 2 \\mid X = x^*) = \\frac{0.5 \\times 0.0024}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.056\n\\]\n\n\n\nKey Takeaways:\n\nNaïve Bayes Assumption: The assumption of feature independence simplifies computation by allowing the class-conditional densities to be computed separately for each feature.\nPosterior Probabilities: The posterior probability combines the prior (\\(\\pi_k\\)) and the likelihood (\\(\\hat{f}_k(x^*)\\)).\nClassification: The observation \\(x^*\\) is classified as the class with the highest posterior probability (\\(Y = 1\\))."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs",
    "text": "Naïve Bayes and GAMs\n\nNaïve Bayes classifier can be understood as a special case of a GAM.\n\\[\n\\begin{aligned}\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n&= \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right) \\\\\n&= a_k + \\sum_{j=1}^p g_{kj}(x_j),\n\\end{aligned}\n\\]\nwhere \\(a_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\) and \\(g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\).\nHence, the Naïve Bayes model is a Generalized Additive Model (GAM):\n\nThe log-odds are expressed as a sum of additive terms.\n\\(a_k\\): Represents prior influence.\n\\(g_{kj}(x_j)\\): Represents feature contributions."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nLog-Odds of Posterior Probabilities\nThe Naïve Bayes classifier starts with the log-odds of the posterior probabilities:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n\\]\nThis is the log of the ratio of the probabilities of class \\(k\\) and a reference class \\(K\\), given the feature vector \\(X = x\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-1",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nBayes’ Theorem\nUsing Bayes’ theorem, the posterior probabilities can be expressed as:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right)\n\\]\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_k(x)\\): Class-conditional density for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-2",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nNaïve Bayes Assumption\nThe Naïve Bayes assumption states that features are conditionally independent given the class:\n\\[\nf_k(x) = \\prod_{j=1}^p f_{kj}(x_j)\n\\]\nSubstituting this into the equation:\n\\[\n\\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) = \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-3",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nSeparate the Terms\nThe terms can now be separated:\n\\[\n\\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\n\n\\(\\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\): Influence of prior probabilities.\n\\(\\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\): Contribution from each feature."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-4",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nAdditive Form\nDefine:\n\\[\na_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right), \\quad g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\nThe equation becomes:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = a_k + \\sum_{j=1}^p g_{kj}(x_j)\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-1",
    "href": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLinear regression is used for quantitative responses.\nLinear logistic regression is the counterpart for a binary response and models the logit of the probability as a linear model.\nOther response types exist, such as non-negative responses, skewed distributions, and more.\nGeneralized linear models provide a unified framework for dealing with many different response types."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-bikeshare-data",
    "href": "lecture_slides/03_classification/03_classification.html#example-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Bikeshare Data",
    "text": "Example: Bikeshare Data\n\nLinear regression with response bikers: number of hourly users in the bikeshare program in Washington, DC.\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n73.60\n5.13\n14.34\n0.00\n\n\nworkingday\n1.27\n1.78\n0.71\n0.48\n\n\ntemp\n157.21\n10.26\n15.32\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-12.89\n1.96\n-6.56\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-66.49\n2.97\n-22.43\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-109.75\n76.67\n-1.43\n0.15"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-meanvariance-relationship",
    "href": "lecture_slides/03_classification/03_classification.html#example-meanvariance-relationship",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Mean/Variance Relationship",
    "text": "Example: Mean/Variance Relationship\n\n\nLeft plot: we see that the variance mostly increases with the mean.\n10% of a linear model predictions are negative! (not shown here.). However, we know that the response variable, bikers, is always positive.\nTaking log(bikers) alleviates this, but is not a good solution. It has its own problems: e.g. predictions are on the wrong scale, and some counts are zero!"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#poisson-regression-model",
    "href": "lecture_slides/03_classification/03_classification.html#poisson-regression-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nPoisson distribution is useful for modeling counts:\n\\[\n  Pr(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\, \\text{for } k = 0, 1, 2, \\ldots\n\\]\nMean/variance relationship: \\(\\lambda = \\mathbb{E}(Y) = \\text{Var}(Y)\\) i.e., there is a mean/variance dependence. When the mean is higher, the variance is higher.\nModel with Covariates:\n\\[\n  \\log(\\lambda(X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\nOr equivalently:\n\\[\n  \\lambda(X_1, \\ldots, X_p) = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\]\nAutomatic positivity: The model ensures that predictions are non-negative by construction."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-poisson-regression-on-bikeshare-data",
    "href": "lecture_slides/03_classification/03_classification.html#example-poisson-regression-on-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Poisson Regression on Bikeshare Data",
    "text": "Example: Poisson Regression on Bikeshare Data\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n4.12\n0.01\n683.96\n0.00\n\n\nworkingday\n0.01\n0.00\n7.50\n0.00\n\n\ntemp\n0.79\n0.01\n68.43\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-0.08\n0.00\n-34.53\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-0.58\n0.00\n-141.91\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-0.93\n0.17\n-5.55\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\nNote: in this case, the variance is somewhat larger than the mean — a situation known as overdispersion. As a result, the p-values may be misleadingly small."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-2",
    "href": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\nWe have covered three GLMs: Gaussian, binomial, and Poisson.\nThey each have a characteristic link function. This is the transformation of the mean represented by a linear model:\n\n\\[\n\\eta(\\mathbb{E}(Y|X_1, X_2, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\n\nThe link functions for linear, logistic, and Poisson regression are \\(\\eta(\\mu) = \\mu\\), \\(\\eta(\\mu) = \\log(\\mu / (1 - \\mu))\\), \\(\\eta(\\mu) = \\log(\\mu)\\), respectively.\nEach GLM has a characteristic variance function.\nThe models are fit by maximum likelihood, and model summaries are produced using glm() in R.\nOther GLMs include Gamma, Negative-binomial, Inverse Gaussian, and more."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confusion-matrix",
    "href": "lecture_slides/03_classification/03_classification.html#confusion-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nThe confusion matrix provides a summary of prediction results. It compares the predicted and actual classes, offering insights into the model’s classification performance.\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confusion-matrix-1",
    "href": "lecture_slides/03_classification/03_classification.html#confusion-matrix-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nKey metrics derived from the confusion matrix include:\n\n\n\n\n\n\n\nMetric\nDefinition\n\n\n\n\nAccuracy\nThe proportion of correct predictions.\n\n\nSensitivity (Recall)\nThe model’s ability to identify positive cases.\n\n\nSpecificity\nThe model’s ability to identify negative cases.\n\n\nPrecision\nAmong predicted positive cases, the proportion that are truly positive.\n\n\nF1 Score\nThe harmonic mean of Precision and Sensitivity, balancing false positives and false negatives."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#accuracy",
    "href": "lecture_slides/03_classification/03_classification.html#accuracy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Accuracy",
    "text": "Accuracy\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\\]\n\n\n\n\nOverall effectiveness of the model.\nIn the context of weather forecasting, for example, accuracy reflects how well a model predicts weather events correctly (e.g., rainy or sunny days).\nHigh accuracy: lots of correct predictions!"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#recall-sensitivity",
    "href": "lecture_slides/03_classification/03_classification.html#recall-sensitivity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recall (Sensitivity)",
    "text": "Recall (Sensitivity)\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\n\n\n\n\nIs the fraction of positives correctly identified.\nIn criminal justice, it would assess how well a predictive policing model identifies all potential criminal activities (True Positives) without missing any (thus minimizing False Negatives).\nHigh recall: low false-negative rates."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#specificity",
    "href": "lecture_slides/03_classification/03_classification.html#specificity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Specificity",
    "text": "Specificity\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n\\]\n\n\n\n\nIt is the true negative rate, measuring a model’s ability to correctly identify actual negatives.\nCrucial in fields where incorrectly identifying a negative case as positive could have serious implications (e.g., criminal justice).\nHigh specificity: the model is very effective at identifying true negatives."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#precision",
    "href": "lecture_slides/03_classification/03_classification.html#precision",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Precision",
    "text": "Precision\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\n\n\n\n\nAccuracy of positive predictions.\nIn email spam detection, it would indicate the percentage of emails correctly identified as spam (True Positives) out of all emails flagged as spam, aiming to reduce the number of legitimate emails incorrectly marked as spam (False Positives).\nHigh precision: low false-positive rates."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#f1-score",
    "href": "lecture_slides/03_classification/03_classification.html#f1-score",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "F1-Score",
    "text": "F1-Score\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\n\nHarmonic mean of Precision and Recall.\nIn a medical diagnosis scenario, it would help in evaluating a test’s effectiveness in correctly identifying patients with a disease (True Positives) while minimizing the misclassification of healthy individuals as diseased (False Positives and False Negatives).\nHigh F1 score: a better balance between precision and recall."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#roc-curve-auc-1",
    "href": "lecture_slides/03_classification/03_classification.html#roc-curve-auc-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve & AUC",
    "text": "ROC Curve & AUC\n\nThe ROC (Receiver Operating Characteristic) curve is a graphical representation that illustrates the trade-off between the true positive rate (Sensitivity) and the false positive rate (1 - Specificity) across various threshold values. The AUC (Area Under the Curve) summarizes the ROC curve into a single value, indicating the model’s overall performance.\n\nAUC Interpretation:\n\nAUC \\(\\approx\\) 1: The model has near-perfect predictive capability.\nAUC \\(\\approx\\) 0.5: The model performs no better than random guessing."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#roc-curve",
    "href": "lecture_slides/03_classification/03_classification.html#roc-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve",
    "text": "ROC Curve\nLDA on Credit Data Example:\n\nThe ROC plot displays both, True Positive rate and False Positive rate, simultaneously.\nSometimes we use the AUC or area under the curve to summarize the overall performance. Higher AUC is good."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#summary-2",
    "href": "lecture_slides/03_classification/03_classification.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\nKey Concepts:\n\nClassification involves predicting categorical outcomes based on input features.\nPopular approaches include:\n\nLogistic Regression: Directly models probabilities; suitable for \\(K=2\\) and extendable to \\(K &gt; 2\\).\nDiscriminant Analysis: Assumes Gaussian distributions; suitable for small datasets or when classes are well separated.\nNaïve Bayes: Assumes feature independence; works well with large \\(p\\) or mixed data types.\n\nThresholds and ROC Curves allow fine-tuning between false positive and false negative rates.\n\n\n\n\nPractical Insights\n\nLinear vs Logistic Regression: Logistic regression avoids issues with probabilities outside \\(0, 1\\).\nDiscriminant Analysis: Use Linear Discriminant Analysis (LDA) for shared covariance matrices or Quadratic Discriminant Analysis (QDA) when covariance matrices differ.\nNaïve Bayes: Despite its simplicity, it often performs well due to reduced variance and works for both qualitative and quantitative data.\nGeneralized Linear Models (GLMs): Extend regression to different types of responses with appropriate link and variance functions."
  }
]