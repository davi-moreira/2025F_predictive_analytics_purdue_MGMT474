[
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, time series, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025F_predictive_analytics_purdue_MGMT474/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 1007\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required):\n\nGoogle Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\nMicrosoft Copilot: is an AI-powered assistant designed to enhance productivity and streamline workflows across various applications and services. It utilizes large language models and is integrated within Microsoft 365 apps like Word, Excel, PowerPoint, Outlook, and Teams, providing real-time, context-aware assistance for tasks such as drafting documents, analyzing data, managing projects, and communicating more efficiently. Users can leverage Copilot to automate repetitive tasks, generate ideas, summarize information, and access data across their work environment and the web, all within a secure and privacy-conscious framework.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Deep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#overview",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Deep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#deep-learning-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\n\nEarly Rise (1980s)\n\nNeural networks first gained popularity.\n\nHigh levels of excitement, with dedicated conferences (e.g., NeurIPS, Snowbird).\n\n1990s Shift\n\nEmergence of other methods (SVMs, Random Forests, Boosting).\n\nNeural networks receded into the background.\n\nResurgence (2010)\n\nRebranded and refined under the banner of Deep Learning.\n\nBy the 2020s, became extremely successful and widely adopted.\n\nKey Drivers of Success\n\nRapid increases in computing power (GPUs, parallel computing).\n\nAvailability of large-scale datasets.\n\nUser-friendly deep learning libraries (e.g., TensorFlow, PyTorch).\n\n\n\n\nMuch of the credit goes to three pioneers and their students:\n\nYann LeCun, Geoffrey Hinton, and Yoshua Bengio,\nwho received the 2019 ACM Turing Award for their work in Neural Networks."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#ai-visionaries-interviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#ai-visionaries-interviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AI Visionaries: Interviews",
    "text": "AI Visionaries: Interviews\n\n\n\n\n\n\n\n\n Yann LeCunThe Future of AIDec 16, 2023 \n\n\n\n\n\n\n Geoffrey Hinton60 Minutes InterviewOct 9, 2023 \n\n\n\n\n\n\n Yoshua BengioPath to Human-Level AIApr 24, 2024"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#what-are-deep-learning-frameworks",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#what-are-deep-learning-frameworks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What Are Deep Learning Frameworks?",
    "text": "What Are Deep Learning Frameworks?\n\nDeep learning frameworks reduce boilerplate code, handle tensor operations efficiently, and make it easier to prototype and iterate on new architectures.\nSoftware libraries designed to streamline the creation, training, and deployment of neural networks.\n\nProvide pre-built functions, automatic differentiation, and GPU/TPU support.\n\nNecessity: They allow researchers and developers to focus on model design rather than low-level implementation details."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#pytorch-and-tensor-flow",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#pytorch-and-tensor-flow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PyTorch and Tensor Flow",
    "text": "PyTorch and Tensor Flow\n\n\n\n\nWhat is PyTorch?\n\nDeveloped primarily by Facebook (Meta) and released on September 2016.\nEmphasizes a dynamic computation graph (eager execution).\nHighly “Pythonic”: feels natural for Python developers.\nStrong community presence in academia and research.\n\n\n\n\nWhy is PyTorch Necessary?\n\nEase of Use & Debugging\n\nEvaluate expressions immediately without building a separate graph.\n\nMore intuitive for experimenting with complex, innovative models.\n\nResearch Focus\n\nQuickly prototype new ideas and iterate.\n\nActive Ecosystem\n\nLibraries like torchvision, torchaudio, and others for specialized tasks.\n\n\n\n\n\n\nHow to begin\n\nhttps://pytorch.org/tutorials/beginner/basics/intro.html.\nThere is also a YouTube Series (PyTorch Beginner Series) also here (Introduction to PyTorch)\n\n\n\n\n\n\nWhat is TensorFlow?\n\nDeveloped primarily by Google and released in November 2015.\nHistorically used a static graph approach (with an “eager mode” added later).\nComes with extensive tools for deployment (mobile, web, and production).\nLarge ecosystem with well-integrated components (e.g., TensorBoard, TFX, TensorFlow Lite).\n\n\n\n\n\nWhy is TensorFlow Necessary?\n\nProduction-Ready\n\nStrong support for model serving at scale in enterprise environments.\n\nComprehensive Ecosystem\n\nVisualization (TensorBoard), data processing (TFX), and model deployment pipelines.\n\n\nCross-Platform & Hardware Support\n\nEasily deploy models to cloud infrastructures, mobile devices, and specialized hardware (TPUs).\n\n\n\n\n\n\nHow to begin\n\nhttps://www.tensorflow.org/tutorials. There is also a Quick Start!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#key-differences",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#key-differences",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Key Differences",
    "text": "Key Differences\n\n\n\n\n\n\n\n\nAspect\nPyTorch\nTensorFlow\n\n\n\n\nComputation Graph\nDynamic graph (eager execution by default).\nHistorically static graph with a build-and-execute phase (now supports eager execution).\n\n\nDebugging & Development Style\nMore straightforward for Python developers, immediate error feedback.\nCan be trickier to debug in graph mode; eager mode helps but is relatively newer.\n\n\nDeployment & Production\nTorchServe and growing enterprise support, but historically overshadowed by TensorFlow’s tools.\nTensorFlow Serving, TensorFlow Lite, and easy Google Cloud integration.\n\n\n\n\nWhile the fundamental math and building blocks are similar, the biggest difference typically lies in how you prototype, debug, and deploy models."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#similarities",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#similarities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Similarities",
    "text": "Similarities\n\n\n\n\n\n\n\nSimilarity\nDescription\n\n\n\n\nWide Range of Neural Network Layers\nConvolutional, Recurrent, Transformers, etc. Both frameworks maintain robust libraries for standard and advanced layers.\n\n\nAuto-Differentiation\nNo need to manually compute gradients; backpropagation is handled automatically.\n\n\nGPU Acceleration\nBoth leverage CUDA (NVIDIA GPUs) or other backends to speed up training.\n\n\nRich Communities\nAbundant tutorials, example code, pretrained models, and Q&A forums.\n\n\n\n\nDespite differing philosophies, PyTorch and TensorFlow share many core functionalities and have large, supportive user communities."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Advantages and Disadvantages",
    "text": "Comparison of Advantages and Disadvantages\n\n\n\n\n\n\n\n\n\nPyTorch\nTensorFlow\n\n\n\n\nAdvantages\n- Intuitive, Pythonic Syntax: Feels like standard Python, reducing friction for experimentation  - Dynamic Graph Execution: Simplifies debugging and model design  - Research & Academia Favorite: widely used in cutting-edge papers\n- Static Graph Optimization: Graph-based execution can be highly optimized for speed and memory usage  - Extensive Production Ecosystem: Includes TensorFlow Serving, TensorFlow Lite, TFX for data pipelines  - Large Corporate Adoption: Backed by Google, widely used in enterprise settings\n\n\nDisadvantages\n- Deployment Maturity: Production tooling and ecosystem are improving but still behind TensorFlow  - Smaller Enterprise Adoption: Historically overshadowed by TensorFlow’s widespread adoption\n- Learning Curve: The graph-based approach can be challenging for newcomers  - Historically Less Intuitive: Older APIs and tutorials can be confusing, though Eager Mode improves usability"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#recommendations",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#recommendations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommendations",
    "text": "Recommendations\n\n\n\nChoose PyTorch if:\n\nYour focus is on rapid experimentation and academic research\nYou prioritize a Pythonic workflow and easy debugging\nYou prefer a dynamic graph approach (about it).\nYou are working on cutting-edge models with high flexibility\nYou value seamless interaction with Python libraries\n\n\n\n\nChoose TensorFlow if:\n\nYou need robust production and deployment pipelines\nYou plan to integrate with Google Cloud services\nYou require support for mobile/edge devices (e.g., TensorFlow Lite)\nYou benefit from static graph optimization for performance\nYou want an end-to-end ecosystem (TFX, TensorBoard, Serving)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#tensors-in-pytorch",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#tensors-in-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tensors in PyTorch",
    "text": "Tensors in PyTorch"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders\n\n\n\nThe code below extracts a single image‑tensor from the training_data used in the tutorial (you can use test_data the same way), prints its basic properties, and visualizes it.\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Choose the index of the image you wish to inspect\nidx = 0  # e.g., the first image; change as desired\n\n# Fetch the sample\nimage_tensor, label = training_data[idx]   # image_tensor is a 1×28×28 tensor\n\n# Inspect the raw tensor values\nprint(\"Shape :\", image_tensor.shape)  # torch.Size([1, 28, 28])\nprint(\"Label :\", label) # integer class id\nprint(\"Tensor (first 5 rows):\\n\", image_tensor[0, :5, :])\n\n# Visualize the image\nplt.imshow(image_tensor.squeeze(), cmap=\"gray\")\nplt.title(f\"Fashion‑MNIST class{label}\")\nplt.axis(\"off\")\nplt.show()\n\n\n\nHow it works\n\nIndex selection – set idx to any integer in range(len(training_data)).\n\nDataset access – indexing the dataset returns (image, label) with the transform already applied (here, ToTensor() scales to [0,1]).\n\nInspection – the printed tensor slice lets you verify pixel values, and plt.imshow renders the sample for visual confirmation.\n\n\nTo see a different image you just need to adjust the index."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#transforms",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#transforms",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Transforms",
    "text": "Transforms"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nWhat Are We Doing?\nWe are defining a neural network class using PyTorch. This network is designed to work with images, specifically 28×28 grayscale images like those from the FashionMNIST dataset. The network will output 10 values, one for each digit from 0 to 9.\nStep-by-Step Breakdown\n\nclass NeuralNetwork(nn.Module):\n\nWe create a new neural network class called NeuralNetwork. It inherits from PyTorch’s nn.Module, which is the base class for all neural network models.\n\ndef __init__(self): and super().__init__()\n\n__init__ is the constructor. It’s run when we create the model.\nsuper().__init__() tells Python to also run the initialization code from the parent class (nn.Module). This is required for PyTorch to keep track of everything inside the model.\n\nself.flatten = nn.Flatten():\n\nchanges the input from a 2D image (28×28) into a 1D vector (784 values), which is easier for linear layers to handle."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-2",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\nHere we build the main body of the neural network.\nself.linear_relu_stack = nn.Sequential(\n    nn.Linear(28*28, 512),\n    nn.ReLU(),\n    nn.Linear(512, 512),\n    nn.ReLU(),\n    nn.Linear(512, 10),\n)\nIn most contexts when we say “how many layers?” we refer to the learnable ones. So this network has three fully‑connected (Linear) layers, with ReLU activations in between.\n\nYou can think of the linear layer as a filter that projects the image into a new space with 512 dimensions. These new values are not pixels anymore, but rather abstract features learned by the network."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-3",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nFirst layer nn.Linear(28*28, 512): takes the 784 values from the image and transforms them into 512 values. A Linear(784, 512) layer performs:\n\nA matrix multiplication between the input vector (length 784) and a weight matrix of size [784 × 512], followed by adding a bias vector of length 512.\nMathematically: \\[\n\\text{output} = x \\cdot W + b\n\\]\nx is the input vector: shape [784]\nW is the weight matrix: shape [784 × 512]\nb is the bias vector: shape [512]\nThe result (output) is a new vector of shape [512]\n\n\n\nEach of the 512 output values is a linear combination of all 784 pixel values in the input image. By default, PyTorch initializes weights using Kaiming Uniform Initialization (a variant of He initialization), which works well with ReLU activation functions."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-4",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nnn.ReLU(): applies the ReLU activation function, which keeps positive numbers and turns negative numbers into zero. This adds non-linearity to the model.\nSecond layernn.Linear(512, 512): takes those 512 values and again outputs 512 values. This is a hidden layer, helping the model learn more complex patterns.\nnn.ReLU(): Another non-linear transformation.\nThird (Final) layer:nn.Linear(512, 10): takes the 512 values and produces 10 output values.\n\nThese are called logits, and each one corresponds to a digit class (0 to 9)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-5",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nforward(self, x): This is the forward pass, the function that runs when we send data through the model.\nStep-by-step:\n\n\nx = self.flatten(x): Convert the 28×28 image into a 1D tensor with 784 values.\nlogits = self.linear_relu_stack(x): Pass the input through the series of layers.\nreturn logits: Output the final predictions (raw scores for each class).\n\n\nIn summary this neural network:\n\nTakes an image (28×28) as input,\nFlattens it into a vector,\nPasses it through two fully connected layers with ReLU,\nOutputs a vector of size 10 (one for each digit)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-6",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\ntorch.rand(1, 28, 28, device=device): Creates a random image with shape [1, 28, 28]\n\n1 is the batch size (just one image)\n28×28 is the image dimension\ndevice=device ensures the tensor goes to CPU or GPU (wherever the model is)\n\n\n\n\n# To see tensor:\nprint(X)\n\n\n\nLet’s say the tensor shown is:\nX = torch.tensor([[\n    [0.1177, 0.2669, 0.6367, 0.6148, 0.3085, ...],  # row 0\n    [0.8672, 0.3645, 0.4822, 0.9566, 0.8999, ...],  # row 1\n    ...\n]])\n\n\nThis is a 3D tensor of shape [1, 28, 28]:\n\nThe first dimension 1 is the batch size,\nThe next two are height and width of the image.\n\nThe full index of 0.2669 in the 3D tensor is: X[0, 0, 1].\n\n0 → first (and only) image in the batch\n0 → first row of the image\n1 → second column in that row"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-7",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\nlogits = model(X): This calls the model with input X.\n\nBehind the scenes, it runs model.forward(X)\nOutput: a vector of 10 values (called logits), one for each class (digits 0 through 9)\n\nNote: We do not call model.forward() directly — PyTorch manages hooks and gradients when we use model(X)\npred_probab = nn.Softmax(dim=1)(logits): Applies softmax to the raw output logits\n\nSoftmax turns logits into probabilities (values between 0 and 1 that sum to 1)\ndim=1 means we apply softmax across the 10 output class values (not across the batch)\n\ny_pred = pred_probab.argmax(1): Picks the index of the largest probability, i.e., the predicted class\n\nargmax(1) returns the class with the highest probability from each row (here we have just one row)\n\nprint(f\"Predicted class: {y_pred}\"): Prints the predicted digit class (0 through 9)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Automatic Differentiation with torch.autograd",
    "text": "Automatic Differentiation with torch.autograd"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#optimizing-model-parameters",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#optimizing-model-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Optimizing Model Parameters",
    "text": "Optimizing Model Parameters"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#save-and-load-the-model",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#save-and-load-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Save and Load the Model",
    "text": "Save and Load the Model"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#introduction-to-pytorch---youtube-series",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#introduction-to-pytorch---youtube-series",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Introduction to PyTorch - YouTube Series",
    "text": "Introduction to PyTorch - YouTube Series\n\n\n\nPro tip: Use Colab with a GPU runtime to speed up operations Runtime &gt; Change runtime type &gt; GPU"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#neural-networks---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#neural-networks---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Neural Networks - Video",
    "text": "Neural Networks - Video\n\n\n\n\n\n\nBut what is a neural network?"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\n\nNetwork Diagram of Single Layer Neural Network"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Introduction and Layers Overview",
    "text": "Single Layer Neural Network: Introduction and Layers Overview\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nNeural networks are often displayed using network diagrams, as shown in the figure.\n\nInput Layer (Orange Circles):\n\n\\(X_1, X_2, X_3, X_4\\)\nThese are observed variables from the dataset.\n\nHidden Layer (Blue Circles):\n\n\\(A_1, A_2, A_3, A_4, A_5\\)\nThese are transformations (activations) computed from the inputs.\n\nOutput Layer (Pink Circle):\n\n\\(f(X) \\to Y\\)\n\\(Y\\) is also observed, e.g., a label or continuous response."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Observed vs. Latent",
    "text": "Single Layer Neural Network: Observed vs. Latent\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nWhere is the observed data?\n\n\\(X_j\\) are observed (the input features).\n\\(Y\\) is observed (the response or label).\nThe hidden units (\\(A_k\\)) are not observed; they’re learned transformations."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Hidden Layer as Transformations",
    "text": "Single Layer Neural Network: Hidden Layer as Transformations\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nIn the hidden layer, each activation \\(A_k\\) is computed as:\n\\[\nA_k = g\\Bigl(w_{k0} + \\sum_{j=1}^4 w_{kj} X_j\\Bigr),\n\\]\n\nIn the formula, these \\(h_k(X)\\) are the same as the activations \\(A_k\\).\n\\(h_k(X)\\) = \\(g(w_{k0} + \\sum_{j=1}^p w_{kj} X_j)\\).\n\\(g(\\cdot)\\) is a nonlinear function (e.g., ReLU, sigmoid, tanh).\n\\(w_{kj}\\) are the weights learned during training.\nEach hidden unit has a different set of weights, hence different transformations."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-training-the-network",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-training-the-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Training the Network",
    "text": "Single Layer Neural Network: Training the Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\n\nThe network learns all weights \\(w_{kj}, w_{k0}, \\beta_k, \\beta_0\\) during training.\nObjective: predict \\(Y\\) from \\(X\\) accurately.\nKey insight: Hidden layer learns useful transformations on the fly to help approximate the true function mapping \\(X\\) to \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-details",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Details",
    "text": "Single Layer Neural Network: Details\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A_k = h_k(X) = g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j)\\) are called the activations in the hidden layer. We can think of it as a non-linear tranformation of a linear function.\n\\(g(z)\\) is called the activation function. Two popular activation functions are: the sigmoid and rectified linear (ReLU).\nActivation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.\nSo the activations are like derived features — nonlinear transformations of linear combinations of the features.\nThe model is fit by minimizing \\(\\sum_{i=1}^{n} (y_i - f(x_i))^2\\) (e.g., for regression)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#nn-example-mnist-digits",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#nn-example-mnist-digits",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NN Example: MNIST Digits",
    "text": "NN Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights).\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent - Video",
    "text": "Gradient Descent - Video\n\n\n\n\n\n\nGradient descent, how neural networks learn"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-intuition---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-intuition---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Intuition - Video",
    "text": "Backpropagation Intuition - Video\n\n\n\n\n\n\nBackpropagation, intuitively"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-calculus---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-calculus---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Calculus - Video",
    "text": "Backpropagation Calculus - Video\n\n\n\n\n\n\nBackpropagation calculus"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#fitting-neural-networks-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#fitting-neural-networks-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Neural Networks",
    "text": "Fitting Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\min_{\\{w_k\\}_{1}^K, \\beta} \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2, \\quad \\text{where}\n\\]\n\\[\nf(x_i) = \\beta_0 + \\sum_{k=1}^K \\beta_k g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}\\right).\n\\]\nThis problem is difficult because the objective is non-convex.\nDespite this, effective algorithms have evolved that can optimize complex neural network problems efficiently."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#non-convex-functions-and-gradient-descent",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#non-convex-functions-and-gradient-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non Convex Functions and Gradient Descent",
    "text": "Non Convex Functions and Gradient Descent\nLet \\(R(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - f_\\theta(x_i))^2\\) with \\(\\theta = (\\{w_k\\}_{1}^K, \\beta)\\).\n\n\n\n\n\n\n\n\n\n\nStart with a guess \\(\\theta^0\\) for all the parameters in \\(\\theta\\), and set \\(t = 0\\).\nIterate until the objective \\(R(\\theta)\\) fails to decrease:\n\nFind a vector \\(\\delta\\) that reflects a small change in \\(\\theta\\), such that \\(\\theta^{t+1} = \\theta^t + \\delta\\) reduces the objective; i.e., \\(R(\\theta^{t+1}) &lt; R(\\theta^t)\\).\nSet \\(t \\gets t + 1\\)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent-continued",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent Continued",
    "text": "Gradient Descent Continued\n\nIn this simple example, we reached the global minimum.\nIf we had started a little to the left of \\(\\theta^0\\), we would have gone in the other direction and ended up in a local minimum.\nAlthough \\(\\theta\\) is multi-dimensional, we have depicted the process as one-dimensional. It is much harder to identify whether one is in a local minimum in high dimensions.\nHow to find a direction \\(\\delta\\) that points downhill? We compute the gradient vector: \\[\n\\nabla R(\\theta^t) = \\frac{\\partial R(\\theta)}{\\partial \\theta} \\bigg|_{\\theta = \\theta^t}\n\\]\ni.e., the vector of partial derivatives at the current guess \\(\\theta^t\\).\nThe gradient points uphill, so our update is \\(\\delta = - \\rho \\nabla R(\\theta^t)\\) or \\[\n\\theta^{t+1} \\gets \\theta^t - \\rho \\nabla R(\\theta^t),\n\\] where \\(\\rho\\) is the learning rate (typically small, e.g., \\(\\rho = 0.001\\))."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#gradients-and-backpropagation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#gradients-and-backpropagation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradients and Backpropagation",
    "text": "Gradients and Backpropagation\n\n\\[\nR(\\theta) = \\sum_{i=1}^n R_i(\\theta) \\text{ is a sum, so gradient is sum of gradients.}\n\\]\n\\[\nR_i(\\theta) = \\frac{1}{2}(y_i - f_\\theta(x_i))^2 = \\frac{1}{2} \\left( y_i - \\beta_0 - \\sum_{k=1}^K \\beta_k g\\left( w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij} \\right) \\right)^2\n\\]\nFor ease of notation, let\n\\[\nz_{ik} = w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}.\n\\]\nBackpropagation uses the chain rule for differentiation:\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial \\beta_k} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\beta_k}\n= -(y_i - f_\\theta(x_i)) \\cdot g(z_{ik}).\n\\]\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial w_{kj}} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial g(z_{ik})} \\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}} \\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}}\n= -(y_i - f_\\theta(x_i)) \\cdot \\beta_k \\cdot g'(z_{ik}) \\cdot x_{ij}.\n\\]"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#tricks-of-the-trade",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#tricks-of-the-trade",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tricks of the Trade",
    "text": "Tricks of the Trade\n\nSlow learning. Gradient descent is slow, and a small learning rate \\(\\rho\\) slows it even further. With early stopping, this is a form of regularization.\nStochastic gradient descent. Rather than compute the gradient using all the data, use a small minibatch drawn at random at each step. E.g. for MNIST data, with \\(n = 60K\\), we use minibatches of 128 observations.\nAn epoch is a count of iterations and amounts to the number of minibatch updates such that \\(n\\) samples in total have been processed; i.e. \\(60K/128 \\approx 469\\) for MNIST.\nRegularization. Ridge and lasso regularization can be used to shrink the weights at each layer. Two other popular forms of regularization are dropout and augmentation, discussed next."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#dropout-learning",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#dropout-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dropout Learning",
    "text": "Dropout Learning\n\n\n\n\n\n\n\n\n\n\nAt each Stochastic Gradient Descent (SGD) update, randomly remove units with probability \\(\\phi\\), and scale up the weights of those retained by \\(1/(1-\\phi)\\) to compensate.\nIn simple scenarios like linear regression, a version of this process can be shown to be equivalent to ridge regularization.\nAs in ridge, the other units stand in for those temporarily removed, and their weights are drawn closer together.\nSimilar to randomly omitting variables when growing trees in random forests."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#ridge-and-data-augmentation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#ridge-and-data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge and Data Augmentation",
    "text": "Ridge and Data Augmentation\n\n\n\n\n\n\n\n\n\n\nMake many copies of each \\((x_i, y_i)\\) and add a small amount of Gaussian noise to the \\(x_i\\) — a little cloud around each observation — but leave the copies of \\(y_i\\) alone!\nThis makes the fit robust to small perturbations in \\(x_i\\), and is equivalent to ridge regularization in an OLS setting."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation-on-the-fly",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation-on-the-fly",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation on the Fly",
    "text": "Data Augmentation on the Fly\n\n\n\n\n\n\n\n\n\n\nData augmentation is especially effective with SGD, here demonstrated for a CNN and image classification.\nNatural transformations are made of each training image when it is sampled by SGD, thus ultimately making a cloud of images around each original training image.\nThe label is left unchanged — in each case still tiger.\nImproves performance of CNN and is similar to ridge."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#double-descent",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#double-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Double Descent",
    "text": "Double Descent\n\nWith neural networks, it seems better to have too many hidden units than too few.\nLikewise more hidden layers better than few.\nRunning stochastic gradient descent till zero training error often gives good out-of-sample error.\nIncreasing the number of units or layers and again training till zero error sometimes gives even better out-of-sample error.\nWhat happened to overfitting and the usual bias-variance trade-off?\n\nBelkin, Hsu, Ma, and Mandal (arXiv 2018) Reconciling Modern Machine Learning and the Bias-Variance Trade-off."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#the-double-descent-error-curve",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#the-double-descent-error-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Double-Descent Error Curve",
    "text": "The Double-Descent Error Curve\n\n\n\n\n\n\n\n\n\n\nWhen \\(d \\leq 20\\), model is OLS, and we see usual bias-variance trade-off.\nWhen \\(d &gt; 20\\), we revert to minimum-norm. As \\(d\\) increases above 20, \\(\\sum_{j=1}^d \\hat{\\beta}_j^2\\) decreases since it is easier to achieve zero error, and hence less wiggly solutions."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#less-wiggly-solutions",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#less-wiggly-solutions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Less Wiggly Solutions",
    "text": "Less Wiggly Solutions\n\n\n\n\n\n\n\n\n\n\nTo achieve a zero-residual solution with \\(d = 20\\) is a real stretch!\nEasier for larger \\(d\\)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#some-facts",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#some-facts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Facts",
    "text": "Some Facts\n\nIn a wide linear model (\\(p \\gg n\\)) fit by least squares, SGD with a small step size leads to a minimum norm zero-residual solution.\nStochastic gradient flow — i.e. the entire path of SGD solutions — is somewhat similar to ridge path.\nBy analogy, deep and wide neural networks fit by SGD down to zero training error often give good solutions that generalize well.\nIn particular cases with high signal-to-noise ratio — e.g. image recognition — are less prone to overfitting; the zero-error solution is mostly signal!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-introduction",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-introduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN: Introduction",
    "text": "CNN: Introduction\n\nNeural networks rebounded around 2010 with big successes in image classification.\nAround that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#the-cifar100-database",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#the-cifar100-database",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The CIFAR100 Database",
    "text": "The CIFAR100 Database\n\n\n\n\n\n\n\n\n\n\n\nThe figure shows 75 images drawn from the CIFAR100 database.\nThis database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale).\nEach image has a resolution of 32 × 32 pixels, with three eight-bit numbers per pixel representing red, green, and blue. The numbers for each image are organized in a three-dimensional array called a feature map.\nThe first two axes are spatial (both 32-dimensional), and the third is the channel axis, representing the three (blue, green or red) colors.\nThere is a designated training set of 50,000 images, and a test set of 10,000."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#the-convolutional-network-hierarchy",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#the-convolutional-network-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Convolutional Network Hierarchy",
    "text": "The Convolutional Network Hierarchy\n\n\n\n\n\n\n\n\n\n\n\nCNNs mimic, to some degree, how humans classify images, by recognizing specific features or patterns anywhere in the image that distinguish each particular object class.\nThe network first identifies low-level features in the input image, such as small edges or patches of color.\nThese low-level features are then combined to form higher-level features, such as parts of ears or eyes. Eventually, the presence or absence of these higher-level features contributes to the probability of any given output class.\nThis hierarchical construction is achieved by combining two specialized types of hidden layers: convolution layers and pooling layers:\nConvolution layers search for instances of small patterns in the image.\nPooling layers downsample these results to select a prominent subset.\nTo achieve state-of-the-art results, contemporary neural network architectures often use many convolution and pooling layers."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-layer",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Layer",
    "text": "Convolution Layer\n\n\nA convolution layer is made up of a large number of convolution filters, each of which is a template that determines whether a particular local feature is present in an image.\nA convolution filter relies on a very simple operation, called a convolution, which basically amounts to repeatedly multiplying matrix elements and then adding the results. \\[\n\\text{Input Image} =\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l\n\\end{bmatrix}\n\\quad \\text{Convolution Filter} =\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{bmatrix}.\n\\]\nWhen we convolve the image with the filter, we get the result: \\[\n\\text{Convolved Image} =\n\\begin{bmatrix}\na\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}.\n\\]\nthe convolution filter is applied to every 2 × 2 submatrix of the original image in order to obtain the convolved image.\nIf a 2 × 2 submatrix of the original image resembles the convolution filter, then it will have a large value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that resemble the convolution filter.\nThe filter is itself an image and represents a small shape, edge, etc.\nThe filters are learned during training."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-example",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Example",
    "text": "Convolution Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe idea of convolution with a filter is to find common patterns that occur in different parts of the image.\nThe two filters shown here highlight vertical and horizontal stripes.\nThe result of the convolution is a new feature map.\nSince images have three color channels, the filter does as well: one filter per channel, and dot-products are summed.\nThe weights in the filters are learned by the network."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#pooling-layer",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#pooling-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pooling Layer",
    "text": "Pooling Layer\nA pooling layer provides a way to condense a large image into a smaller summary image.\n\\[\n\\text{Max pool}\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]\n\nEach non-overlapping \\(2 \\times 2\\) block is replaced by its maximum.\nThis sharpens the feature identification.\nAllows for locational invariance.\nReduces the dimension by a factor of 4 — i.e., factor of 2 in each dimension."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#architecture-of-a-cnn",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#architecture-of-a-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Architecture of a CNN",
    "text": "Architecture of a CNN\n\n\n\n\n\n\n\n\n\n\nMany convolve + pool layers.\nFilters are typically small, e.g., each channel \\(3 \\times 3\\).\nEach filter creates a new channel in the convolution layer.\nAs pooling reduces size, the number of filters/channels is typically increased.\nNumber of layers can be very large.\nE.g., resnet50 trained on imagenet 1000-class image database has 50 layers!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\n\n\n\n\n\n\n\n\n\n\nAn additional important trick used with image modeling is data augmentation.\nEssentially, each training image is replicated many times, with each replicate randomly distorted in a natural way such that human recognition is unaffected.\nTypical distortions are zoom, horizontal and vertical shift, shear, small rotations, and in this case horizontal flips.\nAt face value this is a way of increasing the training set considerably with somewhat different examples, and thus protects against overfitting.\nIn fact we can see this as a form of regularization: we build a cloud of images around each original image, all with the same label."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN Example: Pretrained Networks to Classify Images",
    "text": "CNN Example: Pretrained Networks to Classify Images\n\n\n\n\n\n\n\n\n\nHere we use the 50-layer resnet50 network trained on the 1000-class imagenet corpus to classify some photographs.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-imdb-movie-reviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-imdb-movie-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification: IMDB Movie Reviews",
    "text": "Document Classification: IMDB Movie Reviews\nThe IMDB corpus consists of user-supplied movie ratings for a large collection of movies. Each has been labeled for sentiment as positive or negative. Here is the beginning of a negative review:\n\nThis has to be one of the worst films of the 1990s. When my friends & I were watching this film (being the target audience it was aimed at) we just sat & watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn …\n\nWe have labeled training and test sets, each consisting of 25,000 reviews, and each balanced with regard to sentiment.\nGoal: We want to build a classifier to predict the sentiment of a review."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#featurization-bag-of-words",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#featurization-bag-of-words",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Featurization: Bag-of-Words",
    "text": "Featurization: Bag-of-Words\nDocuments have different lengths and consist of sequences of words. How do we create features \\(X\\) to characterize a document?\n\nFrom a dictionary, identify the 10K most frequently occurring words.\nCreate a binary vector of length \\(p = 10K\\) for each document, and score a 1 in every position that the corresponding word occurred.\nWith \\(n\\) documents, we now have an \\(n \\times p\\) sparse feature matrix \\(\\mathbf{X}\\).\nWe compare a lasso logistic regression model to a two-hidden-layer neural network on the next slide. (No convolutions here!)\nBag-of-words are unigrams. We can instead use bigrams (occurrences of adjacent word pairs) and, in general, m-grams."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews",
    "text": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews\n\n\n\n\n\n\n\n\n\n\n\nSimpler lasso logistic regression model works as well as neural network in this case.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#recurrent-neural-networks---rnn-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#recurrent-neural-networks---rnn-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks - RNN",
    "text": "Recurrent Neural Networks - RNN\n\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#simple-recurrent-neural-network-architecture",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#simple-recurrent-neural-network-architecture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Recurrent Neural Network Architecture",
    "text": "Simple Recurrent Neural Network Architecture\n\n\n\n\n\n\n\n\n\n\nThe hidden layer is a sequence of vectors \\(A_\\ell\\), receiving as input \\(X_\\ell\\) as well as \\(A_{\\ell-1}\\). \\(A_\\ell\\) produces an output \\(O_\\ell\\).\nThe same weights \\(\\mathbf{W}\\), \\(\\mathbf{U}\\), and \\(\\mathbf{B}\\) are used at each step in the sequence — hence the term recurrent.\nThe \\(A_\\ell\\) sequence represents an evolving model for the response that is updated as each element \\(X_\\ell\\) is processed."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-in-detail",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN in Detail",
    "text": "RNN in Detail\n\nSuppose \\(X_\\ell = (X_{\\ell1}, X_{\\ell2}, \\ldots, X_{\\ell p})\\) has \\(p\\) components, and \\(A_\\ell = (A_{\\ell1}, A_{\\ell2}, \\ldots, A_{\\ell K})\\) has \\(K\\) components. Then the computation at the \\(k\\)-th components of hidden unit \\(A_\\ell\\) is:\n\\[\nA_{\\ell k} = g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_{\\ell j} + \\sum_{s=1}^{K} u_{ks} A_{\\ell-1,s}\\right)\n\\]\n\\[\nO_\\ell = \\beta_0 + \\sum_{k=1}^{K} \\beta_k A_{\\ell k}\n\\]\nOften we are concerned only with the prediction \\(O_L\\) at the last unit. For squared error loss, and \\(n\\) sequence/response pairs, we would minimize:\n\\[\n\\sum_{i=1}^{n} (y_i - o_{iL})^2 = \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} x_{iL,j} + \\sum_{s=1}^{K} u_{ks} a_{i,L-1,s}\\right)\\right)\\right)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-for-document-classification-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-for-document-classification-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN for Document Classification",
    "text": "RNN for Document Classification\n\nThe document feature is a sequence of words \\(\\{\\mathcal{W}_\\ell\\}_{1}^{L}\\). We typically truncate/pad the documents to the same number \\(L\\) of words (we use \\(L = 500\\)).\nEach word \\(\\mathcal{W}_\\ell\\) is represented as a one-hot encoded binary vector \\(X_\\ell\\) (dummy variable) of length \\(10K\\), with all zeros and a single one in the position for that word in the dictionary.\nThis results in an extremely sparse feature representation and would not work well.\nInstead, we use a lower-dimensional pretrained word embedding matrix \\(\\mathbf{E}\\) (\\(m \\times 10K\\), next slide).\nThis reduces the binary feature vector of length \\(10K\\) to a real feature vector of dimension \\(m \\ll 10K\\) (e.g., \\(m\\) in the low hundreds)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Word Embedding - RNN Example: IMDB Reviews",
    "text": "Word Embedding - RNN Example: IMDB Reviews\n\nReview:\n\nthis is one of the best films actually the best I have ever seen the film starts one fall day…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings are pretrained on very large corpora of documents, using methods similar to principal components. word2vec and GloVe are popular.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-time-series-forecasting",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-time-series-forecasting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN: Time Series Forecasting",
    "text": "RNN: Time Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew-York Stock Exchange Data\nThree daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#autocorrelation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#autocorrelation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\n\n\n\n\n\n\nThe autocorrelation at lag \\(\\ell\\) is the correlation of all pairs \\((v_t, v_{t-\\ell})\\) that are \\(\\ell\\) trading days apart.\nThese sizable correlations give us confidence that past values will be helpful in predicting the future.\nThis is a curious prediction problem: the response \\(v_t\\) is also a feature \\(v_{t-\\ell}\\)!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-forecaster",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Forecaster",
    "text": "RNN Forecaster\nWe only have one series of data! How do we set up for an RNN?\nWe extract many short mini-series of input sequences \\(\\mathbf{X} = \\{ X_1, X_2, \\ldots, X_L \\}\\) with a predefined length \\(L\\) known as the lag:\n\\[\nX_1 = \\begin{pmatrix}\nv_{t-L} \\\\\nr_{t-L} \\\\\nz_{t-L}\n\\end{pmatrix}, \\quad\nX_2 = \\begin{pmatrix}\nv_{t-L+1} \\\\\nr_{t-L+1} \\\\\nz_{t-L+1}\n\\end{pmatrix}, \\quad\n\\cdots, \\quad\nX_L = \\begin{pmatrix}\nv_{t-1} \\\\\nr_{t-1} \\\\\nz_{t-1}\n\\end{pmatrix}, \\quad \\text{and} \\quad Y = v_t.\n\\]\nSince \\(T = 6,051\\), with \\(L = 5\\), we can create 6,046 such \\((X, Y)\\) pairs.\nWe use the first 4,281 as training data, and the following 1,770 as test data. We fit an RNN with 12 hidden units per lag step (i.e., per \\(A_\\ell\\))."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-results-for-nyse-data",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Results for NYSE Data",
    "text": "RNN Results for NYSE Data\n\n\n\n\n\n\n\n\n\nThe figure shows predictions and truth for the test period.\n\\[\nR^2 = 0.42 \\text{ for RNN}\n\\]\n\\(R^2 = 0.18\\) for the naive approach — uses yesterday’s value of Log trading volume to predict that of today.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-forecaster",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Forecaster",
    "text": "Autoregression Forecaster\nThe RNN forecaster is similar in structure to a traditional autoregression procedure.\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\nv_{L+1} \\\\\nv_{L+2} \\\\\nv_{L+3} \\\\\n\\vdots \\\\\nv_T\n\\end{bmatrix}, \\quad\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & v_L & v_{L-1} & \\cdots & v_1 \\\\\n1 & v_{L+1} & v_L & \\cdots & v_2 \\\\\n1 & v_{L+2} & v_{L+1} & \\cdots & v_3 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & v_{T-1} & v_{T-2} & \\cdots & v_{T-L}\n\\end{bmatrix}.\n\\]\nFit an OLS regression of \\(\\mathbf{y}\\) on \\(\\mathbf{M}\\), giving:\n\\[\n\\hat{v}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 v_{t-1} + \\hat{\\beta}_2 v_{t-2} + \\cdots + \\hat{\\beta}_L v_{t-L}.\n\\]\nKnown as an order-\\(L\\) autoregression model or \\(AR(L)\\).\nFor the NYSE data, we can include lagged versions of DJ_return and log_volatility in matrix \\(\\mathbf{M}\\), resulting in \\(3L + 1\\) columns."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-results-for-nyse-data",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Results for NYSE Data",
    "text": "Autoregression Results for NYSE Data\n\n\\(R^2 = 0.41 \\text{ for } AR(5) \\text{ model (16 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for RNN model (205 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for } AR(5) \\text{ model fit by neural network.}\\)\n\\(R^2 = 0.46 \\text{ for all models if we include } \\textbf{day_of_week} \\text{ of day being predicted.}\\)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#summary-of-rnns",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#summary-of-rnns",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary of RNNs",
    "text": "Summary of RNNs\n\nWe have presented the simplest of RNNs. Many more complex variations exist.\nOne variation treats the sequence as a one-dimensional image, and uses CNNs for fitting. For example, a sequence of words using an embedding representation can be viewed as an image, and the CNN convolves by sliding a convolutional filter along the sequence.\nCan have additional hidden layers, where each hidden layer is a sequence, and treats the previous hidden layer as an input sequence.\nCan have output also be a sequence, and input and output share the hidden units. So called seq2seq learning are used for language translation."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#when-to-use-deep-learning-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#when-to-use-deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\n\nCNNs have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.\nRNNs have had big wins in speech modeling, language translation, and forecasting.\n\n\nShould we always use deep learning models?\n\nOften the big successes occur when the signal to noise ratio is high — e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.\nFor noisier data, simpler models can often work better:\n\nOn the NYSE data, the AR(5) model is much simpler than an RNN, and performed as well.\nOn the IMDB review data, a linear model fit (e.g. with glmnet) did as well as the neural network, and better than the RNN."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\nAs the authors suggest, I also endorse the Occam’s razor principle — we prefer simpler models if they work as well. More interpretable!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#additional-material",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Additional Material",
    "text": "Additional Material\n\n3Blue1Brown: Neural Networks\nDeep Learning, by Ian Goodfellow and Yoshua Bengio and Aaron Courvill\nWelch Labs: Neural Networks Demystified\nWelch Labs: Learning To See\nDistill: A Gentle Introduction to Graph Neural Networks\nNeural Networks and Deep Learning, by Michael Nielsen\nCITS4012 Natural Language Processing\nDeep Learning with PyTorch Step-by-Step"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#summary",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nDeep Learning Renaissance\n\nNeural networks first rose to prominence in the 1980s, waned in the 1990s, then surged again around 2010.\nAdvances in computing (GPUs) and availability of massive labeled datasets propelled deep learning success.\n\nFrameworks (PyTorch vs. TensorFlow)\n\nPyTorch is known for its dynamic graph and Pythonic syntax; widely used in research.\nTensorFlow has an extensive production ecosystem, ideal for enterprise and deployment.\n\nEssential Concepts\n\nAutomatic differentiation, gradient descent, and backpropagation are at the core of training neural networks.\n\n\n\n\n\n\nCNNs and RNNs\n\nCNNs excel in image classification by learning local patterns via convolution and pooling layers.\n\nRNNs (and variants like LSTM, GRU) handle sequential data for tasks like language modeling and time-series forecasting.\n\nWhen to Use Deep Learning\n\nWorks best on large datasets with high signal-to-noise ratio (e.g., image, text).\n\nSimpler models often perform well on noisier tasks or smaller datasets.\n\nOver-parameterization can still generalize due to “double-descent” effects.\n\nPractical Tips\n\nUse regularization (dropout, data augmentation, weight decay) to mitigate overfitting.\n\nMonitor convergence with appropriate learning rates and consider mini-batch stochastic gradient descent."
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, time series, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 1007\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required):\n\nGoogle Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\nMicrosoft Copilot: is an AI-powered assistant designed to enhance productivity and streamline workflows across various applications and services. It utilizes large language models and is integrated within Microsoft 365 apps like Word, Excel, PowerPoint, Outlook, and Teams, providing real-time, context-aware assistance for tasks such as drafting documents, analyzing data, managing projects, and communicating more efficiently. Users can leverage Copilot to automate repetitive tasks, generate ideas, summarize information, and access data across their work environment and the web, all within a secure and privacy-conscious framework.\n\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance\n3%\n\n\nParticipation\n7%\n\n\nQuizzes\n10%\n\n\nHomework\n30%\n\n\nCourse Case Competition\n20%\n\n\nFinal Project\n30%\n\n\n\n\nAttendance\nAttend your classes. If you do not attend class, you will likely not succeed. The instructor will take attendance and keep the attendance record.\n\n\nParticipation\nParticipate in activities, and complete any participatory exercises. Almost every class the instructor you will be required to take participatory activities. Your participation grade will be based on this record.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nCourse Case Competition\nWwe will have a semester-long, team-based predictive analytics competition hosted on Kaggle. Students must work in teams and will be allowed up to five submissions per day. The Kaggle platform will automatically evaluate submissions and maintain a leaderboard throughout the competition period.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (30%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPeer Review (10%): Each group will review and provide constructive feedback on other teams’ posters. This process encourages engagement, enhances critical analysis skills, and promotes a culture of constructive critique.\nPoster Presentation at the Purdue Undergraduate Research Conference (20%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\nInstructor/TA Evaluation (20%): After the Undergraduate Research Conference your instructor and the TA will evaluate your final submission based on a rubric that will be shared.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nReadings ISLP\nMaterial*\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction.\nCh. 1; Ch. 2;\nslidespodcast**video**book lab\n- Video: Statistical Learning: 2.1 Introduction to Regression Models- Video: Statistical Learning: 2.2 Dimensionality and Structured Models- Video: Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff- Video: Statistical Learning: 2.4 Classification- Video: Statistical Learning: 2.Py Data Types, Arrays, and Basics - 2023- Video: Statistical Learning: 2.Py.3 Graphics - 2023- Video: Statistical Learning: 2.Py Indexing and Dataframes - 2023\n\n\nWeek 2\nLinear Regression\nCh. 3.\nslidespodcastvideonotebookbook lab\n- Video: Statistical Learning: 3.1 Simple linear regression- Video: Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals- Video: Statistical Learning: 3.3 Multiple Linear Regression- Video: Statistical Learning: 3.4 Some important questions- Video: Statistical Learning: 3.5 Extensions of the Linear Model- Video: Statistical Learning: 3.Py Linear Regression and statsmodels Package - 2023- Video: Statistical Learning: 3.Py Multiple Linear Regression Package - 2023- Video: Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023\n\n\nWeek 3\nClassification\nCh. 4\nslidespodcastvideonotebookbook lab\n- Video: Statistical Learning: 4.1 Introduction to Classification Problems- Video: Statistical Learning: 4.2 Logistic Regression- Video: Statistical Learning: 4.3 Multivariate Logistic Regression- Video: Statistical Learning: 4.4 Logistic Regression Case Control Sampling and Multiclass- Video: Statistical Learning: 4.5 Discriminant Analysis- Video: Statistical Learning: 4.6 Gaussian Discriminant Analysis (One Variable)- Video: Statistical Learning: 4.7 Gaussian Discriminant Analysis (Many Variables)- Video: Statistical Learning: 4.8 Generalized Linear Models- Video: Statistical Learning: 4.9 Quadratic Discriminant Analysis and Naive Bayes- Video: Statistical Learning: 4.Py Logistic Regression I 2023- Video: Statistical Learning: 4.Py Linear Discriminant Analysis (LDA) I 2023- Video: Statistical Learning: 4.Py K-Nearest Neighbors (KNN) I 2023\n\n\nWeek 4\nResampling Methods\nCh. 5\nslidespodcastvideobook lab\n- Video: Statistical Learning: 5.1 Cross Validation- Video: Statistical Learning: 5.2 K-fold Cross Validation- Video: Statistical Learning: 5.3 Cross Validation the wrong and right way- Video: Statistical Learning: 5.4 The Bootstrap- Video: Statistical Learning: 5.5 More on the Bootstrap- Video: Statistical Learning: 5.Py Cross-Validation I 2023- Video: Statistical Learning: 5.Py Bootstrap I 2023- Book Chapter: Modern Dive -Bootstrapping and Confidence Intervals\n\n\nWeek 5\nLinear Model Selection & Regularization\nCh. 6\nslidespodcastvideobook lab\n- Video: Statistical Learning: 6.1 Introduction and Best Subset Selection- Video: Statistical Learning: 6.2 Stepwise Selection- Video: Statistical Learning: 6.3 Backward stepwise selection- Video: Statistical Learning: 6.4 Estimating test error- Video: Statistical Learning: 6.5 Validation and cross validation- Video: Statistical Learning: 6.6 Shrinkage methods and ridge regression- Video: Statistical Learning: 6.7 The Lasso- Video: Statistical Learning: 6.8 Tuning parameter selection- Video: Statistical Learning: 6.9 Dimension Reduction Methods- Video: Statistical Learning: 6.10 Principal Components Regression and Partial Least Squares- Video: Statistical Learning: 6.Py Stepwise Regression I 2023- Video: Statistical Learning: 6.Py Ridge Regression and the Lasso I 2023\n\n\nWeek 6\nBeyond Linearity\nCh. 7\nslidespodcastvideobook lab\n- Video: Statistical Learning: 7.1 Polynomials and Step Functions- Video: Statistical Learning: 7.2 Piecewise Polynomials and Splines- Video: Statistical Learning: 7.3 Smoothing Splines- Video: Statistical Learning: 7.4 Generalized Additive Models and Local Regression- Video: Statistical Learning: 7.Py Polynomial Regressions and Step Functions I 2023- Video: Statistical Learning: 7.Py Splines I 2023- Video: Statistical Learning: 7.Py Generalized Additive Models (GAMs) I 2023\n\n\nWeek 7\nTree-Based Methods\nCh. 8\nslidespodcastvideobook lab\n- Video: Statistical Learning: 8.1 Tree based methods- Video: Statistical Learning: 8.2 More details on Trees- Video: Statistical Learning: 8.3 Classification Trees- Video: Statistical Learning: 8.4 Bagging- Video: Statistical Learning: 8.5 Boosting- Video: Statistical Learning: 8.6 Bayesian Additive Regression Trees- Video: Statistical Learning: 8.Py Tree-Based Methods I 2023\n\n\nWeek 8\nTime Series\nLecture Material\nnotebookdatapodcastvideo\n- Book: Forecasting: Principles and Practice, the Pythonic Way- Videos: Forecasting: Principles and Practice - with R- Videos: Forecasting: Principles and Practice - with R- Book: Time series analysis with Python- PyTorch Forecasting Library- sktime Library- prophet Library- statsmodels Library- scikit-learn Library: Cross Calidation of Time Series Data- prophet Library\n\n\nWeek 9\nFinal Project\n.\n.\n.\n\n\nWeek 10\nElements of Data Communication\n.\nslidespodcast - TBPvideo TBP\nPoster TemplateRubricVideo: Creating a Professional Poster\n\n\nWeek 11\nFinal Project\n.\n.\n.\n\n\nWeek 12\nFinal Project\n.\n.\n.\n\n\nWeek 13\nFinal Project\n.\n.\n.\n\n\nWeek 14\nDeep Learning\nCh. 10\nslidespodcast - TBPvideo TBPbook labcourse lab\n- Video: Statistical Learning: 10.1 Introduction to Neural Networks- Video: Statistical Learning: 10.2 Convolutional Neural Networks- Video: Statistical Learning: 10.3 Document Classification- Video: Statistical Learning: 10.4 Recurrent Neural Networks- Video: Statistical Learning: 10.6 Fitting Neural Networks- Video: Statistical Learning: 10.7 Interpolation and Double Descent- Video: Statistical Learning: 10.Py Single Layer Model: Hitters Data I 2023- Video: Statistical Learning: 10.Py Multilayer Model: MNIST Digit Data I 2023- Video: Statistical Learning: 10.Py Convolutional Neural Network: CIFAR Image Data I 2023- Video: Statistical Learning: 10.Py Document Classification and Recurrent Neural Networks I 2023\n\n\nWeek 15\nDeep Learning\nCh. 10\n.\n.\n\n\nWeek 16\nSpecial Topic: LLM\n.\nslides - TBPpodcast - TBPvideo TBPlab - TBP\n.\n\n\n\n* The majority of the course slides and labs are based on the ISLP book, “An Introduction to Statistical Learning with Applications in Python” by James, G., Witten, D., Hastie, T., and Tibshirani, R., and have been adapted to suit the specific needs of our course. ** This material was generated with Google NotebookLM based on the slides I prepared for the Predictive Analytics course.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overview",
    "href": "lecture_slides/01_introduction/01_introduction.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nSupervised Learning\nUnsupervised Learning\nStatistical Learning Overview\n\nWhat is Statistical Learning?\nParametric and Structured Models\nAssessing Model Accuracy\nClassification Problems\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Quantitative Methods Department at the Mitch. Daniels School of Business at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#students",
    "href": "lecture_slides/01_introduction/01_introduction.html#students",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Students",
    "text": "Students\n\n\nWhat do you expect to gain from this Predictive Analytics course?\n\n\n\nIt is your turn! - 10 minutes\nPresent yourself to your left/right colleague and ask:\nCollect her/his answer and submit your first Participation Assignment!"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\n\n\n\nCourse Info:\n\nBrightspace - Official.\nCourse Webpage - Supplementary.\n\nSyllabus\n\nClass Times & Location: check the course syllabus.\n\nOffice Hours: check the course syllabus for group and individual appointments.\n\nSchedule and Materials:\n\nPodcast (before class)\n\nRequired Readings (before class)\n\nLecture Slides (before class)\n\nLecture Video (during class)\n\nBook labs (during/after class)\n\nSupplementary Material (after class)\n\n\n\n\nCourse Tracks\n\nStandard Track\n\nExternal Case Competition (bonus)\n\nAssessments\n\nAttendance\n\nParticipation\n\nQuizzes\n\nHomework\n\nCourse Case Competition\n\nFinal Project"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "href": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Spam Detection",
    "text": "Spam Detection\n\n\n\nData from 4601 emails sent to an individual (named George, at HP Labs, before 2000). Each is labeled as spam or email.\nGoal: build a customized spam filter.\nInput features: relative frequencies of 57 of the most commonly occurring words and punctuation marks in these email messages.\n\n\n\n\nWord\nSpam\nEmail\n\n\n\n\ngeorge\n0.00\n1.27\n\n\nyou\n2.26\n1.27\n\n\nhp\n0.02\n0.90\n\n\nfree\n0.52\n0.07\n\n\n!\n0.51\n0.11\n\n\nedu\n0.01\n0.29\n\n\nremove\n0.28\n0.01\n\n\n\nAverage percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "href": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Zip Code",
    "text": "Zip Code\n\n\nIdentify the numbers in a handwritten zip code."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Netflix Prize",
    "text": "Netflix Prize\n\n\n\n\n\n\n\n\n\n\n\nVideo: Winning the Netflix Prize\nNetflix Prize - Wiki"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "href": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Starting point",
    "text": "Starting point\n\n\n\n\nOutcome measurement \\(Y\\) (also called dependent variable, response, target).\nVector of \\(p\\) predictor measurements \\(X\\) (also called inputs, regressors, covariates, features, independent variables).\nIn the regression problem, \\(Y\\) is quantitative (e.g., price, blood pressure).\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (e.g., survived/died, digit 0–9, cancer class of tissue sample).\nWe have training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "href": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Objectives",
    "text": "Objectives\nOn the basis of the training data, we would like to:\n\nAccurately predict unseen test cases.\nUnderstand which inputs affect the outcome, and how.\nAssess the quality of our predictions and inferences."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "href": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nIt is important to understand the ideas behind the various techniques, in order to know how and when to use them.\nWe wil understand the simpler methods first to grasp the more sophisticated ones later.\nIt is important to accurately assess the performance of a method, to know how well or how badly it is working."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\n\nDefinition:\nLearning the structure or patterns in data without labeled outputs.\nThe algorithm is unsupervised because no outcome variable guides the learning.\nGoal:\n\nDiscover hidden structures in \\(X\\).\n\nGroup, reduce, or represent data meaningfully.\n\n\n\n\nExamples of Methods:\n\nClustering: k-means, hierarchical clustering\n\nDimensionality Reduction: PCA, t-SNE\n\nApplications:\n\nCustomer segmentation\n\nMarket basket analysis\n\nReducing dimensionality of large datasets\n\nCharacteristics:\n\nDifficult to know how well we are doing.\n\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\n\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#notation",
    "href": "lecture_slides/01_introduction/01_introduction.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\n\n\n\n\n\n\n\n\n\n\nThere can be many \\(Y\\) values at \\(X=4\\). A good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\n\nIs also defined for a vector \\(\\mathbf{X}\\).\n\n\\[\nf(\\mathbf{x}) = f(x_1, x_2, x_3) = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\n\n\nIs the ideal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{is the function that minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error.\n\nEven if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is a distribution of possible \\(Y\\) values.\n\n\n\n\n\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "href": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large.\n\nReason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point \\((0, 0)\\).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point \\((0, 0)\\).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBotton panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\nRed points are simulated values for income from the model:\n\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\nLinear regression model fit to the simulated data:\n\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\n\nHere we use a technique called a thin-plate spline to fit a flexible surface. We control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "href": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. We tunned the parameter all the way down to zero and this surface actually goes through every single data point.\n\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "href": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso.\n\nIntermediate: Least squares, Generalized Additive Models, Trees.\n\nHigh flexibility: Support Vector Machines, Deep Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\), the Mean Squared Error (MSE):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\nBias (tends to underfit)\n\nDefinition: The error introduced by approximating a real-world problem with a simplified model. High bias implies underfitting.\n\nImplication: High bias ⇒ the model misses important patterns (systematic error).\n\nTypical causes: Too-simple model (e.g., overly rigid assumptions), insufficient features, heavy regularization.\n\n\nVariance (tends to overfit)\n\nDefinition: The amount by which the model’s prediction would change if trained on a different training dataset. High variance implies overfitting.\n\nImplication: High variance ⇒ the model is overly sensitive to noise (unstable across samples).\n\nTypical causes: Overly flexible model, too many features vs. observations, weak regularization.\n\n\n\n\n\n\n\n\nThe trade-off we manage\n\n\n\nIncreasing flexibility ↓ bias but ↑ variance.\n\nGoal: choose complexity that minimizes expected test error.\n\n\n\n\nDecomposition of expected test MSE\n\\[\n\\mathbb{E}\\big[(Y-\\hat f(X))^2\\big]\n   \\;=\\; \\underbrace{\\big(\\mathrm{Bias}[\\hat f(X)]\\big)^2}_{\\text{misspecification}}\n   \\;+\\; \\underbrace{\\mathrm{Var}[\\hat f(X)]}_{\\text{sensitivity}}\n   \\;+\\; \\underbrace{\\sigma^2}_{\\text{irreducible noise}}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Panel: Model Fits\n\nBlack Curve: The true generating function, representing the underlying relationship we want to estimate.\nData Points: Observations generated from the black curve, with added noise (error).\nFitted Models:\n\nOrange Line: A simple linear model (low flexibility).\nBlue Line: A moderately flexible model, likely a spline or thin plate spline.\nGreen Line: A highly flexible model that closely fits the data points but may overfit.\n\n\nKey Insight:\nThe green model captures the data points well but risks overfitting, while the orange model is too rigid and misses the underlying structure. The blue model strikes a balance.\n\nBotton Panel: Mean Squared Error (MSE)\n\nGray Curve: Training data MSE.\n\nDecreases consistently as flexibility increases.\nFlexible models fit the training data well, but this does not generalize to test data.\n\nRed Curve: Test data MSE across models of increasing flexibility.\n\nStarts high for rigid models (orange line).\nDecreases to a minimum (optimal model complexity, blue line).\nIncreases again for overly flexible models (green line), due to overfitting.\n\n\nKey Takeaway:\nThere is an optimal model complexity (the “magic point”) where test data MSE is minimized. Beyond this point, models become overly complex and generalization performance deteriorates."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off: Other Examples",
    "text": "Bias-Variance Trade-off: Other Examples\n\n\n\n\n\nHere, the truth is smoother, so smoother fits and linear models perform well.\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off of the Examples",
    "text": "Bias-Variance Trade-off of the Examples\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Problems",
    "text": "Classification Problems\n\n\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "href": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ideal Classifier and Bayes Decision Rule",
    "text": "Ideal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest-Neighbor Averaging",
    "text": "Nearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification: Some Details",
    "text": "Classification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "href": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: K-Nearest Neighbors in Two Dimensions",
    "text": "Example: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 10",
    "text": "KNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 1 vs. K = 100",
    "text": "KNN: K = 1 vs. K = 100\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN Error Rates",
    "text": "KNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure illustrates how training errors (blue curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line is the bayes error, used as reference for comparison."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nStatistical Learning and Predictive Analytics\n\nGoal: Build models to predict outcomes and understand relationships between inputs (predictors) and responses.\nSupervised Learning: Focuses on predicting \\(Y\\) (response) using \\(X\\) (predictors) via models like regression and classification.\nUnsupervised Learning: Focuses on finding patterns in data without predefined responses (e.g., clustering).\n\nBias-Variance Trade-off\n\nKey Trade-off: Model flexibility affects bias and variance:\n\nHigh flexibility → Low bias but high variance (overfitting).\nLow flexibility → High bias but low variance (underfitting).\n\nGoal: Find the optimal flexibility that minimizes test error.\n\n\nTechniques and Applications\n\nParametric Models:\n\nSimpler and interpretable (e.g., linear regression).\nOften used as approximations.\n\nFlexible Models:\n\nHandle complex patterns (e.g., splines, SVMs, deep learning).\nRequire careful tuning to avoid overfitting.\n\n\nPractical Considerations\n\nAssessing Model Accuracy:\n\nUse test data to calculate MSE.\nBalance between training performance and generalizability.\n\n\nKey Challenges\n\nCurse of Dimensionality:\n\nHigh-dimensional data affects distance-based methods like KNN.\nLarger neighborhoods needed, losing “locality.”"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#supervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#supervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\n\n\n\nDefinition:\nLearning a mapping from inputs \\(X\\) to an output \\(Y\\) using labeled data.\nThe algorithm is supervised because the correct answers are known during training.\nGoal:\n\nPredict \\(Y\\) for new unseen \\(X\\).\n\nMinimize prediction error.\n\n\n\n\nExamples of Methods:\n\nLinear Regression, Logistic Regression\n\nDecision Trees, Random Forests, SVMs\n\nNeural Networks\n\nApplications:\n\nPredicting stock prices\n\nDiagnosing diseases\n\nSpam email detection"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#overview",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nRecap\nLinear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\n\nLinear regression is a simple approach to supervised learning. It assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\ldots, X_p\\) is linear.\nTrue regression functions are never linear!\n\n\n\n\n\n\n\n\n\n\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-for-the-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression for the Advertising Data",
    "text": "Linear Regression for the Advertising Data\n\n\nConsider the advertising data shown:\n\n\n\n\n\n\n\n\n\nSuppose that, in our job as business analysts, we are asked to suggest, based on this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation?"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Linear Regression using a single predictor \\(X\\)",
    "text": "Simple Linear Regression using a single predictor \\(X\\)\n\n\n\nWe assume a model:\n\n\\[\n  Y = \\beta_0 + \\beta_1X + \\epsilon,\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are two unknown constants that represent the intercept and slope, also known as coefficients or parameters, and \\(\\epsilon\\) is the error term.\n\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the model coefficients, we predict future sales using:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X = x\\). The hat symbol denotes an estimated value."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation of the parameters by least squares",
    "text": "Estimation of the parameters by least squares\n\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) be the prediction for \\(Y\\) based on the \\(i\\)th value of \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nWe define the residual sum of squares (RSS) as:\n\n\\[\n    RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,\n\\]\nor equivalently as:\n\\[\n    RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\nThe least squares approach selects the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS. The minimizing values can be shown to be:\n\n\\[\n    \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad\n    \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\nwhere \\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\) and \\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\) are the sample means."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#example-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#example-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Advertising Data",
    "text": "Example: Advertising Data\n\n\n\n\n\n\n\n\n\n\n\nThe least squares fit for the regression of sales onto TV is shown. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Accuracy of the Coefficient Estimates",
    "text": "Assessing the Accuracy of the Coefficient Estimates\n\n\n\nThe standard error of an estimator reflects how it varies under repeated sampling:\n\n\\[\n  SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right].\n\\]\nwhere \\(\\sigma^2 = Var(\\epsilon)\\)\n\n\nThese standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. It has the form:\n\n\\[\n  \\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1).\n\\]\n\n\n\nThere is approximately a 95% chance that the interval:\n\n\\[\n  \\left[ \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1) \\right]\n\\]\nwill contain the true value of \\(\\beta_1\\) (under a scenario where we obtained repeated samples like the present sample).\n\nFor the advertising data, the 95% confidence interval for \\(\\beta_1\\) is:\n\n\\[\n  [0.042, 0.053].\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nStandard errors can be used to perform hypothesis tests on coefficients. The most common hypothesis test involves testing the null hypothesis:\n\n\\[\n  H_0: \\text{There is no relationship between } X \\text{ and } Y\n\\] versus the alternative hypothesis:\n\\[\n  H_A: \\text{There is some relationship between } X \\text{ and } Y.\n\\]\n\n\nMathematically, this corresponds to testing:\n\n\\[\n  H_0: \\beta_1 = 0\n\\] versus:\n\\[\n  H_A: \\beta_1 \\neq 0,\n\\]\nsince if \\(\\beta_1 = 0\\), then the model reduces to \\(Y = \\beta_0 + \\epsilon\\), and \\(X\\) is not associated with \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nTo test the null hypothesis (\\(H_0\\)), compute a \\(t\\)-statistic as follows:\n\n\\[\n  t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.\n\\]\n\nThe \\(t\\)-statistic follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom under the null hypothesis (\\(\\beta_1 = 0\\)).\nUsing statistical software, we can compute the \\(p\\)-value to determine the likelihood of observing a \\(t\\)-statistic as extreme as the one calculated."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-the-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for the Advertising Data",
    "text": "Results for the Advertising Data\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Overall Accuracy of the Model",
    "text": "Assessing the Overall Accuracy of the Model\n\n\n\nResidual Standard Error (RSE):\n\n\\[\n  RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\] where the Residual Sum of Square (RSS) is \\(\\sum_{i=1}^n (y_i - \\hat{y})^2\\).\n\n\n\\(R^2\\), the fraction of variance explained:\n\n\\[\n  R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\] where TSS is the Total Sums of Squares.\n\n\n\nIt can be shown that in this Simple Linear Regression setting that \\(R^2 = r^2\\), where \\(r\\) is the correlation between X and Y:\n\n\\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#advertising-data-results",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#advertising-data-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advertising Data Results",
    "text": "Advertising Data Results\n\nKey metrics for model accuracy:\n\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n3.26\n\n\nR²\n0.612\n\n\nF-statistic\n312.1"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#multiple-linear-regression-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#multiple-linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nHere our model is\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon,\n\\]\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\nIn the advertising example, the model becomes\n\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon.\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interpreting-regression-coefficients",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interpreting-regression-coefficients",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpreting Regression Coefficients",
    "text": "Interpreting Regression Coefficients\n\nThe ideal scenario is when the predictors are uncorrelated — a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed” are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically.\nInterpretations become hazardous — when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation and Prediction for Multiple Regression",
    "text": "Estimation and Prediction for Multiple Regression\n\n\n\nGiven estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\), we can make predictions using the formula:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + \\cdots + \\hat{\\beta}_px_p.\n\\]\n\n\nWe estimate \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) as the values that minimize the sum of squared residuals:\n\n\\[\n  \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2\n             = \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_{i1} - \\hat{\\beta}_2x_{i2} - \\cdots - \\hat{\\beta}_px_{ip} \\right)^2.\n\\]\n\nThis is done using standard statistical software. The values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Advertising Data",
    "text": "Results for Advertising Data\n\n\n\nRegression Coefficients\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599\n\n\n\n\n\n\nCorrelations\n\n\n\n\nPredictor\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n\n\n1.0000\n0.2283\n\n\nsales\n\n\n\n1.0000"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#some-important-questions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#some-important-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Important Questions",
    "text": "Some Important Questions\n\n\nIs at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response?\nDo all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#is-at-least-one-predictor-useful",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#is-at-least-one-predictor-useful",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is at Least One Predictor Useful?",
    "text": "Is at Least One Predictor Useful?\nFor the first question, we can use the F-statistic:\n\\[\nF = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n1.69\n\n\n\\(R^2\\)\n0.897\n\n\nF-statistic\n570\n\n\n\n\nThe F-statistic is huge and it’s p-value is less than \\(.0001\\). This says that there’s a strong association of the predictors on the outcome variable."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#forward-selection",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#forward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Selection",
    "text": "Forward Selection\n\nBegin with the null model — a model that contains an intercept but no predictors.\nFit \\(p\\) Simple Linear Regressions and add to the null model the variable that results in the lowest RSS.\nAdd to that model the variable that results in the lowest RSS amongst all two-variable models.\nContinue until some stopping rule is satisfied:\n\nFor example, when all remaining variables have a p-value above some threshold."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#backward-selection",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#backward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Selection",
    "text": "Backward Selection\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new (\\(p - 1\\))-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached:\n\nFor instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#model-selection",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#model-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Model Selection",
    "text": "Model Selection\n\nWe will discuss other criterias for choosing an “optimal” member in the path of models produced by forward or backward stepwise selection, including:\n\nMallow’s \\(C_p\\)\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\nAdjusted \\(R^2\\)\nCross-validation (CV)"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nSome predictors are qualitative, taking discrete values.\nCategorical predictors can be represented using factor variables.\nQualitative variables: Student (Student Status), Status (Marital Status), Own (Owns a House)."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\nSuppose we investigate differences in credit card balance between those who own a house and those who do not, ignoring the other variables. We create a new variable:\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if } i\\text{th person owns a house} \\\\\n0 & \\text{if } i\\text{th person does not own a house}\n\\end{cases}\n\\]\nResulting model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if } i\\text{th person owns a house} \\\\\n\\beta_0 + \\epsilon_i & \\text{if } i\\text{th person does not own a house.}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-2",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\n\nResults for the model:\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n509.80\n33.13\n15.389\n&lt; 0.0001\n\n\nOwn [Yes]\n19.73\n46.05\n0.429\n0.6690\n\n\n\n\nWe see the coefficient is 19.73, but it’s not significant. The p value is 0.66 which is not significant (&gt; 0.05). So, owing a house don’t generally means a higher credit card balance than not owing one."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors with More Than Two Levels",
    "text": "Qualitative Predictors with More Than Two Levels\n\nWith more than two levels, we create additional dummy variables.\nFor example, for the `region`` variable, we create two dummy variables:\n\\[\nx_{i1} =\n\\begin{cases}\n      1 & \\text{if i-th person is from the South} \\\\\n      0 & \\text{if i-th person is not from the South}\n    \\end{cases}\n\\]\n\\[\nx_{i2} = \\begin{cases}\n      1 & \\text{if i-th person is from the West} \\\\\n      0 & \\text{if i-th person is not from the West}\n    \\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nBoth variables can be used in the regression equation to obtain the model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i =\n\\begin{cases}\n      \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if i-th person is from the South} \\\\\n      \\beta_0 + \\beta_2 + \\epsilon_i & \\text{if i-th person is from the West}\\\\\n      \\beta_0 + \\epsilon_i & \\text{if i-th person is from the East (baseline)}\n    \\end{cases}\n\\] \nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — East in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-ethnicity",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-ethnicity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Ethnicity",
    "text": "Results for Ethnicity\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n531.00\n46.32\n11.464\n&lt; 0.0001\n\n\nethnicity \\(South\\)\n-18.69\n65.02\n-0.287\n0.7740\n\n\nethnicity \\(West\\)\n-12.50\n56.68\n-0.221\n0.8260\n\n\n\n\nThe coefficient -18.69 compares South to East and that’s not significant. Likewise, the West to East is also not significant.\n\nNote: the choice of the baseline does not affect the fit of the model. The residual sum of sum of squares will be the same no matter which category we chose as the baseline. At its turn, the p-values will potentially change as we change the baseline category."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\nIn our previous analysis of the Advertising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media.\n\nFor example, the linear model\n\\[\n\\widehat{\\text{sales}} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper}\n\\]\nstates that the average effect on sales of a one-unit increase in TV is always \\(\\beta_1\\), regardless of the amount spent on radio."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\n\nBut suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\nIn this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\nIn marketing, this is known as a synergy effect, and in statistics, it is referred to as an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interaction-in-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interaction-in-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interaction in Advertising Data",
    "text": "Interaction in Advertising Data\n\n\nWhen levels of TV or radio are low, true sales are lower than predicted.\nSplitting advertising between TV and radio underestimates sales."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#modeling-interactions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#modeling-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Modeling Interactions",
    "text": "Modeling Interactions\nModel takes the form:\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{radio} \\times \\text{TV}) + \\epsilon\n\\]\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n6.7502\n0.248\n27.23\n&lt; 0.0001\n\n\nTV\n0.0191\n0.002\n12.70\n&lt; 0.0001\n\n\nradio\n0.0289\n0.009\n3.24\n0.0014\n\n\nTV × radio\n0.0011\n0.000\n20.73\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interpretation",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe results in this table suggest that interactions are important.The p-value for the interaction term TV \\(\\times\\) radio is extremely low, indicating that there is strong evidence for ( H_A : \\(\\beta_3 \\neq 0\\)).\nThe ( \\(R^2\\) ) for the interaction model is 96.8%, compared to only 89.7% for the model that predicts sales using TV and radio without an interaction term.\nThis means that (\\(\\frac{96.8 - 89.7}{100 - 89.7}\\)) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\nThe coefficient estimates in the table suggest that an increase in TV advertising of $1,000 is associated with increased sales of (\\(\\hat{\\beta}_1 + \\hat{\\beta}_3 \\times \\text{radio}\\)) \\(\\times 1000 = 19 + 1.1 \\times \\text{radio} \\text{ units}.\\)\nAn increase in radio advertising of $1,000 will be associated with an increase in sales of (\\(\\hat{\\beta}_2 + \\hat{\\beta}_3 \\times \\text{TV}\\)) \\(\\times 1000 = 29 + 1.1 \\times \\text{TV} \\text{ units}.\\)"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#hierarchy",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchy",
    "text": "Hierarchy\n\nSometimes it is the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.\nThe hierarchy principle:\n\nIf we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.\n\nThe rationale for this principle is that interactions are hard to interpret in a model without main effects.\nSpecifically, the interaction terms also contain main effects, if the model has no main effect terms."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions Between Qualitative and Quantitative Variables",
    "text": "Interactions Between Qualitative and Quantitative Variables\nConsider the Credit data set, and suppose that we wish to predict balance using income (quantitative) and student (qualitative).\nWithout an interaction term, the model takes the form:\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]\n\\[\n= \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_0 + \\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n\\beta_0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#with-interactions-it-takes-the-form",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#with-interactions-it-takes-the-form",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "With Interactions, It Takes the Form",
    "text": "With Interactions, It Takes the Form\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\\[\n=\n\\begin{cases}\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & \\text{if student} \\\\\n\\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{if not student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#visualizing-interactions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#visualizing-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\n\n\nLeft: no interaction between income and student.\nRight: with an interaction term between income and student."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-effects-of-predictors-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-effects-of-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear effects of predictors",
    "text": "Non-linear effects of predictors\n\nPolynomial regression on Auto data"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-regression-results",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-regression-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear regression results",
    "text": "Non-linear regression results\nThe figure suggests that the following model\n\\[\nmpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon\n\\]\nmay provide a better fit.\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n56.9001\n1.8004\n31.6\n&lt; 0.0001\n\n\nhorsepower\n-0.4662\n0.0311\n-15.0\n&lt; 0.0001\n\n\n\\(\\text{horsepower}^2\\)\n0.0012\n0.0001\n10.1\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#what-we-did-not-cover",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#what-we-did-not-cover",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What we did not cover",
    "text": "What we did not cover\n\n\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#generalizations-of-the-linear-model",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#generalizations-of-the-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\n\nIn much of the rest of the course we discuss methods that expand the scope of linear models and how they are fit:\n\nClassification problems: logistic regression, support vector machines.\nNon-linearity: kernel smoothing, splines, generalized additive models; nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests, boosting (these also capture non-linearities).\nRegularized fitting: Ridge regression and lasso."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#summary-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nLinear Regression:\n\nA foundational supervised learning method.\nAssumes a linear relationship between predictors (\\(X\\)) and the response (\\(Y\\)).\nUseful for both prediction and understanding relationships.\n\nSimple vs. Multiple Regression:\n\nSimple regression: one predictor.\nMultiple regression: multiple predictors.\n\nKey Metrics:\n\nResidual Standard Error (RSE), \\(R^2\\), and F-statistic.\nConfidence intervals and hypothesis testing for coefficients.\n\n\n\n\nQualitative Predictors:\n\nUse dummy variables for categorical predictors.\nInterpret results based on chosen baselines.\n\nInteractions:\n\nModels with interaction terms (e.g., \\(X_1 \\times X_2\\)) capture synergistic effects.\n\nNon-linear Effects:\n\nPolynomial regression accounts for curvature in data.\n\nChallenges:\n\nMulticollinearity, outliers, high leverage points.\nOverfitting vs. underfitting: balance flexibility and interpretability."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#recap",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#recap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nWhat is Statistical Learning?\n\n\nA framework to learn the relationship between predictors \\(X\\) and an outcome \\(Y\\) for prediction and inference, using parametric (e.g., linear/logistic regression) and non-parametric methods (e.g., trees, kNN).\n\n\n\n\nWhat is the difference between supervised and unsupervised learning?\n\n\nSupervised: learn \\(f: X \\to Y\\) from labeled data to predict/explain \\(Y\\). Unsupervised: find structure in \\(X\\) without labels (e.g., clustering, dimensionality reduction).\n\n\n\n\nWhat is the curse of dimensionality?\n\n\nWith many features/predictors, data become sparse; distances degrade, required sample size grows rapidly, and generalization deteriorates, especially for local/partitioning methods.\n\n\n\n\n\nWhat is the difference between overfitting and underfitting?\n\n\nOverfitting: too flexible; captures noise (high variance); low training error, poor test error. Underfitting: too simple; misses signal (high bias); high error on both train and test.\n\n\n\n\nWhy assess accuracy on test data rather than training data?\n\n\nTraining accuracy is optimistically biased (evaluated on seen data). Test accuracy estimates out-of-sample performance and reveals overfitting."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#recap-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#recap-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recap",
    "text": "Recap\n\n\nWhat is Statistical Learning?\n\n\n\nA framework for estimating and interpreting the relationship between predictors \\(X\\) and an outcome \\(Y\\) for prediction and inference. Methods can be parametric (e.g., linear/logistic regression) or non-parametric (e.g., trees, kNN).\n\n\n\nWhat is the difference between supervised and unsupervised learning?\n\n\n\nSupervised: learn \\(f: X \\rightarrow Y\\) using labeled outcomes to predict or explain \\(Y\\).\nUnsupervised: find structure in \\(X\\) without labels (e.g., clustering, dimensionality reduction).\n\n\n\nWhat is the curse of dimensionality?\n\n\n\nAs the number of features grows, data become sparse and distances lose meaning; sample requirements rise exponentially, hurting generalization and making naive nearest-neighbor/partitioning methods unreliable.\n\n\n\n\nWhat is the difference between overfitting and underfitting?\n\n\n\nOverfitting: model captures noise (high variance), low training error, poor test performance.\nUnderfitting: model too simple (high bias), misses signal, high error on both train and test.\n\n\n\nWhy is test accuracy preferred over training accuracy?\n\n\n\nTraining accuracy is optimistically biased because the model is evaluated on data it has seen. Test (or CV) accuracy estimates out-of-sample performance, detecting overfitting and guiding model/parameter selection."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#participation",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#participation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Participation",
    "text": "Participation\n\n\nIn pairs, ask AI:\n\nIn the context of Statistical Learning, what is the fundamental difference between a regression problem and a classification problem in supervised learning, based on the nature of the outcome variable?\n\n\n\n\n\nPresent the response you got to your right/left classmate. Do your understanding converge?\nSubmit your participation response."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions",
    "text": "Guiding Questions\n\n\n\n\nIs there a relationship between advertising budget and sales?\nOur first goal is to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, one might argue that no money should be spent on advertising.\nHow strong is the relationship between advertising budget and sales?\nAssuming a relationship exists, how strong is it? Does knowledge of the advertising budget provide a lot of information about product sales?\nWhich media are associated with sales?\nAre all three media—TV, radio, and newspaper—associated with sales, or just one or two? We must separate the individual contribution of each medium to sales when money is spent on all three.\nHow large is the association between each medium and sales?\nFor every dollar spent on a particular medium, by what amount will sales increase? How accurately can we predict this amount?\n\n\n\nHow accurately can we predict future sales?\nFor any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?\nIs the relationship linear?\nIf the relationship between advertising expenditure and sales is approximately linear, then linear regression is appropriate. If not, we may need to transform the predictor(s) or the response so that linear regression can be used.\nIs there synergy among the advertising media?\nPerhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect; in statistics, it is called an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-13",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-13",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions (1–3)",
    "text": "Guiding Questions (1–3)\n\nIs there a relationship between advertising budget and sales?\nOur first goal is to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, one might argue that no money should be spent on advertising.\nHow strong is the relationship between advertising budget and sales?\nAssuming a relationship exists, how strong is it? Does knowledge of the advertising budget provide a lot of information about product sales?\nWhich media are associated with sales?\nAre all three media—TV, radio, and newspaper—associated with sales, or just one or two? We must separate the individual contribution of each medium to sales when money is spent on all three."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-45",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-45",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions (4–5)",
    "text": "Guiding Questions (4–5)\n\nHow large is the association between each medium and sales?\nFor every dollar spent on a particular medium, by what amount will sales increase? How accurately can we predict this amount?\nHow accurately can we predict future sales?\nFor any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-67",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-67",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions (6–7)",
    "text": "Guiding Questions (6–7)\n\nIs the relationship linear?\nIf the relationship between advertising expenditure and sales is approximately linear, then linear regression is appropriate. If not, we may need to transform the predictor(s) or the response so that linear regression can be used.\nIs there synergy among the advertising media?\nPerhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect; in statistics, it is called an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-guiding-questions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-guiding-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Marketing Plan: Guiding Questions",
    "text": "The Marketing Plan: Guiding Questions\n\n\n\n\nIs there a relationship between advertising budget and sales?\nOur first goal is to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, one might argue that no money should be spent on advertising.\nHow strong is the relationship between advertising budget and sales?\nAssuming a relationship exists, how strong is it? Does knowledge of the advertising budget provide a lot of information about product sales?\nWhich media are associated with sales?\nAre all three media—TV, radio, and newspaper—associated with sales, or just one or two? We must separate the individual contribution of each medium to sales when money is spent on all three.\nHow large is the association between each medium and sales?\nFor every dollar spent on a particular medium, by what amount will sales increase? How accurately can we predict this amount?\n\n\n\nHow accurately can we predict future sales?\nFor any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?\nIs the relationship linear?\nIf the relationship between advertising expenditure and sales is approximately linear, then linear regression is appropriate. If not, we may need to transform the predictor(s) or the response so that linear regression can be used.\nIs there synergy among the advertising media?\nPerhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect; in statistics, it is called an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Marketing Plan: Answers",
    "text": "The Marketing Plan: Answers\n\n\n\n1.Is there a relationship between sales and advertising budget?\nFit a multiple regression of sales on TV, radio, and newspaper, and test\n\\[\nH_0:\\ \\beta_{\\text{TV}}=\\beta_{\\text{radio}}=\\beta_{\\text{newspaper}}=0.\n\\]\nUsing the \\(F\\)-statistic, the very small \\(p\\)-value indicates clear evidence of a relationship between advertising and sales.\n\n2. How strong is the relationship?\nTwo accuracy measures:\n\nRSE: estimates the standard deviation of the response from the population regression line. For Advertising, RSE \\(\\approx\\) 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average. In the data set, the mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23 %.\n\\(R^2\\) (variance explained): records the percentage of variability in the response that is explained by the predictors. Predictors explain ~90% of the variance in sales.\n\n\n\n\n3. Which media are associated with sales?\nCheck \\(p\\)-values for each predictor’s \\(t\\)-statistic. The \\(p\\)-values for TV and radio are low, but newspaper is not, suggesting only TV and radio are related to sales.\n\n\n4. How large is the association between each medium and sales?\nWe used SEs to build 95% CIs for coefficients related to TV, Radio, Newspaper.\nTV and radio intervals are narrow and far from 0, which means a strong evidence of association. Newspaper CI includes 0, not significant given TV and radio.\n\n\n5. How accurately can we predict future sales?\nAccuracy depends on whether predicting an individual response, \\(Y=f(X)+\\epsilon\\), or the average response, \\(f(X)\\).\n\nIndividual \\(\\rightarrow\\) prediction interval\nAverage \\(\\rightarrow\\) confidence interval\n\n\nPrediction intervals are always wider because they include irreducible error uncertainty from \\(\\epsilon\\)."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#section-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#section-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "5. How accurately can we predict future sales?\nUse (3.21). Accuracy depends on whether predicting an individual response, \\(Y=f(X)+\\epsilon\\), or the average response, \\(f(X)\\) (Sec. 3.2.2).\n- Individual ⇒ prediction interval\n- Average ⇒ confidence interval\nPrediction intervals are always wider because they include irreducible error uncertainty from \\(\\epsilon\\).\n6. Is the relationship linear?\nResidual plots (Sec. 3.3.3) help diagnose nonlinearity; linear relationships yield residuals with no pattern. For Advertising, Figure 3.5 indicates a nonlinear effect; transformations (Sec. 3.3.2) can accommodate nonlinearity within linear regression.\n\n7. Is there synergy among the advertising media?\nStandard linear regression assumes additivity. This is interpretable but may be unrealistic. Include an interaction term (Sec. 3.3.2) to allow non-additive relationships; a small \\(p\\)-value on the interaction indicates synergy. For Advertising, Figure 3.5 suggests non-additivity; adding an interaction increases \\(R^2\\) substantially—from ~90% to almost 97%."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-2",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Marketing Plan: Answers",
    "text": "The Marketing Plan: Answers\n\n\n\n6. Is the relationship linear?\nResidual plots help diagnose nonlinearity; linear relationships yield residuals with no pattern. Transformations can accommodate nonlinearity within linear regression.\n\n\n\n7. Is there synergy among the advertising media?\nStandard linear regression assumes additivity. This is interpretable but may be unrealistic. An interaction term allows non-additive relationships; a small \\(p\\)-value on the interaction indicates synergy. For Advertising, adding an interaction increases \\(R^2\\) substantially — from ~90% to almost 97%."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#overview",
    "href": "lecture_slides/03_classification/03_classification.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroduction to Classification\nLinear versus Logistic Regression\nMaking Predictions\nMultinomial Logistic Regression\n\n\n\nDiscriminant Analysis\nLinear Discriminant Analysis when \\(p &gt; 1\\)\nTypes of errors\nOther Forms of Discriminant Analysis\nNaive Bayes\nGeneralized Linear Models\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#what-is-a-classification-problem",
    "href": "lecture_slides/03_classification/03_classification.html#what-is-a-classification-problem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is a classification problem?",
    "text": "What is a classification problem?\n\n\n\n\nClassification involves categorizing data into predefined classes or groups based on their features."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#classification",
    "href": "lecture_slides/03_classification/03_classification.html#classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification",
    "text": "Classification\n\nQualitative variables take values in an unordered set \\(C\\), such as:\n\n\\(\\text{eye color} \\in \\{\\text{brown}, \\text{blue}, \\text{green}\\}\\)\n\\(\\text{email} \\in \\{\\text{spam}, \\text{ham}\\}\\)\n\nGiven a feature vector \\(X\\) and a qualitative response \\(Y\\) taking values in the set \\(C\\), the classification task is to build a function \\(C(X)\\) that takes as input the feature vector \\(X\\) and predicts its value for \\(Y\\); i.e. \\(C(X) \\in C\\).\nOften, we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(C\\).\n\nFor example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification as fraudulent or not."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-credit-card-default",
    "href": "lecture_slides/03_classification/03_classification.html#example-credit-card-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Card Default",
    "text": "Example: Credit Card Default\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot of income vs. balance with markers indicating whether a person defaulted (e.g., “+” for defaulted, “o” for not defaulted).\n\n\n\n\n\n\n\n\n\n\n\nBoxplots comparing balance and income for default (“Yes”) vs. no default (“No”)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#can-we-use-linear-regression",
    "href": "lecture_slides/03_classification/03_classification.html#can-we-use-linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can we use Linear Regression?",
    "text": "Can we use Linear Regression?\nSuppose for the Default classification task that we code:\n\\[\nY =\n\\begin{cases}\n0 & \\text{if No} \\\\\n1 & \\text{if Yes.}\n\\end{cases}\n\\]\nCan we simply perform a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y} &gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression does a good job as a classifier and is equivalent to linear discriminant analysis, which we discuss later.\nSince in the population \\(E(Y|X = x) = \\Pr(Y = 1|X = x)\\), we might think that regression is perfect for this task.\nHowever, linear regression might produce probabilities less than zero or greater than one. Logistic regression is more appropriate."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-probability-of-default",
    "href": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-probability-of-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression: Probability of Default",
    "text": "Linear versus Logistic Regression: Probability of Default\n\n\n\nThe orange marks indicate the response \\(Y\\), either 0 or 1.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression does not estimate \\(\\Pr(Y = 1|X)\\) well.\n\n\n\nLogistic regression seems well-suited to the task."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-regression-continued",
    "href": "lecture_slides/03_classification/03_classification.html#linear-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression continued",
    "text": "Linear Regression continued\n\n\n\nNow suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke;} \\\\\n2 & \\text{if drug overdose;} \\\\\n3 & \\text{if epileptic seizure.}\n\\end{cases}\n\\]\nThis coding suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure.\nLinear regression is not appropriate here. Multiclass Logistic Regression or Discriminant Analysis are more appropriate."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet’s write \\(p(X) = \\Pr(Y = 1|X)\\) for short and consider using balance to predict default. Logistic regression uses the form:\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\n\\((e \\approx 2.71828)\\) is a mathematical constant Euler’s number.\nIt is easy to see that no matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take, \\(p(X)\\) will have values between 0 and 1.\n\nA bit of rearrangement gives:\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X.\n\\]\nThis monotone transformation is called the log odds or logit transformation of \\(p(X)\\). (By log, we mean natural log: \\(\\ln\\).)"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 1: Express \\(1 - p(X)\\)\n\nSince \\(p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\), we can write:\n\\[\n1 - p(X) = 1 - \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\nSimplify:\n\\[\n1 - p(X) = \\frac{1 + e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-1",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 2: Compute the Odds\n\nThe odds are defined as:\n\\[\n\\frac{p(X)}{1 - p(X)}\n\\]\nSubstitute \\(p(X)\\) and \\(1 - p(X)\\):\n\\[\n\\frac{p(X)}{1 - p(X)} =\n\\frac{\\dfrac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}}\n{\\dfrac{1}{1 + e^{\\beta_0 + \\beta_1 X}}}\n\\]\nSimplify:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-2",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 3: Take the Log of the Odds\n\nTaking the natural logarithm:\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\log\\!\\Bigl(e^{\\beta_0 + \\beta_1 X}\\Bigr)\n\\]\nSimplify using the log property \\(\\log(e^x) = x\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-3",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-transformation-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nConclusion\n\nThe final transformation shows that the log-odds (logit) of \\(p(X)\\) is a linear function of \\(X\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-1",
    "href": "lecture_slides/03_classification/03_classification.html#linear-versus-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression",
    "text": "Linear versus Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression ensures that our estimate for \\(p(X)\\) lies between 0 and 1."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\nWe use maximum likelihood to estimate the parameters.\n\\[\n\\ell(\\beta_0, \\beta) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0} (1 - p(x_i)).\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a model by maximizing the likelihood function, which measures how likely the observed data is given the parameters.\n\nThe likelihood function is based on the probability distribution of the data. If you assume that the data points are independent, the likelihood function is the product of the probabilities of each observation.\n\nConsidering a data series of observed zeros and ones, and a model for the probabilities involving parameters (e.g., \\(\\beta_0\\) and \\(\\beta_1\\)), for any specific parameter values, we can compute the probability of observing the data.\nSince the observations are assumed to be independent, the joint probability of the observed sequence is the product of the probabilities for each observation. For each “1,” we use the model’s predicted probability, \\(p(x_i)\\), and for each “0,” we use \\(1 - p(x_i)\\).\nThe goal of MLE is to find the parameter values that maximize this joint probability, as they make the observed data most likely to have occurred."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSuppose you are flipping a coin, and you observe 5 heads out of 10 flips. The coin’s bias (the probability of heads) is \\(p\\), and you want to estimate \\(p\\).\nThe probability of observing a single outcome (heads or tails) follows the Bernoulli distribution:\n\\[\nP(\\text{Heads or Tails}) = p^x (1-p)^{1-x}, \\quad \\text{where } x = 1 \\text{ for heads, } x = 0 \\text{ for tails.}\n\\]\nFor 10 independent flips, the likelihood function is:\n\\[\nL(p) = P(\\text{data} \\mid p) = \\prod_{i=1}^{10} p^{x_i}(1-p)^{1-x_i}.\n\\]\nIf there are 5 heads (\\(x=1\\)) and 5 tails (\\(x=0\\)):\n\\[\nL(p) = p^5 (1-p)^5.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSimplify with the Log-Likelihood\nSince multiplying probabilities can result in very small numbers, we take the logarithm of the likelihood (log-likelihood). The logarithm simplifies the product into a sum:\n\\[\n\\ell(p) = \\log L(p) = \\log \\left(p^5 (1-p)^5\\right) = 5\\log(p) + 5\\log(1-p).\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nMaximize the Log-Likelihood\nTo find the value of \\(p\\) that maximizes \\(\\ell(p)\\), take the derivative of the log-likelihood with respect to \\(p\\) and set it to zero:\n\\[\n\\frac{\\partial\\ell(p)}{\\partial p} = \\frac{5}{p} - \\frac{5}{1-p} = 0.\n\\]\nSimplify:\n\\[\n\\frac{5}{p} = \\frac{5}{1-p}.\n\\]\nSolve for \\(p\\):\n\\[\n1 - p = p \\quad \\Rightarrow \\quad 1 = 2p \\quad \\Rightarrow \\quad p = 0.5.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nTo confirm that \\(p = 0.5\\) is the maximum, you can check the second derivative of the log-likelihood (concavity) or use numerical methods.\nIn our example, \\(p = 0.5\\) makes sense intuitively because the data (5 heads out of 10 flips) suggests the coin is unbiased.\nThe maximum likelihood estimate of \\(p\\) is \\(0.5\\). The MLE method finds the parameter values that make the observed data most likely, given the assumed probability model."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nAssumptions:\n\nData \\(x_1, x_2, \\dots, x_n\\) are drawn from a normal distribution with:\n\n\\[\n  f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\]\n\nAssume \\(\\sigma\\) is known (say, \\(\\sigma = 1\\)) and we want to estimate \\(\\mu\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\nThe likelihood for \\(n\\) independent observations is:\n\\[\nL(\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x_i - \\mu)^2}{2}}\n\\]\nTaking the natural log:\n\\[\n\\ell(\\mu) = \\log L(\\mu) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi) - \\frac{(x_i - \\mu)^2}{2} \\right]\n\\]\nSimplify (since \\(-\\frac{1}{2} \\log(2\\pi)\\) is constant):\n\\[\n\\ell(\\mu) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "href": "lecture_slides/03_classification/03_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nDifferentiate with respect to \\(\\mu\\):\n\\[\n\\frac{\\partial \\ell(\\mu)}{\\partial \\mu} = -\\sum_{i=1}^n (x_i - \\mu)\n\\] Set this to zero:\n\\[\n\\sum_{i=1}^n (x_i - \\mu) = 0\n\\]\nSolve for \\(\\mu\\):\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nThe MLE for the mean \\(\\mu\\) is simply the sample mean:\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#making-predictions-1",
    "href": "lecture_slides/03_classification/03_classification.html#making-predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Making Predictions",
    "text": "Making Predictions\nMost statistical packages can fit linear logistic regression models by maximum likelihood.\nLogistic Regression Coefficients\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.6513\n0.3612\n-29.5\n&lt; 0.0001\n\n\nbalance\n0.0055\n0.0002\n24.9\n&lt; 0.0001\n\n\n\n\nWhat is our estimated probability of default for someone with a credit card balance of $1000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}} = 0.006\n\\]\nWith a a credit card balance of $2000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 2000}}{1 + e^{-10.6513 + 0.0055 \\times 2000}} = 0.586\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-student-predictor",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-student-predictor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Student Predictor",
    "text": "Logistic Regression with Student Predictor\nLet’s do it again, using student as the predictor.\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-3.5041\n0.0707\n-49.55\n&lt; 0.0001\n\n\nstudent \\(Yes\\)\n0.4049\n0.1150\n3.52\n0.0004\n\n\n\n\nPredicted Probabilities\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{Yes}) = \\frac{e^{-3.5041 + 0.4049 \\times 1}}{1 + e^{-3.5041 + 0.4049 \\times 1}} = 0.0431,\n\\]\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{No}) = \\frac{e^{-3.5041 + 0.4049 \\times 0}}{1 + e^{-3.5041 + 0.4049 \\times 0}} = 0.0292.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-several-variables",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-with-several-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Several Variables",
    "text": "Logistic Regression with Several Variables\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\]\n\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.8690\n0.4923\n-22.08\n&lt; 0.0001\n\n\nbalance\n0.0057\n0.0002\n24.74\n&lt; 0.0001\n\n\nincome\n0.0030\n0.0082\n0.37\n0.7115\n\n\nstudent Yes\n-0.6468\n0.2362\n-2.74\n0.0062\n\n\n\nWhy is the coefficient for student negative, while it was positive before?"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confounding",
    "href": "lecture_slides/03_classification/03_classification.html#confounding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\nRelationship between Y and X controlled for W\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confounding-1",
    "href": "lecture_slides/03_classification/03_classification.html#confounding-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.\nBut for each level of balance, students default less than non-students.\nMultiple logistic regression can tease this out."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#multinomial-logistic-regression-1",
    "href": "lecture_slides/03_classification/03_classification.html#multinomial-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nLogistic regression is frequently used when the response is binary, or \\(K = 2\\) classes. We need a modification when there are \\(K &gt; 2\\) classes. E.g. stroke, drug overdose, and epileptic seizure for the emergency room example.\nThe simplest representation uses different linear functions for each class, combined with the softmax function to form probabilities:\n\\[\n\\Pr(Y = k | X = x) = \\text{Softmax}(z_k) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^{K} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.\n\\]\n\nWe really only need \\(K - 1\\) functions (see the book for details).\nWe fit by maximizing the multinomial log-likelihood (cross-entropy) — a generalization of the binomial.\nAn example will given later in the course, when we fit the 10-class model to the MNIST digit dataset."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#what-is-the-softmax-function",
    "href": "lecture_slides/03_classification/03_classification.html#what-is-the-softmax-function",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the Softmax Function?",
    "text": "What is the Softmax Function?\n\n\nThe softmax function is used in multinomial logistic regression to convert raw scores (logits) into probabilities for multiple classes.\n\n\nLogits are the raw, untransformed output of the linear component in logistic regression. For a given class \\(k\\), the logit is defined as:\n\\[\nz_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\n\\]\nWhere:\n\n\\(z_k\\): The logit for class \\(k\\).\n\\(\\beta_{k0}\\): Intercept term.\n\\(\\beta_{kj}\\): Coefficients for predictor \\(x_j\\).\n\n\n\nSoftmax Definition:\nFor \\(K\\) classes and input \\(x\\), the softmax function is defined as:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{l=1}^K e^{z_l}}\n\\]\nWhere:\n\n\\(z_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\\): The linear score (logit) for class \\(k\\).\n\\(\\beta_{k0}, \\beta_{k1}, \\dots, \\beta_{kp}\\): Coefficients for class \\(k\\).\n\\(e^{z_k}\\): Exponentiated score for class \\(k\\), ensuring all values are positive.\n\n\n\n\nKey Features of the Softmax Function\n\nProbability Distribution: Outputs probabilities that sum to 1 across all \\(K\\) classes. \\(\\text{Pr}(Y = k \\mid X = x) = \\text{Softmax}(z_k)\\).\nNormalization: Normalizes logits by dividing each exponentiated logit by the sum of all exponentiated logits.\nHandles Multiclass Classification: Extends binary logistic regression to \\(K &gt; 2\\) classes."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-of-softmax-in-action",
    "href": "lecture_slides/03_classification/03_classification.html#example-of-softmax-in-action",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example of Softmax in Action",
    "text": "Example of Softmax in Action\n\n\n\n\nImagine classifying three emergency room conditions: Stroke, Drug Overdose, and Epileptic Seizure.\nSuppose the logits are: \\(z_{\\text{stroke}} = 2.5, \\quad z_{\\text{drug overdose}} = 1.0, \\quad z_{\\text{epileptic seizure}} = 0.5\\)\nThe probabilities are:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{e^{2.5} + e^{1.0} + e^{0.5}}\n\\]\n\nStep 1: Exponentiate the Logits\n\\(e^{z_{\\text{stroke}}} = e^{2.5} \\approx 12.182\\)\n\\(e^{z_{\\text{drug overdose}}} = e^{1.0} \\approx 2.718\\)\n\\(e^{z_{\\text{epileptic seizure}}} = e^{0.5} \\approx 1.649\\)\n\n\nStep 2: Compute the Denominator\n\\(\\sum_{l=1}^K e^{z_l} = e^{2.5} + e^{1.0} + e^{0.5}\\)\n\\(\\sum_{l=1}^K e^{z_l} \\approx 12.182 + 2.718 + 1.649 = 16.549\\)\n\n\n\nStep 3: Calculate the Probabilities\n\\(\\text{Pr}(\\text{stroke}) = \\frac{e^{z_{\\text{stroke}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{12.182}{16.549} \\approx 0.7366\\)\n\\(\\text{Pr}(\\text{drug overdose}) = \\frac{e^{z_{\\text{drug overdose}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{2.718}{16.549} \\approx 0.1642\\)\n\\(\\text{Pr}(\\text{epileptic seizure}) = \\frac{e^{z_{\\text{epileptic seizure}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{1.649}{16.549} \\approx 0.0996\\)\nThe output probabilities represent the likelihood of each condition, ensuring:\n\\[\n\\sum_{k=1}^3 \\text{Pr}(Y = k) = 1\n\\]\nWe have:\n\\[\n   0.7366 + 0.1642 + 0.0996 \\approx 1.000\n\\]\n\n\nConclusion\n\nThe softmax function translates raw scores into probabilities, making it essential for multiclass classification.\nIt ensures a probabilistic interpretation while maintaining normalization across all classes."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#discriminant-analysis-1",
    "href": "lecture_slides/03_classification/03_classification.html#discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of \\(X\\) in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(\\Pr(Y \\mid X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions as input for \\(f_k(x)\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#bayes-theorem-for-classification",
    "href": "lecture_slides/03_classification/03_classification.html#bayes-theorem-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes Theorem for Classification",
    "text": "Bayes Theorem for Classification\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\Pr(X = x \\mid Y = k) \\cdot \\Pr(Y = k)}{\\Pr(X = x)}\n\\]\nOne writes this slightly differently for discriminant analysis:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{\\ell=1}^K \\pi_\\ell f_\\ell(x)}, \\quad \\text{where}\n\\]\n\n\\(f_k(x) = \\Pr(X = x \\mid Y = k)\\) is the density for \\(X\\) in class \\(k\\). Here we will use normal densities for these, separately in each class.\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#bayes-theorem-explanation",
    "href": "lecture_slides/03_classification/03_classification.html#bayes-theorem-explanation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes’ Theorem: Explanation",
    "text": "Bayes’ Theorem: Explanation\nIt describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A|B)\\): Posterior probability - Probability of event \\(A\\) occurring given that \\(B\\) is true — updated probability after the evidence is considered.\n\\(P(A)\\): Prior probability - Initial probability of event \\(A\\) — the probability before the evidence is considered.\n\\(P(B|A)\\): Likelihood - Probability of observing event \\(B\\) given that \\(A\\) is true.\n\\(P(B)\\): Marginal probability - Total probability of the evidence, event \\(B\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#understanding-conditional-probability",
    "href": "lecture_slides/03_classification/03_classification.html#understanding-conditional-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Understanding Conditional Probability",
    "text": "Understanding Conditional Probability\nConditional probability is the probability of an event occurring given that another event has already occurred.\nDefinition:\n\\[\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nis the probability of event \\(A\\) occurring given that \\(B\\) is true.\n\nInterpretation: How likely is \\(A\\) if we know that \\(B\\) happens?"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#what-is-joint-probability",
    "href": "lecture_slides/03_classification/03_classification.html#what-is-joint-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Joint Probability?",
    "text": "What is Joint Probability?\nJoint probability refers to the probability of two events occurring together.\nDefinition: \\(P(A \\cap B)\\) is the probability that both \\(A\\) and \\(B\\) occur.\n\nConnection to Conditional Probability:\n\\[\n  P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n  P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\nThis formula is crucial for understanding Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#symmetry-in-joint-events",
    "href": "lecture_slides/03_classification/03_classification.html#symmetry-in-joint-events",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Symmetry in Joint Events",
    "text": "Symmetry in Joint Events\nJoint probability is symmetric, meaning:\n\\[\nP(A \\cap B) = P(B \\cap A)\n\\]\nThus, we can also express it as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\nThis symmetry is the key to deriving Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#deriving-bayes-theorem",
    "href": "lecture_slides/03_classification/03_classification.html#deriving-bayes-theorem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deriving Bayes’ Theorem",
    "text": "Deriving Bayes’ Theorem\n\n\nGiven that the definition of Conditional Probability is:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n\n\nUsing the Definition of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n   P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\n\n\n\nSymmetry of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(B \\cap A)\n\\]\n\n\nThus, we can express the joint probability as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\n\nThe Bayes’ Theorem!\n\nSubstitute this back into the conditional probability definition:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#why-bayes-theorem-matters",
    "href": "lecture_slides/03_classification/03_classification.html#why-bayes-theorem-matters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Bayes’ Theorem Matters?",
    "text": "Why Bayes’ Theorem Matters?\n\n\nBayes’ Theorem is a foundational principle in probability theory and statistics, enabling:\n\nIncorporation of Prior Knowledge:\nIt allows for the integration of prior knowledge or beliefs when making statistical inferences.\nBeliefs Update:\nIt provides a systematic way to update the probability estimates as new evidence or data becomes available.\nProbabilistic Thinking:\nEncourages a probabilistic approach to decision-making, quantifying uncertainty, and reasoning under uncertainty.\nVersatility in Applications:\nFrom medical diagnosis to spam filtering, Bayes’ Theorem is pivotal in areas requiring probabilistic assessment.\n\nBayes’ Theorem is a paradigm that shapes the way we interpret and interact with data, offering a powerful tool for learning from information and making decisions in an uncertain world."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#classify-to-the-highest-density",
    "href": "lecture_slides/03_classification/03_classification.html#classify-to-the-highest-density",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classify to the Highest Density",
    "text": "Classify to the Highest Density\n\n\nLeft-hand plot: single variable X and \\(\\pi_k f_k(x)\\) in the vertical axis for both classes \\(k\\) equals 1 and \\(k\\) equals 2. In this case the the pies are the same for both, so anything to the left of zero we classify as as green and anything to the right we classify as as purple.\nRight-hand plot: here we have different priors. The probability of \\(k = 2\\) is 0.7 and and of of \\(k= 1\\) is 0.3. The decision boundary moved slightly to the left. On the right, we favor the pink class."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#why-discriminant-analysis",
    "href": "lecture_slides/03_classification/03_classification.html#why-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Discriminant Analysis?",
    "text": "Why Discriminant Analysis?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p = 1\\)",
    "text": "Linear Discriminant Analysis when \\(p = 1\\)\nThe Gaussian density has the form:\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2}\n\\]\nHere \\(\\mu_k\\) is the mean, and \\(\\sigma_k^2\\) the variance (in class \\(k\\)). We will assume that all the \\(\\sigma_k = \\sigma\\) are the same.\n\nPlugging this into Bayes formula, we get a rather complex expression for \\(p_k(x) = \\Pr(Y = k \\mid X = x)\\):\n\\[\np_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2}}{\\sum_{\\ell=1}^K \\pi_\\ell \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2}}\n\\]\nHappily, there are simplifications and cancellations."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#discriminant-functions",
    "href": "lecture_slides/03_classification/03_classification.html#discriminant-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Functions",
    "text": "Discriminant Functions\nTo classify one observation at the value \\(X = x\\) into a class, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\n\nIf there are \\(K = 2\\) classes and \\(\\pi_1 = \\pi_2 = 0.5\\), then one can see that the decision boundary is at:\n\\[\nx = \\frac{\\mu_1 + \\mu_2}{2}.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "href": "lecture_slides/03_classification/03_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Estimating Parameters for Discriminant Analysis",
    "text": "Example: Estimating Parameters for Discriminant Analysis\n\n\nLeft-Panel: Synthetic population data with \\(\\mu_1 = -1.5\\), \\(\\mu_2 = 1.5\\), \\(\\pi_1 = \\pi_2 = 0.5\\), and \\(\\sigma^2 = 1\\).\nTypically, we don’t know these parameters; we just have the training data. In that case, we simply estimate the parameters and plug them into the rule.\nRight-Panel: histograms of the sample. We see that the estimation provided a decision boundary (black solid line) pretty close to the correct one, the one of the population."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#estimating-the-parameters",
    "href": "lecture_slides/03_classification/03_classification.html#estimating-the-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\n\n\nThe prior is the number in each class divided by the total number:\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nThe means in each class is the sample mean:\n\\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n\\]\nWe assume that the variance is the same in each of the classes and so we assume a pooled variance estimate:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n\\]\n\\[\n= \\sum_{k=1}^K \\frac{n_k - 1}{n - K} \\cdot \\hat{\\sigma}_k^2\n\\]\nwhere \\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\\) is the usual formula for the estimated variance in the \\(k\\)-th class."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-2",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nGaussian density in two Dimensions, two variables \\(x_1\\) and \\(x_2\\). On the Left-panel, we have a bell function and this is the case when the two variables are uncorrelated. On the Right-panel, there is correlation between the two predictors and it is like a stretched bell.\nDensity:\n\\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\\] where \\(\\Sigma\\) is the covariance matrix."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#covariance-matrix",
    "href": "lecture_slides/03_classification/03_classification.html#covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\n\n\nThe covariance matrix is a square matrix that summarizes the covariance (a measure of how much two random variables vary together) between multiple variables in a dataset.\nDefinition:\nFor a random vector \\(X = [X_1, X_2, \\dots, X_p]^\\top\\) with \\(p\\) variables, the covariance matrix \\(\\Sigma\\) is defined as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_p) \\\\\n\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_p) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(X_p, X_1) & \\text{Cov}(X_p, X_2) & \\cdots & \\text{Var}(X_p)\n\\end{bmatrix}\n\\]\n\nKey Properties:\n\n\\(\\text{Var}(X_i)\\): Variance of variable \\(X_i\\).\n\\(\\text{Cov}(X_i, X_j)\\): Covariance between variables \\(X_i\\) and \\(X_j\\).\n\\(\\Sigma\\) is symmetric: \\(\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)\\).\nDiagonal elements represent variances, and off-diagonal elements represent covariances."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-3",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nDiscriminant function: after simplifying the density function we can find\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\]\nNote that it is a linear function where the first component, \\(x^T \\Sigma^{-1} \\mu_k\\), has the \\(x\\) variable multiplied by a coefficient vector and, the second component, \\(\\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\), is a constant."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-4",
    "href": "lecture_slides/03_classification/03_classification.html#linear-discriminant-analysis-when-p-1-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nThe Discriminant function can be written as\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\]\na linear function. That is a function for class \\(k\\), where \\(c_{k0}\\) represents the constant we find in the second component of the Discriminant function and \\(c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\) come from the first component of the Discriminant function. We compute \\(\\delta_k(x)\\) for each of the classes and then you classify to the class for which it is largest."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#illustration-p-2-and-k-3-classes",
    "href": "lecture_slides/03_classification/03_classification.html#illustration-p-2-and-k-3-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes",
    "text": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes\n\n\nLeft-panel: The circle presents the countor of the density of a particular level of probability for the blue, green, and the orange class. Here \\(\\pi_1 = \\pi_2 = \\pi_3 = \\frac{1}{3}\\). The dashed lines are known as the Bayes decision boundaries. They are the “True” decision boundaries, were they known, they would yield the fewest misclassification errors, among all possible classifiers.\nRight-panel: We compute the mean for \\(x_1\\) and \\(x_2\\) for the each blue, green, and orange class. After plugging them into the formula, instead of getting the the dotted lines we get the solid black lines."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-fishers-iris-data-1",
    "href": "lecture_slides/03_classification/03_classification.html#example-fishers-iris-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Iris Data",
    "text": "Example: Fisher’s Iris Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 variables\n3 species\n50 samples/class\n\n🟦 Setosa\n🟧 Versicolor\n🟩 Virginica\n\nLDA classifies all but 3 of the 150 training samples correctly."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-fishers-discriminant-plot",
    "href": "lecture_slides/03_classification/03_classification.html#example-fishers-discriminant-plot",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Discriminant Plot",
    "text": "Example: Fisher’s Discriminant Plot\n\n\nDiscriminant variables 1 and 2 are linear combinations of the original variables.\nLDA classifies points based on their proximity to centroids in discriminant space.\nThe centroids lie in a subspace of the multi-dimensional space (e.g., a plane within 4D space).\nFor \\(K\\) classes:\n\nLDA can be visualized in \\(K - 1\\)-dimensional space.\nFor \\(K &gt; 3\\), the “best” 2D plane can be chosen for visualization."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#from-delta_kx-to-probabilities",
    "href": "lecture_slides/03_classification/03_classification.html#from-delta_kx-to-probabilities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "From \\(\\delta_k(x)\\) to Probabilities",
    "text": "From \\(\\delta_k(x)\\) to Probabilities\n\n\n\nOnce we have estimates of the Discriminat Functions, \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\n\\[\n\\hat{\\Pr}(Y = k | X = x) = \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^K e^{\\hat{\\delta}_l(x)}}.\n\\]\n\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{\\Pr}(Y = k | X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{\\Pr}(Y = 2 | X = x) \\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#lda-on-credit-data",
    "href": "lecture_slides/03_classification/03_classification.html#lda-on-credit-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "LDA on Credit Data",
    "text": "LDA on Credit Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Default Status\n\n\n\nPredicted Default Status\nNo\nYes\nTotal\n\n\n\n\nNo\n9644\n252\n9896\n\n\nYes\n23\n81\n104\n\n\nTotal\n9667\n333\n10000\n\n\n\n\n\n\n\n\n\\(\\frac{23 + 252}{10000}\\) errors — a 2.75% misclassification rate!\n\n\nSome caveats:\n\nThis is training error, and we may be overfitting.\nIf we classified to the prior, the proportion of cases in the classes (e.g. always assuming the class No default). We would make \\(\\frac{333}{10000}\\) errors, or only 3.33%. This is what we call the null rate.\nWe can break the errors into different kinds: of the true No’s, we make \\(\\frac{23}{9667} = 0.2\\%\\) errors; of the true Yes’s, we make \\(\\frac{252}{333} = 75.7\\%\\) errors!"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#types-of-errors-1",
    "href": "lecture_slides/03_classification/03_classification.html#types-of-errors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of errors",
    "text": "Types of errors\n\n\nFalse positive rate: The fraction of negative examples that are classified as positive — 0.2% in example.\nFalse negative rate: The fraction of positive examples that are classified as negative — 75.7% in example.\nWe produced this table by classifying to class Yes if:\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq 0.5\n\\]\nWe can change the two error rates by changing the threshold from \\(0.5\\) to some other value in \\([0, 1]\\):\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq \\text{threshold},\n\\]\nand vary \\(\\text{threshold}\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#varying-the-threshold",
    "href": "lecture_slides/03_classification/03_classification.html#varying-the-threshold",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to reduce the false negative rate, we may want to reduce the threshold to 0.1 or less."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#roc-curve",
    "href": "lecture_slides/03_classification/03_classification.html#roc-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve",
    "text": "ROC Curve\nLDA on Credit Data Example:\n\nThe ROC plot displays both, True Positive rate and False Positive rate, simultaneously.\nSometimes we use the AUC or area under the curve to summarize the overall performance. Higher AUC is good."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#other-forms-of-discriminant-analysis-1",
    "href": "lecture_slides/03_classification/03_classification.html#other-forms-of-discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Forms of Discriminant Analysis",
    "text": "Other Forms of Discriminant Analysis\n\nWhen \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\Sigma\\) in each class, this leads to linear discriminant analysis.\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n\\]\nBy altering the forms for \\(f_k(x)\\), we get different classifiers:\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get quadratic discriminant analysis.\nWith \\(f_k(x) = \\prod_{j=1}^{p} f_{jk}(x_j)\\) (conditional independence model) in each class, we get naive Bayes. For Gaussians, this means \\(\\Sigma_k\\) are diagonal.\nMany other forms, by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#quadratic-discriminant-analysis",
    "href": "lecture_slides/03_classification/03_classification.html#quadratic-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Quadratic Discriminant Analysis",
    "text": "Quadratic Discriminant Analysis\n\n\n\n\n\n\n\n\n\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1}(x - \\mu_k) + \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k|\n\\]\nIn the Left-plot we see a case when the true boundary should be linear. In the Right-plot, covariances were different in the true data. It is possible to see that the bayes decision boundary is curved and the quadratic discriminant analysis is also curved whereas the linear discriminant analysis gives a different boundary.\nWhether each class has the same or different covariance matrices significantly impacts how boundaries between the classes are defined. The covariance matrix describes the spread or variability of data points within each class and how the features in that class relate to each other.\n\nKey Insight: If \\(\\Sigma_k\\) are different for each class, the quadratic terms matter significantly.\nQDA allows for non-linear decision boundaries due to unique covariance matrices for each class.\nExample: Suppose we are classifying plants based on two features (e.g., height and leaf width). If one type of plant has a tall and narrow spread of data, while another type has a short and wide spread, QDA can handle these differences and draw curved boundaries to separate the groups."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#assess-the-covariance-matrices",
    "href": "lecture_slides/03_classification/03_classification.html#assess-the-covariance-matrices",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assess the Covariance Matrices",
    "text": "Assess the Covariance Matrices\n\nLDA assumes the covariance matrices of all classes are the same, while QDA allows each class to have its own. To determine which assumption is better:\n\nHypothesys test: we can perform a Test for Equality of Covariance Matrices (e.g. Box’s M Test). If the covariance matrices are similar (test is not significant): LDA is appropriate. If the covariance matrices differ (test is significant): QDA may be better.\nVisual Inspection: Plot the data in two dimensions (e.g., using scatterplots). Check if the spread, shape, or orientation of data points differs significantly between classes. If they are similar, LDA might work well. If they are visibly different, QDA is likely better.\nCompare Model Performance: run both models and choose the model that performs better on unseen data (test set).\nConsider the Number of Features and Data Size: LDA performs well with smaller datasets because it estimates a single covariance matrix across all classes (fewer parameters). QDA requires a larger dataset because it estimates a separate covariance matrix for each class (more parameters).\nDomain Knowledge: Use your understanding of the data to decide."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#logistic-regression-versus-lda",
    "href": "lecture_slides/03_classification/03_classification.html#logistic-regression-versus-lda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression versus LDA",
    "text": "Logistic Regression versus LDA\n\nFor a two-class problem, one can show that for LDA:\n\\[\n\\log \\left( \\frac{p_1(x)}{1 - p_1(x)} \\right) = \\log \\left( \\frac{p_1(x)}{p_2(x)} \\right) = c_0 + c_1 x_1 + \\dots + c_p x_p\n\\]\nif we take the log odds, \\(\\log \\left( \\frac{p_1(x)}{1 - p_1(x)}\\right)\\), which is the log of the probability for class 1 versus the probability for class two, we endup with a linear function of \\(x\\), \\(c_0 + c_1 x_1 + \\dots + c_p x_p\\). So it has the same form as logistic regression.\nThe difference lies in how the parameters are estimated.\n\nLogistic regression uses the conditional likelihood based on \\(\\text{Pr}(Y|X)\\). In Machine Learning, it is known as discriminative learning.\nLDA uses the full likelihood based on the joint distributions of \\(x's\\) and \\(y's\\), \\(\\text{Pr}(X, Y)\\), whereas logistic regression was only using the distribution of \\(y's\\). It is known as generative learning.\nDespite these differences, in practice, the results are often very similar.\n\nLogistic regression can also fit quadratic boundaries like QDA by explicitly including quadratic terms in the model."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naive-bayes-1",
    "href": "lecture_slides/03_classification/03_classification.html#naive-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\n\nAssumes features are independent in each class.\nUseful when \\(p\\) is large, and so multivariate methods like QDA and even LDA break down.\n\nGaussian Naive Bayes assumes each \\(\\Sigma_k\\) is diagonal:\n\\[\n\\begin{aligned}\n\\delta_k(x) &\\propto \\log \\left[ \\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\right] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\left[ \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\sigma_{kj}^2 \\right] + \\log \\pi_k\n\\end{aligned}\n\\]\n\nCan be used for mixed feature vectors (qualitative and quantitative). If \\(X_j\\) is qualitative, replace \\(f_{kj}(x_j)\\) with the probability mass function (histogram) over discrete categories.\nKey Point: Despite strong assumptions, naive Bayes often produces good classification results.\n\nExplanation:\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_{kj}(x_j)\\): Density function for feature \\(j\\) in class \\(k\\).\n\\(\\mu_{kj}\\): Mean of feature \\(j\\) in class \\(k\\).\n\\(\\sigma_{kj}^2\\): Variance of feature \\(j\\) in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#diagonal-covariance-matrix",
    "href": "lecture_slides/03_classification/03_classification.html#diagonal-covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Diagonal Covariance Matrix",
    "text": "Diagonal Covariance Matrix\n\n\nA diagonal covariance matrix is a special case of the covariance matrix where all off-diagonal elements are zero. This implies that the variables are uncorrelated.\nGeneral Form:\nFor \\(p\\) variables, a diagonal covariance matrix \\(\\Sigma\\) is represented as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_p^2\n\\end{bmatrix}\n\\]\nProperties:\n\nDiagonal Elements (\\(\\sigma_i^2\\)): Represent the variance of each variable \\(X_i\\).\nOff-Diagonal Elements: All equal to zero (\\(\\text{Cov}(X_i, X_j) = 0\\) for \\(i \\neq j\\)), indicating no linear relationship between variables.\nA diagonal covariance matrix assumes independence between variables. Each variable varies independently without influencing the others.\nCommonly used in simpler models, such as Naive Bayes, where independence is assumed."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes",
    "href": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLogistic regression models \\(\\Pr(Y = k | X = x)\\) directly, via the logistic function. Similarly, the multinomial logistic regression uses the softmax function. These all model the conditional distribution of \\(Y\\) given \\(X\\).\nBy contrast, generative models start with the conditional distribution of \\(X\\) given \\(Y\\), and then use Bayes formula to turn things around:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}.\n\\]\n\n\\(f_k(x)\\) is the density of \\(X\\) given \\(Y = k\\);\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal probability that \\(Y\\) is in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes-1",
    "href": "lecture_slides/03_classification/03_classification.html#generative-models-and-naïve-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLinear and quadratic discriminant analysis derive from generative models, where \\(f_k(x)\\) are Gaussian.\nUseful if some classes are well separated. A situation where logistic regression is unstable.\nNaïve Bayes assumes that the densities \\(f_k(x)\\) in each class factor:\n\n\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\n\\]\n\nEquivalently, this assumes that the features are independent within each class.\nThen using Bayes formula:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)}{\\sum_{l=1}^{K} \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\cdots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-details",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Details",
    "text": "Naïve Bayes — Details\nWhy the independence assumption?\n\nDifficult to specify and model high-dimensional densities.\nMuch easier to specify one-dimensional densities.\nCan handle mixed features:\n\nIf feature \\(j\\) is quantitative, can model as univariate Gaussian, for example: \\(X_j \\mid Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2).\\) We estimate \\(\\mu_{jk}\\) and \\(\\sigma_{jk}^2\\) from the data, and then plug into Gaussian density formula for \\(f_{jk}(x_j)\\).\nAlternatively, can use a histogram estimate of the density, and directly estimate \\(f_{jk}(x_j)\\) by the proportion of observations in the bin into which \\(x_j\\) falls.\nIf feature \\(j\\) is qualitative, can simply model the proportion in each category.\n\nSomewhat unrealistic but extremely useful in many cases.\nDespite its simplicity, often shows good classification performance due to reduced variance."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis toy example demonstrates the working of the Naïve Bayes classifier for two classes (\\(k = 1\\) and \\(k = 2\\)) and three features (\\(X_1, X_2, X_3\\)). The goal is to compute the posterior probabilities \\(\\Pr(Y = 1 \\mid X = x^*)\\) and \\(\\Pr(Y = 2 \\mid X = x^*)\\) for a given observation \\(x^* = (0.4, 1.5, 1)\\).\n\nThe prior probabilities for each class are:\n\\(\\hat{\\pi}_1 = \\hat{\\pi}_2 = 0.5\\)\n\n\nFor each feature (\\(X_1, X_2, X_3\\)), we estimate the class-conditional density functions:\n\n\\(\\hat{f}_{11}, \\hat{f}_{12}, \\hat{f}_{13}\\): Densities for \\(k = 1\\) (class 1).\n\n\\(\\hat{f}_{11}(0.4) = 0.368 \\\\\\)\n\\(\\hat{f}_{12}(1.5) = 0.484 \\\\\\)\n\\(\\hat{f}_{13}(1) = 0.226 \\\\\\)\n\n\\(\\hat{f}_{21}, \\hat{f}_{22}, \\hat{f}_{23}\\): Densities for \\(k = 2\\) (class 2).\n\n\\(\\hat{f}_{21}(0.4) = 0.030 \\\\\\)\n\\(\\hat{f}_{22}(1.5) = 0.130 \\\\\\)\n\\(\\hat{f}_{23}(1) = 0.616 \\\\\\)"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example-1",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-toy-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\nCompute Class-Conditional Likelihoods for each class \\(k\\), the likelihood is computed as the product of the conditional densities for each feature:\n\n\\[\n   \\hat{f}_k(x^*) = \\prod_{j=1}^3 \\hat{f}_{kj}(x_j^*)\n\\]\n\nFor \\(k = 1\\):\n\n\\[\n     \\hat{f}_{11}(0.4) = 0.368, \\quad \\hat{f}_{12}(1.5) = 0.484, \\quad \\hat{f}_{13}(1) = 0.226\n\\]\n\\[\n     \\hat{f}_1(x^*) = 0.368 \\times 0.484 \\times 0.226 \\approx 0.0402\n\\]\n\nFor \\(k = 2\\):\n\n\\[\n     \\hat{f}_{21}(0.4) = 0.030, \\quad \\hat{f}_{22}(1.5) = 0.130, \\quad \\hat{f}_{23}(1) = 0.616\n\\]\n\\[\n     \\hat{f}_2(x^*) = 0.030 \\times 0.130 \\times 0.616 \\approx 0.0024\n\\]\n\n\n\nCompute Posterior Probabilities using Bayes’ theorem:\n\n\\[\n   \\Pr(Y = k \\mid X = x^*) = \\frac{\\hat{\\pi}_k \\hat{f}_k(x^*)}{\\sum_{k=1}^2 \\hat{\\pi}_k \\hat{f}_k(x^*)}\n\\]\n\nFor \\(k = 1\\): \\[\n\\Pr(Y = 1 \\mid X = x^*) = \\frac{0.5 \\times 0.0402}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.944\n\\]\nFor \\(k = 2\\): \\[\n\\Pr(Y = 2 \\mid X = x^*) = \\frac{0.5 \\times 0.0024}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.056\n\\]\n\n\n\nKey Takeaways:\n\nNaïve Bayes Assumption: The assumption of feature independence simplifies computation by allowing the class-conditional densities to be computed separately for each feature.\nPosterior Probabilities: The posterior probability combines the prior (\\(\\pi_k\\)) and the likelihood (\\(\\hat{f}_k(x^*)\\)).\nClassification: The observation \\(x^*\\) is classified as the class with the highest posterior probability (\\(Y = 1\\))."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs",
    "text": "Naïve Bayes and GAMs\n\n\nNaïve Bayes classifier can be understood as a special case of a GAM.\n\\[\n\\begin{aligned}\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n&= \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right) \\\\\n&= a_k + \\sum_{j=1}^p g_{kj}(x_j),\n\\end{aligned}\n\\]\nwhere \\(a_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\) and \\(g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\).\nHence, the Naïve Bayes model is a Generalized Additive Model (GAM):\n\nThe log-odds are expressed as a sum of additive terms.\n\\(a_k\\): Represents prior influence.\n\\(g_{kj}(x_j)\\): Represents feature contributions."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nLog-Odds of Posterior Probabilities\nThe Naïve Bayes classifier starts with the log-odds of the posterior probabilities:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n\\]\nThis is the log of the ratio of the probabilities of class \\(k\\) and a reference class \\(K\\), given the feature vector \\(X = x\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-1",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nBayes’ Theorem\nUsing Bayes’ theorem, the posterior probabilities can be expressed as:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right)\n\\]\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_k(x)\\): Class-conditional density for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-2",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nNaïve Bayes Assumption\nThe Naïve Bayes assumption states that features are conditionally independent given the class:\n\\[\nf_k(x) = \\prod_{j=1}^p f_{kj}(x_j)\n\\]\nSubstituting this into the equation:\n\\[\n\\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) = \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-3",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nSeparate the Terms\nThe terms can now be separated:\n\\[\n\\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\n\n\\(\\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\): Influence of prior probabilities.\n\\(\\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\): Contribution from each feature."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-4",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes-and-gams-details-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nAdditive Form\nDefine:\n\\[\na_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right), \\quad g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\nThe equation becomes:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = a_k + \\sum_{j=1}^p g_{kj}(x_j)\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-1",
    "href": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLinear regression is used for quantitative responses.\nLinear logistic regression is the counterpart for a binary response and models the logit of the probability as a linear model.\nOther response types exist, such as non-negative responses, skewed distributions, and more.\nGeneralized linear models provide a unified framework for dealing with many different response types."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-bikeshare-data",
    "href": "lecture_slides/03_classification/03_classification.html#example-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Bikeshare Data",
    "text": "Example: Bikeshare Data\n\n\nLinear regression with response bikers: number of hourly users in the bikeshare program in Washington, DC.\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n73.60\n5.13\n14.34\n0.00\n\n\nworkingday\n1.27\n1.78\n0.71\n0.48\n\n\ntemp\n157.21\n10.26\n15.32\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-12.89\n1.96\n-6.56\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-66.49\n2.97\n-22.43\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-109.75\n76.67\n-1.43\n0.15"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-meanvariance-relationship",
    "href": "lecture_slides/03_classification/03_classification.html#example-meanvariance-relationship",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Mean/Variance Relationship",
    "text": "Example: Mean/Variance Relationship\n\n\nLeft plot: we see that the variance mostly increases with the mean.\n10% of a linear model predictions are negative! (not shown here.). However, we know that the response variable, bikers, is always positive.\nTaking log(bikers) alleviates this, but is not a good solution. It has its own problems: e.g. predictions are on the wrong scale, and some counts are zero!"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#poisson-regression-model",
    "href": "lecture_slides/03_classification/03_classification.html#poisson-regression-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nPoisson distribution is useful for modeling counts:\n\\[\n  Pr(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\, \\text{for } k = 0, 1, 2, \\ldots\n\\]\nMean/variance relationship: \\(\\lambda = \\mathbb{E}(Y) = \\text{Var}(Y)\\) i.e., there is a mean/variance dependence. When the mean is higher, the variance is higher.\nModel with Covariates:\n\\[\n  \\log(\\lambda(X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\nOr equivalently:\n\\[\n  \\lambda(X_1, \\ldots, X_p) = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\]\nAutomatic positivity: The model ensures that predictions are non-negative by construction."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#example-poisson-regression-on-bikeshare-data",
    "href": "lecture_slides/03_classification/03_classification.html#example-poisson-regression-on-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Poisson Regression on Bikeshare Data",
    "text": "Example: Poisson Regression on Bikeshare Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n4.12\n0.01\n683.96\n0.00\n\n\nworkingday\n0.01\n0.00\n7.50\n0.00\n\n\ntemp\n0.79\n0.01\n68.43\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-0.08\n0.00\n-34.53\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-0.58\n0.00\n-141.91\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-0.93\n0.17\n-5.55\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\nNote: in this case, the variance is somewhat larger than the mean — a situation known as overdispersion. As a result, the p-values may be misleadingly small."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-2",
    "href": "lecture_slides/03_classification/03_classification.html#generalized-linear-models-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\n\nWe have covered three GLMs: Gaussian, binomial, and Poisson.\nThey each have a characteristic link function. This is the transformation of the mean represented by a linear model:\n\n\\[\n\\eta(\\mathbb{E}(Y|X_1, X_2, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\n\nThe link functions for linear, logistic, and Poisson regression are \\(\\eta(\\mu) = \\mu\\), \\(\\eta(\\mu) = \\log(\\mu / (1 - \\mu))\\), \\(\\eta(\\mu) = \\log(\\mu)\\), respectively.\nEach GLM has a characteristic variance function.\nThe models are fit by maximum likelihood, and model summaries are produced using glm() in R.\nOther GLMs include Gamma, Negative-binomial, Inverse Gaussian, and more."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confusion-matrix",
    "href": "lecture_slides/03_classification/03_classification.html#confusion-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nThe confusion matrix provides a summary of prediction results. It compares the predicted and actual classes, offering insights into the model’s classification performance.\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#confusion-matrix-1",
    "href": "lecture_slides/03_classification/03_classification.html#confusion-matrix-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nKey metrics derived from the confusion matrix include:\n\n\n\n\n\n\n\nMetric\nDefinition\n\n\n\n\nAccuracy\nThe proportion of correct predictions.\n\n\nSensitivity (Recall)\nThe model’s ability to identify positive cases.\n\n\nSpecificity\nThe model’s ability to identify negative cases.\n\n\nPrecision\nAmong predicted positive cases, the proportion that are truly positive.\n\n\nF1 Score\nThe harmonic mean of Precision and Sensitivity, balancing false positives and false negatives."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#accuracy",
    "href": "lecture_slides/03_classification/03_classification.html#accuracy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Accuracy",
    "text": "Accuracy\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\\]\n\n\n\n\nOverall effectiveness of the model.\nIn the context of weather forecasting, for example, accuracy reflects how well a model predicts weather events correctly (e.g., rainy or sunny days).\nHigh accuracy: lots of correct predictions!"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#recall-sensitivity",
    "href": "lecture_slides/03_classification/03_classification.html#recall-sensitivity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recall (Sensitivity)",
    "text": "Recall (Sensitivity)\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\n\n\n\n\nIs the fraction of positives correctly identified.\nIn criminal justice, it would assess how well a predictive policing model identifies all potential criminal activities (True Positives) without missing any (thus minimizing False Negatives).\nHigh recall: low false-negative rates."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#specificity",
    "href": "lecture_slides/03_classification/03_classification.html#specificity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Specificity",
    "text": "Specificity\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n\\]\n\n\n\n\nIt is the true negative rate, measuring a model’s ability to correctly identify actual negatives.\nCrucial in fields where incorrectly identifying a negative case as positive could have serious implications (e.g., criminal justice).\nHigh specificity: the model is very effective at identifying true negatives."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#precision",
    "href": "lecture_slides/03_classification/03_classification.html#precision",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Precision",
    "text": "Precision\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\n\n\n\n\nAccuracy of positive predictions.\nIn email spam detection, it would indicate the percentage of emails correctly identified as spam (True Positives) out of all emails flagged as spam, aiming to reduce the number of legitimate emails incorrectly marked as spam (False Positives).\nHigh precision: low false-positive rates."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#f1-score",
    "href": "lecture_slides/03_classification/03_classification.html#f1-score",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "F1-Score",
    "text": "F1-Score\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\n\nPredicted: 0\nPredicted: 1\n\n\n\n\nActual: 0\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual: 1\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\n\n\nFormula\n\n\n\\[\n\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\n\nHarmonic mean of Precision and Recall.\nIn a medical diagnosis scenario, it would help in evaluating a test’s effectiveness in correctly identifying patients with a disease (True Positives) while minimizing the misclassification of healthy individuals as diseased (False Positives and False Negatives).\nHigh F1 score: a better balance between precision and recall."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#roc-curve-auc-1",
    "href": "lecture_slides/03_classification/03_classification.html#roc-curve-auc-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve & AUC",
    "text": "ROC Curve & AUC\n\n\nThe ROC (Receiver Operating Characteristic) curve is a graphical representation that illustrates the trade-off between the true positive rate (Sensitivity) and the false positive rate (1 - Specificity) across various threshold values. The AUC (Area Under the Curve) summarizes the ROC curve into a single value, indicating the model’s overall performance.\n\nAUC Interpretation:\n\nAUC \\(\\approx\\) 1: The model has near-perfect predictive capability.\nAUC \\(\\approx\\) 0.5: The model performs no better than random guessing."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#summary-1",
    "href": "lecture_slides/03_classification/03_classification.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts:\n\nClassification involves predicting categorical outcomes based on input features.\nPopular approaches include:\n\nLogistic Regression: Directly models probabilities; suitable for \\(K=2\\) and extendable to \\(K &gt; 2\\).\nDiscriminant Analysis: Assumes Gaussian distributions; suitable for small datasets or when classes are well separated.\nNaïve Bayes: Assumes feature independence; works well with large \\(p\\) or mixed data types.\n\nThresholds and ROC Curves allow fine-tuning between false positive and false negative rates.\n\n\n\n\nPractical Insights\n\nLinear vs Logistic Regression: Logistic regression avoids issues with probabilities outside [0, 1].\nDiscriminant Analysis: Use Linear Discriminant Analysis (LDA) for shared covariance matrices or Quadratic Discriminant Analysis (QDA) when covariance matrices differ.\nNaïve Bayes: Despite its simplicity, it often performs well due to reduced variance and works for both qualitative and quantitative data.\nGeneralized Linear Models (GLMs): Extend regression to different types of responses with appropriate link and variance functions."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#recap",
    "href": "lecture_slides/03_classification/03_classification.html#recap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nWhat is the fundamental difference between a regression problem and a classification problem in supervised learning, based on the nature of the outcome variable?\n\n\nRegression predicts a continuous numeric outcome (quantitative response). Classification predicts a discrete category/label (qualitative response, e.g., binary or multiclass).\n\n\n\n\nLoad the Advertising.csv and obtain descriptive statistics:\n\n\ndf = pd.read_csv(Advertising.csv)df.describe\n\n\n\n\nWhat is a possible way to check if there is a relationship between predictors and an outcome variable? performance?\n\n\nRegression Models.\n\n\n\n\n\nIn a regression model, how can we determine the strength of the relationship between predictors and our response variable?\n\n\nCan be assessed by examining the size and statistical significance of the predictors coefficients, the model’s goodness-of-fit, and its predictive performance. Large and statistically significant coefficients (low p-values with narrow confidence intervals) indicate that changes in predictors are strongly associated with changes in the outcome variable. R-squared and adjusted R-squared values quantify how much of the variance in the outcome variable is explained by the predictors.\n\n\n\n\nHow to discover which predictor is associated with the outcome variable?\n\n\nCheck coefficients, p-values, and confidence intervals for statistical significance. In multiple regression, standardized coefficients and model fit (R², adjusted R²) indicate relative importance.\n\n\n\n\nHow can you determine if there is synergy among predictors in terms of the outcome variable?\n\n\nSynergy among predictors can be determined by testing for interaction effects in a regression model, where you add product terms (e.g., TV × Radio) alongside the main predictors. A significant interaction coefficient indicates that the effect of one predictor on the outcome depends on the level of another, revealing synergy (positive or negative)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#discriminative-vs.-generative",
    "href": "lecture_slides/03_classification/03_classification.html#discriminative-vs.-generative",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminative vs. Generative",
    "text": "Discriminative vs. Generative\n\n\n\n\n\nDiscriminative (Logistic Regression)\n\nModel the conditional distribution of the response\n\\[\n\\Pr(Y=k\\mid X=x)\n\\]\nLearn parameters by maximizing the (multinomial) log-likelihood (cross-entropy).\nDirectly outputs class posterior probabilities.\n\n\nGenerative (LDA, QDA, NB)\n\nModel the class-conditional distribution of the predictors\n\\[\nf_k(x)\\equiv \\Pr(X=x\\mid Y=k)\n\\] and the class prior \\(\\pi_k=\\Pr(Y=k)\\).\nApply Bayes’ theorem to “flip” into posteriors: \\[\np_k(x)=\\Pr(Y=k\\mid X=x)=\n\\frac{\\pi_k\\,f_k(x)}{\\sum_{l=1}^{K}\\pi_l\\,f_l(x)}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#why-use-a-generative-approach",
    "href": "lecture_slides/03_classification/03_classification.html#why-use-a-generative-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why use a generative approach?",
    "text": "Why use a generative approach?\n\n\nWith large class separation, logistic-regression parameters can be unstable; generative models are less sensitive.\nIf \\(X\\mid Y=k\\) is approximately normal and \\(n\\) is small, generative methods can be more accurate than logistic regression.\nNatural extension to multi-class (\\(K&gt;2\\)) settings."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#setup-notation",
    "href": "lecture_slides/03_classification/03_classification.html#setup-notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Setup & Notation",
    "text": "Setup & Notation\n\n\n\\(K\\ge 2\\) unordered classes.\n\\(\\pi_k=\\Pr(Y=k)\\): class prior.\n\\(f_k(x)=\\Pr(X=x\\mid Y=k)\\): density of predictors in class \\(k\\).\nLarge \\(f_k(x)\\) ⇒ \\(x\\) looks typical for class \\(k\\).\nPosterior probability (repeat of 4.15): \\[\np_k(x)=\\frac{\\pi_k\\,f_k(x)}{\\sum_{l=1}^{K}\\pi_l\\,f_l(x)}.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#bayes-classifier",
    "href": "lecture_slides/03_classification/03_classification.html#bayes-classifier",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes Classifier",
    "text": "Bayes Classifier\n\n\nDecision rule: \\[\n\\widehat{C}_{\\text{Bayes}}(x)\n=\\arg\\max_{k\\in\\{1,\\dots,K\\}}\\, p_k(x).\n\\]\nIf model components are correctly specified, the Bayes classifier achieves the lowest possible error rate among all classifiers."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#estimating-pi_k-and-f_kx",
    "href": "lecture_slides/03_classification/03_classification.html#estimating-pi_k-and-f_kx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating \\(\\pi_k\\) and \\(f_k(x)\\)",
    "text": "Estimating \\(\\pi_k\\) and \\(f_k(x)\\)\n\n\n\\(\\pi_k\\): estimate via class frequencies in the training set.\n\\(f_k(x)\\): requires assumptions/approximations:\n\nGaussian class-conditional densities with shared covariance ⇒ LDA.\nGaussian class-conditional densities with class-specific covariance ⇒ QDA.\nConditional independence of features given class ⇒ Naïve Bayes.\n\nPlug estimates \\(\\hat{\\pi}_k\\) and \\(\\hat{f}_k(x)\\) into (4.15) to approximate the Bayes classifier."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#three-generative-classifiers-at-a-glance",
    "href": "lecture_slides/03_classification/03_classification.html#three-generative-classifiers-at-a-glance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Three Generative Classifiers (at a glance)",
    "text": "Three Generative Classifiers (at a glance)\n\n\n\n\nLinear Discriminant Analysis (LDA)\n\nAssumption: \\(X\\mid Y=k \\sim \\mathcal{N}(\\mu_k,\\Sigma)\\) with common \\(\\Sigma\\).\nLeads to linear decision boundaries in \\(x\\).\n\n\nQuadratic Discriminant Analysis (QDA)\n\nAssumption: \\(X\\mid Y=k \\sim \\mathcal{N}(\\mu_k,\\Sigma_k)\\) with class-specific \\(\\Sigma_k\\).\nLeads to quadratic decision boundaries; more flexible, higher variance."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#naïve-bayes",
    "href": "lecture_slides/03_classification/03_classification.html#naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes",
    "text": "Naïve Bayes\n\n\nAssumption: features are conditionally independent given \\(Y\\).\nDensity factorization: \\[\nf_k(x)=\\prod_{j=1}^p f_{kj}(x_j\\mid Y=k).\n\\]\nScales well in high dimensions; often competitive even when independence is only approximately true."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#summary",
    "href": "lecture_slides/03_classification/03_classification.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\nLogistic regression: model \\(\\Pr(Y\\mid X)\\) directly (discriminative).\nGenerative route: model \\(\\Pr(X\\mid Y)\\) and \\(\\Pr(Y)\\), then use Bayes Theorem to get posteriors.\nBayes classifier picks the class with the largest posterior and is optimal if the model is correct.\nLDA/QDA/Naïve Bayes are practical estimators of the Bayes rule under different assumptions on \\(f_k(x)\\)."
  },
  {
    "objectID": "lecture_slides/03_classification/03_classification.html#summary-2",
    "href": "lecture_slides/03_classification/03_classification.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts:\n\nClassification involves predicting categorical outcomes based on input features.\nPopular approaches include:\n\nLogistic Regression: Directly models probabilities; suitable for \\(K=2\\) and extendable to \\(K &gt; 2\\).\nDiscriminant Analysis: Assumes Gaussian distributions; suitable for small datasets or when classes are well separated.\nNaïve Bayes: Assumes feature independence; works well with large \\(p\\) or mixed data types.\n\nThresholds and ROC Curves allow fine-tuning between false positive and false negative rates.\n\n\n\n\nPractical Insights\n\nLinear vs Logistic Regression: Logistic regression avoids issues with probabilities outside \\(0, 1\\).\nDiscriminant Analysis: Use Linear Discriminant Analysis (LDA) for shared covariance matrices or Quadratic Discriminant Analysis (QDA) when covariance matrices differ.\nNaïve Bayes: Despite its simplicity, it often performs well due to reduced variance and works for both qualitative and quantitative data.\nGeneralized Linear Models (GLMs): Extend regression to different types of responses with appropriate link and variance functions."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#overview",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nMotivation\nTraining Error versus Test Error\nValidation-Set Approach\nCross-Validation\nCross-Validation for Classification Problems\n\n\n\nBootstrap\nMore on Bootstrap\nCan the Bootstrap Estimate Prediction Error?\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#prediction",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#prediction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Prediction",
    "text": "Prediction\n\nGoal: Build predictors and classifiers to make accurate predictions from data.\nChallenge: How do we evaluate our predictions?\n\n\nIdeal Scenario: New Data\n\nThe best way to test predictions is to use new, independent data from the population.\nProblem: New data isn’t always available."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#why-not-use-training-data",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#why-not-use-training-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Not Use Training Data?",
    "text": "Why Not Use Training Data?\n\nUsing training data for evaluation is not reliable.\n\nModels tend to perform better on data they’ve already seen.\nThis leads to overly optimistic results."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#solution-resampling-methods",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#solution-resampling-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Solution: Resampling methods",
    "text": "Solution: Resampling methods\n\nCross-validation and the Bootstrap are two resampling methods.\nThese methods allows us to evaluate the performance of our predictors using the available data without relying on additional samples.\nThey refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.\nFor example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#training-error-versus-test-error-1",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#training-error-versus-test-error-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training Error versus Test Error",
    "text": "Training Error versus Test Error\n\nRecall the distinction between the test error and the training error:\nThe test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.\nThe training error can be easily calculated by applying the statistical learning method to the observations used in its training.\nBut the training error rate often is quite different from the test error rate, and in particular, the former can dramatically underestimate the latter."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#training--versus-test-set-performance",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#training--versus-test-set-performance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training- versus Test-Set Performance",
    "text": "Training- versus Test-Set Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHorizontal Axis: Represents model complexity (low to high).\n\nLow complexity: Simpler models with fewer parameters (e.g., fitting a straight line or using a few features).\nHigh complexity: More complex models with many parameters (e.g., higher-degree polynomials or many features).\n\nVertical Axis: Represents prediction error.\n\nLower values indicate better predictive performance.\n\n\n\n\nTraining Error (Blue Curve):\n\nStarts high at low complexity because simple models underfit the training data.\nDecreases steadily as the model becomes more complex, fitting the training data better.\nContinues to decline even as the model becomes overly complex.\n\nTest Error (Red Curve):\n\nStarts high at low complexity due to underfitting (failure to generalize).\nDecreases as complexity increases and the model starts capturing relevant patterns.\nReaches a minimum at the optimal complexity (sweet spot).\nIncreases again at high complexity due to overfitting (model captures noise instead of general patterns).\n\n\n\nKey Concepts\nBias-Variance Tradeoff:\n\nHigh Bias (Left Side): Simple models fail to capture the true structure of the data.\nHigh Variance (Right Side): Complex models become overly tailored to the training data and fail to generalize.\n\nOptimal Complexity:\n\nLocated where the test error is minimized.\nBalances bias and variance for the best generalization performance.\n\nThe Goal is to select a model complexity that minimizes test error to ensure good predictive performance on unseen data."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#precision-and-accuracy",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#precision-and-accuracy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Precision and Accuracy",
    "text": "Precision and Accuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision: Refers to the consistency or reliability of the model’s predictions.\nAccuracy: Refers to how close the model’s predictions are to the true values.\n\n\nIn the context of regression:\n\nHigh Precision, Low Accuracy: Predictions are consistent but biased.\nHigh Precision, High Accuracy: Predictions are both consistent and valid.\nLow Precision, Low Accuracy: Predictions are neither consistent nor valid.\nLow Precision, High Accuracy: Predictions are valid on average but have high variability."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#more-on-prediction-error-estimates",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#more-on-prediction-error-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Prediction-Error Estimates",
    "text": "More on Prediction-Error Estimates\n\nBest solution: test the model with a large test set.\n\nHowever, it is not very often available.\n\nIn the absence of a large test set, some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate.\n\nThese include the Cp statistic, AIC, and BIC.\n\nIn this lecture we consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held-out observations."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#validation-set-approach-1",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#validation-set-approach-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation-Set Approach",
    "text": "Validation-Set Approach\n\nHere we randomly divide the available set of samples into two parts: a training set and a validation or hold-out set.\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\nThe resulting validation-set error provides an estimate of the test error.\n\nThis is typically assessed using the Mean Squared Error (MSE) in the case of a quantitative response and Misclassification Rate in the case of a qualitative (discrete) response."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-validation-process",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-validation-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Validation Process",
    "text": "The Validation Process\n\n\nA random splitting of the original dataset into two halves (two-fold validation):\n\nLeft part is the training set\nRight part is the validation set"
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-automobile-data",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-automobile-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Automobile Data",
    "text": "Example: Automobile Data\n\n\n\nWant to compare linear vs higher-order polynomial terms in a linear regression.\nWe randomly split the 392 observations into two sets:\n\nA training set containing 196 of the data points.\nA validation set containing the remaining 196 observations.\n\n\n\n\n\n\n\n\n\n\n\nLeft panel shows single split; right panel shows multiple splits."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#drawbacks-of-validation-set-approach",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#drawbacks-of-validation-set-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Drawbacks of Validation Set Approach",
    "text": "Drawbacks of Validation Set Approach\n\nThe validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nIn the validation approach, only a subset of the observations — those that are included in the training set rather than in the validation set — are used to fit the model.\nThis suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set. Why?\n\nHaving more data generally leads to lower error because it provides more information for training the model.\nFor example, training on 200 observations is typically preferable to 100 observations, as larger datasets improve accuracy.\nHowever, when the training set is reduced (e.g., during validation), error estimates can be higher since smaller datasets may fail to capture all patterns in the data.\nThis limitation highlights the drawbacks of simple validation.\nCross-validation addresses this issue by efficiently using the data to produce more accurate and reliable error estimates."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nWidely used approach for estimating test error.\nEstimates can be used to select the best model and to give an idea of the test error of the final chosen model.\nThe idea is to randomly divide the data into \\(K\\) equal-sized parts. We leave out part \\(k\\), fit the model to the other \\(K-1\\) parts (combined), and then obtain predictions for the left-out \\(k\\)-th part.\nThis is done in turn for each part \\(k = 1, 2, \\ldots, K\\), and then the results are combined."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation-in-detail",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation in Detail",
    "text": "K-Fold Cross-Validation in Detail\nDivide data into \\(K\\) roughly equal-sized parts (\\(K = 5\\) here)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation-in-detail-1",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation-in-detail-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation in Detail",
    "text": "K-Fold Cross-Validation in Detail\nDivide data into \\(K\\) roughly equal-sized parts (\\(K = 3\\) here).\n\n\n\n\nWiki"
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation-in-algebra",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#k-fold-cross-validation-in-algebra",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation: in algebra",
    "text": "K-Fold Cross-Validation: in algebra\n\nLet the \\(K\\) parts be \\(C_1, C_2, \\ldots, C_K\\), where \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(N\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute the cross-validations error rate:\n\n\n\\[\n  \\text{CV}_{(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{MSE}_k\n\\]\nwhere \\(\\text{MSE}_k = \\frac{\\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2}{n_k}\\), and \\(\\hat{y}_i\\) is the fit for observation \\(i\\), obtained from the data with part \\(k\\) removed.\n\nSpecial case: Setting \\(K = n\\) yields \\(n\\)-fold or leave-one-out cross-validation (LOOCV)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#leave-one-out-cross-validation-loocv",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#leave-one-out-cross-validation-loocv",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\n\n\nWiki"
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#a-nice-special-case",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#a-nice-special-case",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A Nice Special Case!",
    "text": "A Nice Special Case!\nWith least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n\\[\n  \\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2,\n\\]\nwhere \\(\\hat{y}_i\\) is the \\(i\\)-th fitted value from the original least-squares fit, and \\(h_i\\) is the leverage (diagonal of the “hat” matrix; see book for details). This is like the ordinary MSE, except the \\(i\\)-th residual is divided by \\(1 - h_i\\).\n\nLOOCV is sometimes useful, but typically doesn’t shake up the data enough. The estimates from each fold are highly correlated, and hence their average can have high variance.\nA better choice is \\(K = 5\\) or \\(K = 10\\)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-auto-data-revisited",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-auto-data-revisited",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Auto Data Revisited",
    "text": "Example: Auto Data Revisited\n\n\nLeft plot: Similar to the two halve validation;\nRight plot: Tenfold cross validation. With 10 different partitions of the data to train and test the model we see there is not much variability. The results are consistent, in contrast to the result when we divided into two parts."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "True and Estimated Test MSE for the Simulated Data",
    "text": "True and Estimated Test MSE for the Simulated Data\n\n\n\n\n\n\n\n\n\n\n\nThe plot presents the cross-validation estimates and true test error rates that result from applying smoothing splines to the simulated data sets illustrated in Figures 2.9–2.11 of Chapter 2 of the book.\nThe true test MSE is displayed in blue.\nThe black dashed and orange solid lines respectively show the estimated LOOCV and 10-fold CV estimates.\nIn all three plots, the two cross-validation estimates are very similar.\nRight-hand panel: the true test MSE and the cross-validation curves are almost identical.\nCenter panel: the two sets of curves are similar at the lower degrees of flexibility, while the CV curves overestimate the test set MSE for higher degrees of flexibility.\nLeft-hand panel: the CV curves have the correct general shape, but they underestimate the true test MSE."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#potential-issues-with-cross-validation",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#potential-issues-with-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Potential Issues with Cross-Validation",
    "text": "Potential Issues with Cross-Validation\n\nSince each training set is only \\(\\frac{K - 1}{K}\\) as big as the original training set, the estimates of prediction error will typically be biased upward. Why?\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has high variance, as noted earlier.\n\\(K = 5\\) or \\(10\\) provides a good compromise for this bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#cross-validation-for-classification-problems-1",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#cross-validation-for-classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation for Classification Problems",
    "text": "Cross-Validation for Classification Problems\n\nWe divide the data into \\(K\\) roughly equal-sized parts \\(C_1, C_2, \\ldots, C_K\\). \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(n\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\n\nCompute the cross-validation misclassification error:\n\n\n\\[\n  \\text{CV}_K = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{Err}_k\n\\]\nwhere \\(\\text{Err}_k = \\frac{\\sum_{i \\in C_k} I(y_i \\neq \\hat{y}_i)}{n_k}\\).\n\nThe estimated standard deviation of \\(\\text{CV}_K\\) is:\n\n\n\n\\[\n  \\widehat{\\text{SE}}(\\text{CV}_K) = \\sqrt{\\frac{1}{K} \\sum_{k=1}^{K} \\frac{(\\text{Err}_k - \\overline{\\text{Err}_k})^2}{K - 1}}\n\\]\n\nThis is a useful estimate, but strictly speaking, not quite valid. Why not?\n\nWe compute the standard errors assuming these were independent observations, but they are not strictly independent as they share some training samples. So there’s some correlation between them."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-setting",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-setting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Setting",
    "text": "The Setting\n\nHigh‐dimensional data: We have 50 samples (observations) but 5000 predictors (features). In many modern applications—such as genomics—it is typical to have many more predictors than observations.\nGoal: Two‐class classification\nFeature selection (Step 1): We first look at the correlation of each of the 5000 predictors with the class labels, and we pick the 100 “best” predictors—the ones that exhibit the largest correlation with the class labels.\nModel fitting (Step 2): Once those top 100 are chosen, we fit a classifier (e.g., logistic regression) using only those top 100 predictors.\nThe question is how to estimate the true test error of this two‐step procedure."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-tempting-but-wrong-approach",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-tempting-but-wrong-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Tempting (but Wrong) Approach",
    "text": "The Tempting (but Wrong) Approach\nA common mistake is to ignore Step 1 when doing cross‐validation and to apply cross‐validation only to Step 2. That is, one might simply take the already‐selected 100 features and then do, say, 10‐fold cross‐validation on the logistic regression.\n\nWhy people do this: It seems natural to say, “Now that we have our 100 features, let’s cross‐validate the classifier we fit with these 100 features.”\nWhat goes wrong: By the time you pick those 100 “best” features, the data set has already “seen” all the labels in the process of ranking and filtering. This filtering step is actually part of training, because it used the outcome labels to choose features.\n\n\nSkipping Step 1 in the cross‐validation will invariably produce an overly optimistic (often wildly optimistic) estimate of test error."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#why-it-is-wrong-data-leakage",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#why-it-is-wrong-data-leakage",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why It Is Wrong: Data Leakage",
    "text": "Why It Is Wrong: Data Leakage\n\nData leakage: The crucial point is that feature selection (filtering) depends on the relationship between each feature and the class labels. Hence, it is not “just a preprocessing step”—it is using the label information. Thus, Step 1 is part of the model‐building process.\nOverfitting by cherry‐picking: With thousands of predictors, even if none is truly predictive, by sheer chance some predictors will appear correlated with the class labels in the sample. Selecting only the strongest correlations can give the illusion that the model has learned meaningful structure, when in fact it is just capturing random noise.\nAn extreme illustration: If you simulate data where the class labels are purely random (true error = 50%), but you pick the top 100 out of 5000 or 5 million random features, then do cross‐validation only after you have chosen those top 100, you can easily see cross‐validation estimates near 0% error—clearly a false, biased result."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-correct-right-way-to-apply-crossvalidation",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-correct-right-way-to-apply-crossvalidation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Correct (Right) Way to Apply Cross‐Validation",
    "text": "The Correct (Right) Way to Apply Cross‐Validation\n\nThe key principle is that any step that uses the outcome labels must occur inside the cross‐validation loop. Concretely:\n\nSplit the data into training/validation folds (e.g., 10‐fold CV).\nFor each fold:\n\nTreat that fold as a hold‐out set.\n\nOn the remaining training folds, perform the entire procedure:\n\nFeature selection (filtering to the top 100 based on correlation with the class labels in the training folds only).\n\nFit the classifier (e.g., logistic regression) to those top 100 features in those training folds.\n\n\nFinally, evaluate the trained model on the hold‐out fold—with only the 100 features selected from the training folds.\n\nRepeat for each fold, then average the error rates (or other metrics).\n\n\nBy doing this, each hold‐out fold is kept separate from both feature selection and model training. This ensures that Step 1 (feature selection) is “relearned” anew in each training subset, just as Step 2 (the classifier) is. As a result, the cross‐validation error you compute properly reflects how the entire procedure—from filtering out thousands of features down to fitting the logistic model—would perform on truly unseen data."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#summary",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nWrong: Select your 100 predictors once using all the data, then cross‐validate only the final classifier. This leads to overly optimistic, biased estimates of test error because it ignores that you used the labels in selecting those 100 predictors.\nRight: Wrap the entire two‐step process (selection and model fitting) inside the cross‐validation loop. Each fold’s feature‐selection step must be done without knowledge of the hold‐out fold’s labels.\n\n\nFollowing this correct approach is essential whenever one performs early filtering, variable selection, hyperparameter tuning, or any other step that uses the outcome labels. Such steps must be regarded as part of the training process and repeated inside each cross‐validation iteration."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-bootstrap",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\nThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.\nFor example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#where-does-the-name-come-from",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#where-does-the-name-come-from",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Where Does the Name Come From?",
    "text": "Where Does the Name Come From?\n\nThe use of the term bootstrap derives from the phrase to pull oneself up by one’s bootstraps, widely thought to be based on one of the eighteenth-century The Surprising Adventures of Baron Munchausen by Rudolph Erich Raspe:\n\nThe Baron had fallen to the bottom of a deep lake. Just when it looked like all was lost, he thought to pick himself up by his own bootstraps.\n\nIt is not the same as the term bootstrap used in computer science, meaning to “boot” a computer from a set of core instructions, though the derivation is similar."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example",
    "text": "Example\n\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y\\), respectively, where \\(X\\) and \\(Y\\) are random quantities.\nWe will invest a fraction \\(\\alpha\\) of our money in \\(X\\), and will invest the remaining \\(1 - \\alpha\\) in \\(Y\\).\nWe wish to choose \\(\\alpha\\) to minimize the total risk, or variance, of our investment. In other words, we want to minimize \\(\\text{Var}(\\alpha X + (1 - \\alpha) Y).\\)\nOne can show that the value that minimizes the risk is given by:\n\n\n\\[\n  \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n\\]\nwhere \\(\\sigma_X^2 = \\text{Var}(X)\\), \\(\\sigma_Y^2 = \\text{Var}(Y)\\), and \\(\\sigma_{XY} = \\text{Cov}(X, Y)\\)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\nBut the values of \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\), and \\(\\sigma_{XY}\\) are unknown.\nWe can compute estimates for these quantities, \\(\\hat{\\sigma}_X^2\\), \\(\\hat{\\sigma}_Y^2\\), and \\(\\hat{\\sigma}_{XY}\\), using a data set that contains measurements for \\(X\\) and \\(Y\\).\nWe can then estimate the value of \\(\\alpha\\) that minimizes the variance of our investment using:\n\n\n\\[\n  \\hat{\\alpha} = \\frac{\\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY}}{\\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 - 2\\hat{\\sigma}_{XY}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued-1",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\n\n\n\n\n\n\n\n\n\nEach panel displays 100 simulated returns for investments X and Y. From left to right and top to bottom, the resulting estimates for \\(\\alpha\\), the fraction to minimize the total risk, are 0.576, 0.532, 0.657, and 0.651."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued-2",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\n\nTo estimate the standard deviation of \\(\\hat{\\alpha}\\), we repeated the process of simulating 100 paired observations of \\(X\\) and \\(Y\\), and estimating \\(\\alpha\\) 1,000 times.\nWe thereby obtained 1,000 estimates for \\(\\alpha\\), which we can call \\(\\hat{\\alpha}_1, \\hat{\\alpha}_2, \\ldots, \\hat{\\alpha}_{1000}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left-hand panel of the Figure displays a histogram of the resulting estimates.\nFor these simulations, the parameters were set to \\(\\sigma_X^2 = 1, \\, \\sigma_Y^2 = 1.25, \\, \\sigma_{XY} = 0.5,\\) and so we know that the true value of \\(\\alpha\\) is 0.6 (indicated by the red line)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued-3",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-continued-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\nThe mean over all 1,000 estimates for \\(\\alpha\\) is:\n\\[\n  \\bar{\\alpha} = \\frac{1}{1000} \\sum_{r=1}^{1000} \\hat{\\alpha}_r = 0.5996,\n\\]\nvery close to \\(\\alpha = 0.6\\), and the standard deviation of the estimates is:\n\n\\[\n  \\sqrt{\\frac{1}{1000 - 1} \\sum_{r=1}^{1000} (\\hat{\\alpha}_r - \\bar{\\alpha})^2} = 0.083.\n\\]\n\nThis gives us a very good idea of the accuracy of \\(\\hat{\\alpha}\\): \\(\\text{SE}(\\hat{\\alpha}) \\approx 0.083\\).\nSo roughly speaking, for a random sample from the population, we would expect \\(\\hat{\\alpha}\\) to differ from \\(\\alpha\\) by approximately 0.08, on average."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-results",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Results",
    "text": "Example Results\n\nLeft: A histogram of the estimates of \\(\\alpha\\) obtained by generating 1,000 simulated data sets from the true population.\nCenter: A histogram of the estimates of \\(\\alpha\\) obtained from 1,000 bootstrap samples from a single data set.\nRight: The estimates of \\(\\alpha\\) displayed in the left and center panels are shown as boxplots.\n\nIn each panel, the pink line indicates the true value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#now-back-to-the-real-world",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#now-back-to-the-real-world",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now Back to the Real World",
    "text": "Now Back to the Real World\n\nThe procedure outlined above cannot be applied, because for real data we cannot generate new samples from the original population.\nHowever, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.\nRather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set with replacement.\nEach of these “bootstrap data sets” is created by sampling with replacement, and is the same size as our original dataset. As a result, some observations may appear more than once in a given bootstrap data set and some not at all."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-with-just-3-observations",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#example-with-just-3-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example with Just 3 Observations",
    "text": "Example with Just 3 Observations\n\n\nA graphical illustration of the bootstrap approach on a small sample containing \\(n = 3\\) observations.\nEach bootstrap data set contains \\(n\\) observations, sampled with replacement from the original data set.\nEach bootstrap data set is used to obtain an estimate of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#bootstrap-standard-error",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#bootstrap-standard-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bootstrap Standard Error",
    "text": "Bootstrap Standard Error\n\nDenoting the first bootstrap data set by \\(Z^{*1}\\), we use \\(Z^{*1}\\) to produce a new bootstrap estimate for \\(\\alpha\\), which we call \\(\\hat{\\alpha}^{*1}\\).\nThis procedure is repeated \\(B\\) times for some large value of \\(B\\) (say 100 or 1000), in order to produce \\(B\\) different bootstrap data sets, \\(Z^{*1}, Z^{*2}, \\ldots, Z^{*B}\\), and \\(B\\) corresponding \\(\\alpha\\) estimates, \\(\\hat{\\alpha}^{*1}, \\hat{\\alpha}^{*2}, \\ldots, \\hat{\\alpha}^{*B}\\).\nWe estimate the standard error of these bootstrap estimates using the formula:\n\n\n\\[\nSE_B(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B - 1} \\sum_{r=1}^B (\\hat{\\alpha}^{*r} - \\bar{\\alpha}^{*})^2}.\n\\]\n\nThis serves as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. See center and right panels of Figure on slide 29. Bootstrap results are in blue.\nFor this example \\(SE_B(\\hat{\\alpha}) = 0.087\\)."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A General Picture for the Bootstrap",
    "text": "A General Picture for the Bootstrap\n\n\n\n\n\nReal World\n\nPopulation \\(P\\)\n\nWe imagine there is a true, unknown population (or data‐generating process).\n\nIn practice, we typically do not have direct access to all of \\(P\\).\n\nRandom Sampling\n\nWe draw a finite sample \\(Z = (z_1, z_2, \\dots, z_n)\\) from the population \\(P\\).\n\nThis sample \\(Z\\) is our observed dataset (often called the “training data” in applied work).\n\nEstimate \\(f(Z)\\)\n\nFrom this observed data \\(Z\\), we compute a statistic or estimate, denoted \\(f(Z)\\).\n\nExamples might include a mean, a regression coefficient, or (in the investment example) an optimal allocation parameter \\(\\alpha\\).\n\n\nIn short, the Real World side shows how our single dataset \\(Z\\) arrives by randomly sampling from the true population \\(P\\).\n\nBootstrap World\n\nEstimated Population \\(\\hat{P}\\)\n\nBecause we usually cannot sample repeatedly from the real population \\(P\\), the bootstrap creates a stand‐in population \\(\\hat{P}\\). We ‘replace’ the population by our sample.\n\\(\\hat{P}\\) is the empirical distribution function of the observed data \\(Z\\). Informally, it assigns probability \\(\\tfrac{1}{n}\\) to each observed point in \\(Z\\).\n\nRandom Sampling from \\(\\hat{P}\\)\n\nTo mimic drawing new data from the real population, we instead draw (with replacement) from \\(\\hat{P}\\).\n\nThis produces a bootstrap dataset \\(Z^* = (z_1^*, z_2^*, \\dots, z_n^*)\\). Each \\(z_i^*\\) is sampled (with replacement) from among the original observed points \\(\\{z_1, \\dots, z_n\\}\\).\n\nBootstrap Estimate \\(f(Z^*)\\)\n\nWe compute the same statistic (or estimator) on each bootstrap sample, giving \\(f(Z^*)\\).\n\nBy repeating this bootstrap sampling many times, we obtain a distribution of estimates \\(\\{f(Z^*_1), f(Z^*_2), \\dots\\}\\). This approximates how \\(f(Z)\\) would vary if we could repeatedly resample from the true population."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-bootstrap-in-general",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#the-bootstrap-in-general",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap in General",
    "text": "The Bootstrap in General\n\nIn more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.\nFor example, if the data is a time series, we can’t simply sample the observations with replacement (why not?).\n\nThe main reason we typically cannot simply resample individual points with replacement in a time series is that time‐ordered data exhibits serial dependence. That is, adjacent observations (e.g., today’s stock price and yesterday’s stock price) are correlated in ways that we lose if we treat all observations as independent units and shuffle them arbitrarily.\nA simple i.i.d. bootstrap would ignore the natural ordering of the data points (and the correlations it encodes), thereby violating a crucial assumption about the structure of time‐series data.\n\nWe can instead create blocks of consecutive observations and sample those with replacements. Then we paste together sampled blocks to obtain a bootstrap dataset."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#other-uses-of-the-bootstrap",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#other-uses-of-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Uses of the Bootstrap",
    "text": "Other Uses of the Bootstrap\n\nPrimarily used to obtain standard errors of an estimate.\nAlso provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the figure on slide 29, the 5% and 95% quantiles of the 1,000 values is (0.43, 0.72).\nThis represents an approximate 90% confidence interval for the true α. How do we interpret this confidence interval?\nThe above interval is called a Bootstrap Percentile confidence interval. It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#can-the-bootstrap-estimate-prediction-error-1",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#can-the-bootstrap-estimate-prediction-error-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can the Bootstrap Estimate Prediction Error?",
    "text": "Can the Bootstrap Estimate Prediction Error?\n\nIn cross-validation, each of the \\(K\\) validation folds is distinct from the other \\(K-1\\) folds used for training: there is no overlap. This is crucial for its success. Why?\n\nThere is a clear separation, no overlap, between the train and the test sets.\n\nTo estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.\nBut each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample.\nThis will cause the bootstrap to seriously underestimate the true prediction error.\nThe other way around— with the original sample as the training sample, and the bootstrap dataset as the validation sample— is worse!"
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#removing-the-overlap",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#removing-the-overlap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Removing the Overlap",
    "text": "Removing the Overlap\n\nCan partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.\nBut the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#summary-2",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nResampling Methods\n\nCross-validation and Bootstrap allow evaluation of model performance using existing data.\nThey provide estimates of:\n\nTest-set prediction error\nStandard deviation and bias of parameter estimates.\n\n\nTraining vs Test Error\n\nTraining error decreases with model complexity.\nTest error decreases, then increases due to bias-variance tradeoff:\n\nHigh Bias: Simple models underfit the data.\nHigh Variance: Complex models overfit the training data.\n\nOptimal complexity minimizes test error.\n\n\nValidation-Set Approach\n\nDivides data into training and validation sets.\nValidation error provides an estimate of test error but:\n\nCan vary based on data split.\nMay overestimate test error due to smaller training sets.\n\n\nCross-Validation\n\nK-Fold Cross-Validation:\n\nDivides data into \\(K\\) folds for iterative training and testing.\nBalances bias and variance (e.g., \\(K = 5\\) or \\(10\\)).\n\nLeave-One-Out Cross-Validation (LOOCV):\n\nUses one data point as validation in each iteration.\nLow bias but high variance.\n\n\nBootstrap\n\nEstimates variability and uncertainty of parameter estimates.\nGenerates multiple samples with replacement from the dataset.\nProvides approximate confidence intervals and standard errors."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-region",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-region",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Region",
    "text": "Results for Region\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n531.00\n46.32\n11.464\n&lt; 0.0001\n\n\nregion \\(South\\)\n-18.69\n65.02\n-0.287\n0.7740\n\n\nregion \\(West\\)\n-12.50\n56.68\n-0.221\n0.8260\n\n\n\n\nThe coefficient -18.69 compares South to East and that’s not significant. Likewise, the West to East is also not significant.\n\nNote: the choice of the baseline does not affect the fit of the model. The residual sum of sum of squares will be the same no matter which category we chose as the baseline. At its turn, the p-values will potentially change as we change the baseline category."
  },
  {
    "objectID": "lecture_slides/04_resampling_methods/04_resampling_methods.html#recap",
    "href": "lecture_slides/04_resampling_methods/04_resampling_methods.html#recap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\n\nWhy is linear regression generally not suitable for estimating probabilities in a classification task, even for binary outcomes?\n\n\nLinear regression can produce predictions that fall outside the [0, 1] range, which are not valid probabilities. Logistic regression, on the other hand, uses a link function that ensures its output always lies between 0 and 1, making it appropriate for probability estimation.\n\n\n\n\nWhat is the core principle of Maximum Likelihood Estimation (MLE)? \n\n\nMLE is a method used to estimate the parameters of a statistical model by finding the parameter values that maximize the likelihood function. This means choosing the parameters that make the observed data most probable under the assumed model.\n\n\n\n\nWhat is the softmax function, and why is it essential in multinomial logistic regression? \n\n\nThe softmax function converts a vector of raw scores (logits) into a probability distribution over multiple classes. It is essential because it ensures that the output probabilities for all classes sum to 1, making it suitable for multiclass classification tasks where the response has more than two categories.\n\n\n\n\nWhat is the difference between a discriminative model and a generative model in classification? \n\n\nA discriminative model (e.g., Logistic Regression) directly models the conditional probability \\(Pr(Y|X)\\). A generative model (e.g., LDA, Naïve Bayes) models the class-conditional distribution \\(Pr(X|Y)\\) and the prior \\(Pr(Y)\\), then uses Bayes’ theorem to derive \\(Pr(Y|X)\\).\n\n\n\n\n\nUnder what key assumption is Linear Discriminant Analysis (LDA) applied, and how does this affect its decision boundaries?\n\n\nLDA assumes that the class-conditional distributions of the predictors are Gaussian and share a common covariance matrix across all classes. This assumption leads to decision boundaries that are linear.\n\n\n\n\nWhen would Quadratic Discriminant Analysis (QDA) be preferred over LDA, and what is the main reason for this preference?\n\n\nQDA is preferred over LDA when the class-conditional covariance matrices are significantly different for each class. This allows QDA to capture more complex relationships in the data, resulting in quadratic (curved) decision boundaries, albeit with potentially higher variance due to more parameters.\n\n\n\n\nWhat is the “Naïve” assumption in Naïve Bayes classification, and why is it often made despite its potential inaccuracy?\n\n\nThe “Naïve” assumption is that the features are conditionally independent given the class. This assumption is often made because it greatly simplifies the modeling of high-dimensional densities by allowing one-dimensional densities to be multiplied, making the model computationally efficient and robust even when the assumption is not perfectly met.\n\n\n\n\nWhat is the ROC curve and how to interpret it?\n\n\nThe ROC curve plots True Positive Rate versus False Positive Rate as you vary the score threshold; curves closer to the upper-left are better, the diagonal is random. AUC summarizes ranking quality (0.5=random, 1=perfect) and equals the probability a random positive scores above a random negative. Pick the operating threshold by costs and class mix (optimal ROC slope \\(=\\frac{C_{FP}\\pi_-}{C_{FN}\\pi_+}\\)); if costs are similar, maximize Youden’s \\(J=\\text{TPR}-\\text{FPR}\\). Use PR curves when positives are rare and precision matters, and remember ROC/AUC don’t assess calibration—pair with reliability metrics."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#overview",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nLinear Model Selection and Regularization\nSubset Selection\nStepwise Selection\nForward Stepwise Selection\nBackward Stepwise Selection\nChoosing the Optimal Model\nIndirect Approaches\nValidation and Cross-Validation\n\n\n\nShrinkage Methods\nRidge Regression\nThe Lasso\nSelecting the Tuning Parameter for Ridge Regression and Lasso\nDimension Reduction Methods\nPrincipal Components Regression\nPartial Least Squares (PLS)\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#linear-model-selection-and-regularization-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#linear-model-selection-and-regularization-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Model Selection and Regularization",
    "text": "Linear Model Selection and Regularization\nRecall the linear model\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\n\nIn the lectures that follow, we consider some approaches for extending the linear model framework. We will generalize the linear model in order to accommodate non-linear, but still additive, relationships.\nIn the lectures covering Chapter 8, we consider even more general non-linear models."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#in-praise-of-linear-models",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#in-praise-of-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In praise of linear models!",
    "text": "In praise of linear models!\n\nDespite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\nHence we discuss in this lecture some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#why-consider-alternatives-to-least-squares",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#why-consider-alternatives-to-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why consider alternatives to least squares?",
    "text": "Why consider alternatives to least squares?\n\nPrediction Accuracy: especially when \\(p &gt; n\\), to control the variance.\nModel Interpretability: By removing irrelevant features — that is, by setting the corresponding coefficient estimates to zero — we can obtain a model that is more easily interpreted. We will present some approaches for automatically performing feature selection."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#three-classes-of-methods",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#three-classes-of-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Three classes of methods",
    "text": "Three classes of methods\n\nSubset Selection. We identify a subset of the \\(p\\) predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.Best Subset Selection, Foward Selection, and Backwards Selection are the main techniques here.\nShrinkage. We fit a model involving all \\(p\\) predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance and can also perform variable selection. Ridge Regression and Lasso are the main techniques here.\nDimension Reduction. We project the \\(p\\) predictors into a \\(M\\)-dimensional subspace, where \\(M &lt; p\\). This is achieved by computing \\(M\\) different linear combinations, or projections, of the variables. Then these \\(M\\) projections are used as predictors to fit a linear regression model by least squares. Principal Components Regression and Partial Least Squares are the main techniques here."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#best-subset-selection",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#best-subset-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Best Subset Selection",
    "text": "Best Subset Selection\n\nThe core idea is to identify a simpler model that includes only a subset of the \\(P\\) available predictors, thereby improving interpretability and potentially enhancing predictive performance.\nTo implement best subset selection systematically, we consider every possible combination of predictors and evaluate each resulting model. The process begins with the null model (\\(M_0\\)), which includes no predictors and only an intercept, meaning it predicts the sample mean for all observations. From there, models are incrementally built by incorporating different subsets of predictors, ultimately selecting the model that optimally balances predictive accuracy and complexity. Here are the steps:\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\nFit all \\(\\binom{p}{k}\\) models, “\\(p\\) choose \\(k\\) models”, that contain exactly \\(k\\) predictors. \\(\\binom{p}{k} = \\frac{p!}{k!(p-k)!}\\)\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest Residual Sum of Squares (RSS), or equivalently the largest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\). The goal is to choose the model with the smallest test error, not the smallest training error."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#example---credit-data-set",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#example---credit-data-set",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example - Credit data set",
    "text": "Example - Credit data set\n\n\n\n\n\n\n\n\n\n\n\nFor each possible model containing a subset of the ten predictors in the Credit data set, the Residual Sum of Squares (RSS) and \\(R^2\\) are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and \\(R^2\\).\nThough the data set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables.\nThe reason that there’s a lot of dots in this picture is because there’s a lot of possible sub models given 10 total predictors. We have \\(2^{p} = 2^{10}\\approx 1,000\\) subsets.\nThe number \\(2^p\\) arises because each predictor (out of \\(p\\) predictors) can either be included or excluded from a subset model. This binary decision for each predictor gives \\(2\\) choices (include or exclude). When there are \\(p\\) predictors, the total number of possible subsets (or models) is calculated as \\(2^p\\)."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#extensions-to-other-models",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#extensions-to-other-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Extensions to other models",
    "text": "Extensions to other models\n\n\nThe same ideas apply to other types of models, such as logistic regression.\nWhen dealing with other type of models, instead of the RSS, we look into the deviance (D), which is commonly used in generalized linear models. The deviance is calculated as:\n\n\\[\nD = -2 \\cdot \\log L_{\\text{max}}\n\\]\nwhere:\n\n\\(D\\) is the deviance,\n\\(\\log L_{\\text{max}}\\) is the maximized log-likelihood of the model.\n\nThis formula allows the deviance to serve as a measure of goodness of fit, analogous to the residual sum of squares (RSS) in linear regression, but applicable to a broader class of models."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#stepwise-selection-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#stepwise-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nFor computational reasons, best subset selection cannot be applied with very large \\(p\\).\nBest subset selection may also suffer from statistical problems when \\(p\\) is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.\nThus an enormous search space can lead to overfitting and high variance of the coefficient estimates. For the authors of the book, it is not recommended to use the best subset approach if you have more than 20 predictors.\nFor both of these reasons, stepwise methods, which explore a far more restricted set of models (\\(p^2\\)), are attractive alternatives to best subset selection."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#forward-stepwise-selection",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nForward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nIn particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#forward-stepwise-selection-in-detail",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#forward-stepwise-selection-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection: In Detail",
    "text": "Forward Stepwise Selection: In Detail\n\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors.\nFor \\(k = 0, \\ldots, p - 1\\):\n\n2.1 Consider all \\(p - k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor. This is different from what we were doing in in the best subset selection case. Here do not look at every possible model containing \\(p\\) predictors. Instead, we are just looking at every possible model that contains one more predictor than \\(M_{k-1}\\).\n2.2 Choose the best among these \\(p - k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\).\n\n\nForward Stepwise Selection presents computational advantage over best subset selection. The total number of models evaluated is the sum of the sequence:\n\n\\[\np + (p - 1) + (p - 2) + \\dots + 1 = \\frac{p(p + 1)}{2}\n\\]\n\nFor large \\(p\\), the term \\(\\frac{p(p + 1)}{2}\\) is dominated by \\(\\frac{p^2}{2}\\). Thus, the computational cost is approximately proportional to \\(p^2\\).\nIt is not guaranteed to find the best possible model out of all \\(2^p\\) models containing subsets of the \\(p\\) predictors."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\nThe first four selected models for best subset selection and forward stepwise selection on the Credit data set.\n\n\n\n\n\n\n\n\n# Variables\nBest subset\nForward stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\ncards, income, student, limit\nrating, income, student, limit\n\n\n\nThe first three models are identical but the fourth models differ.\nThis discrepancy happens because there is correlation between features."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-forward-stepwise-selection",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Forward Stepwise Selection",
    "text": "Summary: Forward Stepwise Selection\n\nStepwise selection is a computationally efficient alternative to best subset selection in model building, especially with large predictor sets.\n\n\nHighlights\n\nStepwise selection offers a practical approach for model selection.\nBest subset selection can lead to overfitting, especially with many predictors.\nForward stepwise selection considers fewer models than best subset, making it computationally efficient.\nDeviance generalizes residual sum of squares across various models.\nBest subset selection becomes impractical beyond 30-40 predictors due to computational limits.\nForward stepwise may not always find the optimal model compared to best subset.\nCorrelation between features impacts model selection outcomes between methods.\n\n\nKey Insights\n\nComputational Efficiency: Stepwise selection significantly reduces the number of models evaluated, making it feasible for larger datasets. This is essential in modern data analysis, where predictors can number in the thousands.\nOverfitting Risks: With best subset selection, the risk of overfitting increases as the number of predictors grows, which can lead to poor performance on unseen data. This highlights the importance of model validation techniques.\nModel Nesting: Forward stepwise selection builds models incrementally, ensuring that each new model is a superset of the previous one, which helps maintain a streamlined search process for the best predictors.\nDeviance vs. RSS: Understanding the difference in metrics like deviance and residual sum of squares is crucial for accurately assessing model fit across various types of regression analyses.\nPractical Limits: Most statistical packages struggle with subset selection beyond 30-40 predictors, indicating the need for streamlined methods like stepwise selection in high-dimensional contexts.\nModel Comparison: Forward stepwise selection may yield different models than best subset selection, emphasizing the need for careful evaluation of model performance on independent datasets.\nCorrelation Effects: The discrepancies between the two methods arise from correlations among predictors, showcasing the intricate dynamics of variable selection in regression modeling."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#backward-stepwise-selection-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#backward-stepwise-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nHowever, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#backward-stepwise-selection-details",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#backward-stepwise-selection-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection: details",
    "text": "Backward Stepwise Selection: details\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains all \\(p\\) predictors.\nFor \\(k = p, p - 1, \\ldots, 1\\):\n\n2.1 Consider all \\(k\\) models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k - 1\\) predictors.\n2.2 Choose the best among these \\(k\\) models, and call it \\(\\mathcal{M}_{k-1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#more-on-backward-stepwise-selection",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#more-on-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Backward Stepwise Selection",
    "text": "More on Backward Stepwise Selection\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1 + p(p+1)/2\\) models, and so can be applied in settings where \\(p\\) is too large to apply best subset selection.\nLike forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the \\(p\\) predictors.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\) (so that the full model can be fit). In contrast, forward stepwise can be used even when \\(n &lt; p\\), and so is the only viable subset method when \\(p\\) is very large."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-backward-stepwise-selection",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Backward Stepwise Selection",
    "text": "Summary: Backward Stepwise Selection\n\nBackward stepwise selection removes predictors from a full model to improve efficiency in model selection, contrasting with forward stepwise selection.\n\n\nHighlights\n\nBackward stepwise starts with a full model (\\(M_p\\)) and removes predictors one at a time.\nIt evaluates the least useful predictor to minimize impact on model fit.\nThis method is computationally efficient, considering around \\(p^2/2\\)models.\nOnly applicable when the number of observations (\\(n\\)) is greater than the number of predictors.\n\\(R^2\\)and RSS might mislead model selection, focusing on training error rather than test error.\nCross-validation, AIC, BIC, or adjusted \\(R^2\\)should guide the final model choice.\nBackward and forward selections are not guaranteed to find the best model but can yield good test set results.\n\n\nKey Insights\n\nMethodology Contrast: Backward stepwise selection is an efficient alternative to forward selection, emphasizing the removal of predictors rather than their addition. This reversal highlights different strategies in model optimization.\nModel Evaluation: The approach assesses the least impactful predictors, ensuring that model performance remains stable as predictors are eliminated, which is crucial for maintaining predictive accuracy.\nComputational Efficiency: Backward stepwise selection dramatically reduces computational load compared to best subset selection, making it a suitable option for larger datasets.\nObservational Requirement: This method necessitates that the number of observations is greater than the number of predictors, ensuring that a least squares model can be appropriately fitted, which is a critical consideration in practical applications.\nTraining vs. Test Error: Relying solely on training error metrics like RSS and \\(R^2\\)can lead to overfitting, indicating the need for broader evaluation methods to predict future performance.\nModel Selection Techniques: Utilizing techniques like cross-validation, AIC, or BIC for model selection can help mitigate the risks associated with simply opting for models with the best training error.\nOutcome Consistency: While backward stepwise may not find the absolute best model, it can produce models that perform well on unseen data, demonstrating its practical utility in predictive modeling."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#choosing-the-optimal-model-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#choosing-the-optimal-model-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Optimal Model",
    "text": "Choosing the Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nTherefore, RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#estimating-test-error-two-approaches",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#estimating-test-error-two-approaches",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating test error: two approaches",
    "text": "Estimating test error: two approaches\n\nIndirect: We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\nDirect: We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)\nThese techniques adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables.\n\nThe figure displays \\(C_p\\), BIC, and adjusted \\(R^2\\) for the best model of each size produced by best subset selection on the Credit data set.\n\nIt suggests that we must choose a model with 4 to 6 predictors.\nThe main recommmendation is to keep the model as simple as possible. By identifying that the values do not change too much as we increase the number of predictors, a model with 4 predictors will be recommended."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#mallows-c_p",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#mallows-c_p",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Mallows’ \\(C_p\\)",
    "text": "Mallows’ \\(C_p\\)\n\nMallows’ \\(C_p\\) balances model fit and model complexity:\n\\[\nC_p = \\frac{1}{n} \\left( \\text{RSS} + 2d\\hat{\\sigma}^2 \\right)\n\\]\nwhere:\n\n\\(d\\): Total number of parameters used in the model (including the intercept).\n\n\\(\\hat{\\sigma}^2\\): Estimate of the error variance \\(\\epsilon\\), associated with each response measurement.\n\n\\(\\text{RSS}\\): Residual Sum of Squares, measuring the error between observed and predicted values.\n\n\\(n\\): Number of observations in the dataset.\n\nExplanation\n\nThe \\(\\text{RSS}\\) measures the fit.\nThe penalty term \\(2d\\hat{\\sigma}^2\\) discourages overfitting.\n\nDecision: The lowest, the better!"
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#aic",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#aic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AIC",
    "text": "AIC\nThe Akaike Information Criteria (AIC) criterion is defined for a large class of models fit by maximum likelihood:\n\\[\n  \\text{AIC} = -2 \\log L + 2 \\cdot d,\n\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model.\n\nIn the case of the linear model, \\(-2 \\log L = \\frac{RSS}{\\sigma^2}\\)\nIn the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equivalent.\nAIC and Mallow’s \\(C_p\\) are proportional to each other.\nAIC is a good approach for non-linear models, e.g. logistic regression."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#bic",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#bic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BIC",
    "text": "BIC\nThis is the Bayesian Information Criterion (BIC):\n\\[\n\\text{BIC} = \\frac{1}{n} \\left( \\text{RSS} + \\log(n)d\\hat{\\sigma}^2 \\right).\n\\]\n\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.\nNotice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(\\log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations.\nSince \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than \\(C_p\\) or AIC."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#adjusted-r2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\nFor a least squares model with \\(d\\) variables, the adjusted \\(R^2\\) statistic is calculated as\n\\[\n    \\text{Adjusted } R^2 = 1 - \\frac{\\text{RSS}/(n - d - 1)}{\\text{TSS}/(n - 1)}.\n\\]\nwhere TSS is the total sum of squares, \\(TSS = \\Sigma_i^n(y_i - \\bar{y})^2\\).\n\nUnlike \\(C_p\\), AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted \\(R^2\\) indicates a model with a small test error.\nMaximizing the adjusted \\(R^2\\) is equivalent to minimizing \\(\\frac{\\text{RSS}}{n - d - 1}\\). While RSS always decreases as the number of variables in the model increases, \\(\\frac{\\text{RSS}}{n - d - 1}\\) may increase or decrease, due to the presence of \\(d\\) in the denominator.\nUnlike the \\(R^2\\) statistic, the adjusted \\(R^2\\) statistic pays a price for the inclusion of unnecessary variables in the model."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-c_p-aic-bic-and-adjusted-r-squared",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-c_p-aic-bic-and-adjusted-r-squared",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: \\(C_p\\), AIC, BIC, and adjusted R-squared",
    "text": "Summary: \\(C_p\\), AIC, BIC, and adjusted R-squared\n\nEstimating test error for models involves adjusting training error or using direct methods like cross-validation. Tools like CP, AIC, BIC, and adjusted R-squared help select optimal models.\n\n\nHighlights\n\nEstimating test error is crucial for model selection.\nTwo approaches: indirect adjustment of training error and direct estimation methods.\n\\(C_p\\), AIC, BIC, and adjusted \\(R^2\\) help model comparison.\nA model should minimize \\(C_p\\) and BIC while maximizing adjusted \\(R^2\\).\nAdjusted \\(R^2\\) allows meaningful comparisons across different models.\nCross-validation is versatile and can be applied to various models.\nSimplicity is favored; fewer predictors often yield better results.\n\n\nKey Insights\n\nTest Error Estimation: Accurate test error estimation is vital for model evaluation. It helps choose the best model among multiple options, ensuring reliability in predictions.\nIndirect vs. Direct Methods: Understanding both indirect (adjusting training error) and direct (cross-validation) methods provides flexibility in model evaluation, catering to different scenarios in data analysis.\nModel Selection Criteria: \\(C_p\\), AIC, BIC, and adjusted \\(R^2\\) serve as essential criteria for model selection. They help quantify model performance and complexity, aiding in decision-making.\nMinimizing \\(C_p\\) and BIC: Aiming for lower \\(C_p\\) and BIC values suggests a more parsimonious model, which is often preferred for its simplicity and interpretability while still capturing the necessary relationships.\nCross-Validation Versatility: Cross-validation is a powerful tool applicable to a wide range of models, including non-linear ones, making it a preferred method for estimating test error in various contexts.\nAdjusted \\(R^2\\) Utility: Unlike traditional \\(R^2\\), adjusted \\(R^2\\) provides a way to compare models with differing numbers of predictors, addressing the limitations of model evaluation in regression analysis.\nSimplicity Preference: Favoring simpler models with fewer predictors can lead to better generalization and reduced risk of overfitting, aligning with the principle of Occam’s Razor in statistical modeling."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#validation-and-cross-validation-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#validation-and-cross-validation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation and Cross-Validation",
    "text": "Validation and Cross-Validation\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2, \\ldots\\). Our job here is to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\).\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted \\(R^2\\), in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g., the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n\n\n\nThe validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.\nThe cross-validation errors were computed using \\(k = 10\\) folds. In this case, the validation and cross-validation methods both result in a six-variable model.\nHowever, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.\nIn this setting, we can select a model using the one-standard-error rule.\n\nEstimate Test Error: We compute the test error (e.g., MSE) for each model size.\nCalculate Standard Error: Compute the standard error (SE) of the test error for each model size to account for variability.\nSelect the Model: Identify the model with the lowest test error (the “best” model). Choose the simplest model whose test error is within one SE of the lowest test error."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-validation-and-cross-validation",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-validation-and-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Validation and Cross-validation",
    "text": "Summary: Validation and Cross-validation\n\nValidation and cross-validation help select the best model size by estimating prediction error without needing sigma squared or the number of parameters.\n\n\nHighlights\n\nValidation splits data into training and validation sets for error estimation.\nCross-validation trains on multiple parts of data to improve error estimates.\nChoosing the optimal model size minimizes validation error effectively.\nAvoiding sigma squared estimation is crucial in high-dimensional data scenarios.\nThe one standard error rule favors simpler models that perform similarly to the best.\nBIC tends to prefer smaller models compared to AIC in error estimation.\nNew data challenges continuously evolve statistical methods and research.\n\n\nKey Insights\n\nModel Selection: Validation and cross-validation provide direct methods for estimating prediction error, making them essential for model selection. This ensures the chosen model performs well on unseen data.\nError Estimation: By dividing data into training and validation sets, we can effectively estimate how well a model will generalize, leading to more robust predictions in practice.\nAvoiding Estimation Challenges: In high-dimensional settings, traditional methods for estimating sigma squared and the number of parameters (\\(d\\)) can be unreliable. Cross-validation mitigates these concerns, simplifying the model selection process.\nSimplicity Preference: The one standard error rule encourages selecting simpler models that perform nearly as well as the best, enhancing interpretability and reducing overfitting.\nIterative Evaluation: Cross-validation’s iterative nature allows for more reliable error estimates by using multiple data partitions, thus improving the stability of model evaluations.\nBIC vs. AIC: BIC’s stronger penalty for model complexity often results in smaller models compared to AIC, which can lead to different model selection outcomes.\nEvolving Challenges: The increasing complexity of data in fields like high-dimensional statistics presents ongoing challenges, propelling research and innovation in statistical methodologies."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#shrinkage-methods-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#shrinkage-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\n\nThe subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\nAs an alternative, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#ridge-regression-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#ridge-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\nRecall that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the values that minimize\n\\[\n    \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2.\n\\]\n\nIn contrast, the ridge regression coefficient estimates \\(\\hat{\\beta}^R\\) are the values that minimize\n\n\\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n    = \\text{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\]\nwhere \\(\\lambda \\geq 0\\) is a tuning parameter, to be determined separately.\n\nShrinkage penalty: \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\) penalizes coefficients that get too large. The model will pay a price according to the number of non-zero coefficients."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#ridge-regression-2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#ridge-regression-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nAs with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\nHowever, the second term, \\(\\lambda \\sum_j \\beta_j^2\\), called a shrinkage penalty, is small when \\(\\beta_1, \\ldots, \\beta_p\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nThe tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates.\nSelecting a good value for \\(\\lambda\\) is critical; cross-validation is used for this."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example-2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n\n\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\). As \\(\\lambda\\) increases, it pushes the coefficients towards zero.\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\).\nIn the right-hand panel, when \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2 = 1\\), \\(\\lambda = 0\\)."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#ridge-regression-scaling-of-predictors",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#ridge-regression-scaling-of-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge Regression: Scaling of Predictors",
    "text": "Ridge Regression: Scaling of Predictors\n\nThe standard least squares coefficient estimates are scale equivariant: multiplying \\(X_j\\) by a constant \\(c\\) simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1/c\\). In other words, regardless of how the \\(j\\)th predictor is scaled, \\(X_j \\hat{\\beta}_j\\) will remain the same.\nIn contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.\nTherefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n\n\n\\[\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}}\n\\]\n\n\nAfter standardizing the predictors, their standard deviations will be 1. That make the features comparable and make the coefficients comparable."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Does Ridge Regression Improve Over Least Squares?",
    "text": "Why Does Ridge Regression Improve Over Least Squares?\nThe Bias-Variance Tradeoff\n\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients.\nSquared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of \\(\\lambda\\) and \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\).\nThe horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-shrinkage-methods",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-shrinkage-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Shrinkage methods",
    "text": "Summary: Shrinkage methods\n\nShrinkage methods like Ridge regression and Lasso use penalties to shrink coefficients towards zero, improving model performance, especially with large datasets.\n\n\nHighlights\n\nRidge regression uses a penalty to shrink coefficients towards zero.\nLasso also shrinks coefficients but can set some to exactly zero.\nThese methods are effective for large datasets with many variables.\nFast computation has revived interest in these techniques recently.\nCross-validation is crucial for selecting the optimal tuning parameter, Lambda.\nScaling of variables is important when applying Ridge regression.\nRidge regression reduces variance while maintaining bias, leading to better mean squared error.\n\n\nKey Insights\n\nShrinkage Techniques: Ridge regression and Lasso are modern approaches to regularization, balancing model fit and complexity. Shrinking coefficients helps mitigate overfitting, particularly in high-dimensional data.\nTuning Parameter Lambda: The choice of Lambda is critical; it determines the strength of the penalty. Using cross-validation to optimize this parameter is essential for achieving the best model performance.\nBias-Variance Tradeoff: Ridge regression effectively controls variance without significantly increasing bias, thereby minimizing mean squared error. This tradeoff is vital for model accuracy.\nLarge Datasets: As datasets grow in size and complexity, shrinkage methods become increasingly relevant. They are designed to handle situations where the number of predictors can exceed the number of observations.\nImportance of Scaling: Unlike least squares, the performance of Ridge regression is sensitive to the scale of the predictors. Standardizing variables ensures comparability and effectiveness of the shrinkage.\nContinuous Shrinkage: Ridge regression produces coefficients that are close to zero but rarely exactly zero, which differs from Lasso. This characteristic can be advantageous for retaining all predictors in the model.\nCurrent Research Trends: Shrinkage methods are a hot topic in statistical research, with ongoing developments aimed at enhancing their effectiveness and applicability across various fields."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#the-lasso-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#the-lasso-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all \\(p\\) predictors in the final model.\nThe Lasso, first published in 1996 by Rob Tibshirani, one of the authors of the book, is an alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}^L_\\lambda\\), minimize the quantity\n\n\\[\n  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]\n\nIn statistical parlance, the lasso uses the sum of absolute values, an \\(\\ell_1\\) (pronounced “ell 1”) penalty, instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(\\|\\beta\\|_1 = \\sum |\\beta_j|\\)."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#the-lasso-2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#the-lasso-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nHowever, in the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nHence, much like best subset selection, the lasso performs variable selection. It is a combination of shirinkage and selection of variables.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#example-credit-dataset",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#example-credit-dataset",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Dataset",
    "text": "Example: Credit Dataset"
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#the-variable-selection-property-of-the-lasso",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#the-variable-selection-property-of-the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Variable Selection Property of the Lasso",
    "text": "The Variable Selection Property of the Lasso\nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?\nOne can show that the lasso and ridge regression coefficient estimates solve the problems (equivalent to Lagrange formulations):\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} |\\beta_j| \\leq s\n\\]\nand\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} \\beta_j^2 \\leq s,\n\\]\nrespectively."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#the-lasso-and-ridge-picture",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#the-lasso-and-ridge-picture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso and Ridge Picture",
    "text": "The Lasso and Ridge Picture\nThis picture helps to explain why the lasso gives sparsity:\n\n\nOn the right we have the ridge regression and on the left is the lasso regression.\nIt is possible to see where the where the red boundary touch the blue constraing.\nIn the case of the ridge regression (right plot), we see that the solution does not create a zero.\nIn the case of the lasso regression (left plot), we see that the solution does create a zero for one of the predictors."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression",
    "text": "Comparing the Lasso and Ridge Regression\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients.\nLeft: Lasso: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression: continued",
    "text": "Comparing the Lasso and Ridge Regression: continued\n\nLeft: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data equals to the one used before, except that now only two predictors are related to the response.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#conclusions-about-ridge-and-lasso",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#conclusions-about-ridge-and-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions about Ridge and Lasso",
    "text": "Conclusions about Ridge and Lasso\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-lasso",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Lasso",
    "text": "Summary: Lasso\n\nThe Lasso regression technique improves upon ridge regression by both shrinking coefficients and performing variable selection, setting some coefficients to zero.\n\n\nHighlights\n\nLasso regression shrinks coefficients while allowing for variable selection.\nIt uses an \\(L_1\\) penalty, contrasting with ridge’s \\(L_2\\) penalty.\nThe concept of sparsity is central to Lasso’s effectiveness.\nIncreased computational efficiency has popularized Lasso in recent years.\nLasso is particularly useful in high-dimensional datasets with many features.\nCross-validation is essential for selecting the optimal lambda value.\nPerformance varies: Lasso excels in sparse models, while ridge may perform better in dense ones.\n\n\nKey Insights\n\nLasso vs. Ridge: Lasso regression not only shrinks coefficients but also sets some to zero, enabling simpler models through variable selection. This property makes it particularly valuable in high-dimensional settings where many variables may be irrelevant.\n\\(L_1\\) vs. \\(L_2\\) Penalty: The \\(L_1\\) penalty used in Lasso creates a constraint that promotes sparsity, while the \\(L_2\\) penalty in ridge regression tends to retain all variables with smaller coefficients. This difference is crucial for effective model building.\nSparsity: The concept of sparsity refers to models that only include a small subset of variables. Sparse models are easier to interpret and can enhance predictive performance when only a few predictors are relevant.\nComputational Advances: Recent improvements in computational power and techniques in convex optimization have made applying Lasso feasible even on large datasets, broadening its applicability across various fields.\nReal-World Applications: In situations like medical diagnostics, where finding a minimal number of significant predictors is vital, Lasso provides a practical solution by efficiently identifying key variables among thousands of measurements.\nChoosing Lambda: The tuning parameter lambda is critical; cross-validation is typically used to determine its optimal value, balancing model complexity and predictive accuracy.\nModel Performance: The effectiveness of Lasso and ridge regression varies based on the underlying data structure. Lasso performs better with sparse true models, while ridge regression may be more effective when many predictors are significant."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Selecting the Tuning Parameter for Ridge Regression and Lasso",
    "text": "Selecting the Tuning Parameter for Ridge Regression and Lasso\n\nAs for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.\nThat is, we require a method selecting a value for the tuning parameter \\(\\lambda\\) or equivalently, the value of the constraint \\(s\\).\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the cross-validation error rate for each value of \\(\\lambda\\).\nWe then select the tuning parameter value for which the cross-validation error is smallest.\nFinally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example-3",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#credit-data-example-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\nLeft: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of \\(\\lambda\\). \\(\\lambda = 0.05\\) minimizes the cross-validation error.\nRight: The coefficient estimates as a function of \\(\\lambda\\). The vertical dashed line indicates the value of \\(\\lambda\\) selected by cross-validation."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#simulated-data-example",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#simulated-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Data Example",
    "text": "Simulated Data Example\n\nLeft: Ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set.\nRight: The corresponding lasso coeﬃcient estimates are displayed. The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-selecting-the-tuning-parameter-lambda",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-selecting-the-tuning-parameter-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Selecting the tuning parameter (lambda)",
    "text": "Summary: Selecting the tuning parameter (lambda)\n\nSelecting the tuning parameter (lambda) for ridge regression and lasso is crucial, as it significantly influences model performance. Cross-validation is an effective method for this selection.\n\n\nHighlights\n\nLambda is crucial: Affects the solution from full least squares to zero coefficients.\n⚖️ Regularization importance: Zero lambda means no regularization; high lambda leads to zero solutions.\nCross-validation advantage: Ideal for tuning parameters as it doesn’t require the unknown number of parameters (\\(d\\)).\nRidge example: With lambda of 100, all variables appear included, but coefficients are shrunk.\nCross-validation curves: Show how errors change with varying lambda values for both ridge and lasso.\nLasso effectiveness: Properly identifies non-zero coefficients while setting others to zero.\nSimulation success: In a simulated scenario, the model accurately identifies the correct number of non-zero coefficients.\n\n\nKey Insights\n\nImportance of Lambda: The tuning parameter lambda significantly influences the model’s complexity and overall performance. Choosing lambda wisely is essential for achieving the desired balance between bias and variance.\nCross-validation as a solution: Cross-validation provides a robust framework for assessing model performance across different lambda values without needing to know the exact number of parameters, making it a practical choice for tuning.\nDegree of freedom confusion: In ridge regression, even when coefficients are shrunk, counting parameters can be misleading, as all variables remain included in the model.\nRegularization trade-offs: The process of regularization through ridge and lasso not only simplifies models but also introduces nuanced definitions of model complexity, changing our understanding of ‘degrees of freedom.’\nError analysis via curves: Cross-validation curves reveal how model errors fluctuate with lambda, helping visualize optimal tuning points.\nLasso’s precision: Lasso regression demonstrates its strength in feature selection, effectively pinpointing relevant variables while ignoring the irrelevant ones, enhancing interpretability."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#dimension-reduction-methods-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#dimension-reduction-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nThe methods that we have discussed so far have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1, X_2, \\ldots, X_p\\).\nWe now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#dimension-reduction-methods-details",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#dimension-reduction-methods-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Details",
    "text": "Dimension Reduction Methods: Details\n\n\nLet \\(Z_1, Z_2, \\ldots, Z_M\\) represent \\(M &lt; p\\) linear combinations of our original \\(p\\) predictors. That is,\n\n\\[\n    Z_m = \\sum_{j=1}^p \\phi_{mj} X_j \\quad \\text{(1)}\n\\]\nfor some constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\).\n\nWe can then fit the linear regression model,\n\n\\[\n    y_i = \\theta_0 + \\sum_{m=1}^M \\theta_m z_{im} + \\epsilon_i, \\quad i = 1, \\ldots, n, \\quad \\text{(2)}\n\\]\nusing ordinary least squares.\n\nNote that in model (2), the regression coefficients are given by \\(\\theta_0, \\theta_1, \\ldots, \\theta_M\\). If the constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\) are chosen wisely, then such dimension reduction approaches can often outperform OLS regression."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#dimension-reduction-methods-2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#dimension-reduction-methods-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\n\nNotice that from definition (1),\n\n\\[\n    \\sum_{m=1}^M \\theta_m z_{im} = \\sum_{m=1}^M \\theta_m \\sum_{j=1}^p \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\sum_{m=1}^M \\theta_m \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where\n\\[\n    \\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{mj}. \\quad \\text{(3)}\n\\]\n\nHence model (2) can be thought of as a special case of the original linear regression model.\nDimension reduction serves to constrain the estimated \\(\\beta_j\\) coefficients, since now they must take the form (3).\nCan win in the bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-dimension-reduction",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-dimension-reduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Dimension reduction",
    "text": "Summary: Dimension reduction\n\nDimension reduction transforms original predictors into fewer linear combinations, improving model fitting while maintaining low bias and variance.\n\n\nHighlights\n\nDimension Reduction simplifies models by using fewer predictors.\nNew predictors are linear combinations of original ones.\nFitting uses least squares on transformed predictors.\nAim: Reduce dimensions from \\(P\\) predictors to \\(M\\) (\\(M &lt; P\\)).\nBalances bias and variance effectively.\nSimilar to Ridge and Lasso, but with different coefficient constraints.\nWorks best when \\(M &lt; P\\); otherwise, it results in standard least squares.\n\n\nKey Insights\n\nEfficiency in Modeling: Dimension reduction allows for a simpler model with fewer predictors, leading to potentially better performance without losing significant information. This method is advantageous in high-dimensional datasets.\nConstruction of New Predictors: By creating new predictors through linear combinations, we can capture essential relationships in the data while reducing complexity, which may help in enhancing interpretability.\nBias-Variance Trade-off: This approach effectively manages the bias-variance trade-off, leading to models with lower bias and variance compared to using all original features, which is crucial for better generalization to unseen data.\nUse of Least Squares: While retaining the least squares fitting method, this approach modifies the predictor space, allowing for a fresh perspective on regression problems and leading to potentially improved outcomes.\nRelation to Ridge and Lasso: Although dimension reduction shares similarities with Ridge and Lasso in terms of model fitting, it introduces unique constraints on coefficients, which can lead to different insights about the data.\nImportance of Dimensions: The effectiveness of dimension reduction hinges on the condition that \\(M\\) (new predictors) is less than \\(P\\) (original predictors). If \\(M = P\\), the method reduces to standard least squares, negating its advantages.\nInnovation in Coefficient Form: The requirement for coefficients to adopt a specific structure in dimension reduction can provide insights into the relationships among predictors, enhancing model interpretability and utility."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#principal-components-regression-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#principal-components-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nBy far the most famous dimension reduction approach. It involves a two-step procedure:\n\nStep 1: we find what are called principal components of the data matrix (linear combinations of the predictors)\nStep 2: we perform least squares regression using those principal components as predictors\n\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has the largest variance, subject to being uncorrelated with the first.\nAnd so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation.\nThe intuition is that if you have a data set with 45 variables and compute a few principal components, those might capture most of the variation in the data."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\nThe population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles. The green solid line indicates the first principal component, and the blue dashed line indicates the second principal component.\nNote that these two principal components are uncorrelated!"
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\n\n\n\n\n\n\n\n\n\nA subset of the advertising data.\n\nLeft: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments.\nRight: The left-hand panel has been rotated so that the first principal component lies on the x-axis."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca-2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\n\nPlots of the first principal component scores \\(z_{i1}\\) versus pop and ad. The relationships are strong.\n\nWe can visualize each principal component by plotting it against the original variables, such as population and ad spending.\nWe observe that the first principal component is highly correlated with both population and ad spending. This indicates that the first principal component effectively captures the variability in these two variables, summarizing the data in a meaningful way.\nThis suggests a valuable insight: instead of using the original variables (population and ad spending) directly, we can use the first principal component as a single, simplified predictor. We have the assumption that a linear combination of the predictors that has high variance is probably going to be associated with the response.\n\nFor example, if my goal is to predict a response variable like sales, we can incorporate the first principal component as a predictor in the model. This approach reduces the dimensionality of the data while retaining much of the information, potentially improving model interpretability and efficiency."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca-3",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#pictures-of-pca-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\nPlots of the second principal component scores \\(z_{i2}\\) versus pop and ad. The relationships are weak."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#application-to-principal-components-regression",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#application-to-principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application to Principal Components Regression",
    "text": "Application to Principal Components Regression\n\nPCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively.\n\nLeft: Simulated data with \\(n= 50\\) observations, \\(p= 45\\) predictors. The plot shows that a model with \\(\\approx 18\\) principal components can provide a good result.\nRight: Simulated data with \\(n= 50\\) observations, \\(p= 45\\) predictors, except that now only two predictors are related to the response. The plot shows that a model with \\(\\approx 25\\) principal components can provide a good result."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#choosing-the-number-of-principal-component-directions-m",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#choosing-the-number-of-principal-component-directions-m",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Number of Principal Component Directions \\(M\\)",
    "text": "Choosing the Number of Principal Component Directions \\(M\\)\n\nLeft: PCR standardized coefficient estimates on the Credit data set for different values of \\(M\\).\nRight: The 10-fold cross-validation MSE obtained using PCR, as a function of \\(M\\). For each of the models we can see the cross-validated mean squared error. Here we have disappointing result. If we pick a model for which the mean squared error is as small as possible, here the mean squared error is really as small as possible when we have a model with 10 or 11 components. However, in our dataset \\(M = 11\\) is going to be the regular least squares on the original data using all variables. Basically, principal components regression does not provide any gains in this case."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#partial-least-squares-pls-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#partial-least-squares-pls-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1, \\dots, X_p\\).\nThese directions are identified in an unsupervised way, since the response \\(Y\\) is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.\nA potential solution is to use Partial Least Squares (PLS)."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#partial-least-squares-pls-2",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#partial-least-squares-pls-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1, \\dots, Z_M\\) that are linear combinations of the original features, and then fits a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way – that is, it makes use of the response \\(Y\\) in order to identify new features that not only approximate the old features well, but also that are related to the response.\nPLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#partial-least-squares-pls-details",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#partial-least-squares-pls-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS): Details",
    "text": "Partial Least Squares (PLS): Details\n\nAfter standardizing the \\(p\\) predictors, PLS computes the first direction \\(Z_1\\) by setting each \\(\\phi_{1j}\\) in (1) equal to the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\).\nOne can show that this coefficient is proportional to the correlation between \\(Y\\) and \\(X_j\\).\nHence, in computing \\(Z_1 = \\sum_{j=1}^p \\phi_{1j} X_j\\), PLS places the highest weight on the variables that are most strongly related to the response.\nSubsequent directions are found by taking residuals and then repeating the above prescription.\nThe authors of the book highlight that PLS does not bring to much gain when compared to Ridge regression approach, for example."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-principal-components-regression-pcr",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-principal-components-regression-pcr",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Principal Components Regression (PCR)",
    "text": "Summary: Principal Components Regression (PCR)\n\nPrincipal Components Regression (PCR) reduces dimensionality by finding principal components and using them in least squares regression for efficient modeling.\n\n\nHighlights\n\nPrincipal Components Regression (PCR) uses a two-step procedure to reduce dimensionality.\nFirst, principal components with the highest variance are identified from the data.\nThe first principal component is aligned with the direction of maximum variance.\nThe second principal component is uncorrelated with the first and captures additional variance.\nUsing few principal components can effectively summarize complex datasets.\nChoosing the optimal number of components is crucial for minimizing mean squared error.\nPartial Least Squares (PLS) improves upon PCR by considering response variables in component selection.\n\n\nKey Insights\n\nDimensionality Reduction: PCR simplifies models by reducing the number of predictors while retaining essential information, aiding in interpretation and computation. This is particularly useful with datasets containing many variables relative to observations.\nUncorrelated Components: The process ensures that the principal components are uncorrelated, which helps in creating more robust models by minimizing multicollinearity issues common in regression analysis.\nModel Selection: The selection of the number of components directly impacts model performance. Cross-validation is recommended to find the optimal number of components for the best predictive accuracy.\nEfficiency in Prediction: PCR can significantly enhance prediction accuracy when dealing with high-dimensional data by focusing on variance rather than individual variable contributions.\nAssumption of Variance-Response Relationship: The effectiveness of PCR hinges on the assumption that high variance directions in predictors correlate with the response, which may not always hold true.\nPartial Least Squares: PLS offers a supervised alternative to PCR by incorporating response variable information, potentially leading to better predictive models, although it may not always outperform PCR.\nModern Applications: Techniques like PCR and PLS are increasingly relevant in fields with large datasets, where simpler models are needed to prevent overfitting and enhance interpretability."
  },
  {
    "objectID": "lecture_slides/05_model_selection/05_model_selection.html#summary-1",
    "href": "lecture_slides/05_model_selection/05_model_selection.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\nKey Concepts\n\nThree classes of methods:\n\nSubset Selection: Focuses on identifying subsets of predictors for simpler models.\nShrinkage Methods: Regularizes coefficients to reduce variance and improve model performance (e.g., Ridge, Lasso).\nDimension Reduction: Reduces the number of predictors using linear combinations (e.g., PCA, PLS).\n\nModel Selection Criteria:\n\nUse metrics like \\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)to balance model fit and complexity.\nCross-validation is essential for estimating test error and selecting tuning parameters.\n\nBias-Variance Tradeoff:\n\nShrinkage methods improve prediction accuracy by reducing variance while maintaining bias.\n\n\n\nPractical Insights\n\nBest Subset Selection:\n\nComputationally intensive (\\(2^p\\) models).\nNot recommended for \\(p &gt; 20\\).\n\nStepwise Selection:\n\nMore efficient (\\(p^2\\) models).\nForward and backward methods balance performance and computation.\n\nRidge vs. Lasso:\n\nRidge shrinks coefficients but includes all predictors.\nLasso performs variable selection by setting coefficients to zero.\n\nPrincipal Components Regression (PCR):\n\nReduces dimensionality by finding uncorrelated components.\nWorks well when high variance directions correlate with the response.\n\nPartial Least Squares (PLS):\n\nSupervised alternative to PCR, incorporating response variable information."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#overview",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nMoving Beyond Linearity\nPolynomial Regression\nStep Functions\nRegression Splines\n\n\n\nSmoothing Splines\nLocal Regression\nGeneralized Additive Models\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#moving-beyond-linearity-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#moving-beyond-linearity-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Moving Beyond Linearity",
    "text": "Moving Beyond Linearity\n\nThe truth is never linear!\nOr almost never!\nBut often the linearity assumption is good enough.\nWhen it’s not…\n\npolynomials,\n\nstep functions,\n\nsplines,\n\nlocal regression, and\n\ngeneralized additive models\n\noffer a lot of flexibility, without losing the ease and interpretability of linear models."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-wage-data",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-wage-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Wage Data",
    "text": "Polynomial Regression: Wage Data\n\nPolynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. It provides a simple way to provide a non-linear fit to data.\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nLeft Plot: We fit a fourth-degree polynomial model to predict Wage using Age as the predictor. The data become sparse toward the higher end of Age, so there is relatively little information to guide the model’s fit in that region. As a result, the standard errors increase toward the tail—a phenomenon often referred to as “leverage.”\n\n\n\nRight Plot: We fit a fourth-degree polynomial model for a logistic regression. Similar to the left plot, the data diminish at the tail end of the predictor range, leaving fewer observations to inform the fit and leading to wider confidence intervals in that region."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Details",
    "text": "Polynomial Regression: Details\n\n\nCreate new variables \\(X_1 = X, \\, X_2 = X^2\\), etc., and then treat as multiple linear regression.\nIt is linear in the coefficients, but it is a non linear function of \\(x\\).\nNot really interested in the coefficients; more interested in the fitted function values at any value \\(x_0\\):\n\n\n\\[\n    \\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 + \\hat{\\beta}_2 x_0^2 + \\hat{\\beta}_3 x_0^3 + \\hat{\\beta}_4 x_0^4.\n\\]\n\n\nSince \\(\\hat{f}(x_0)\\) is a linear function of the \\(\\hat{\\beta}_\\ell\\), can get a simple expression for pointwise-variances (\\(\\text{Var}[\\hat{f}(x_0)]\\)) at any value \\(x_0\\).\n\nIn the figure, we have computed the fit and pointwise standard errors on a grid of values for \\(x_0\\). We show \\(\\hat{f}(x_0) \\pm 2 \\cdot \\text{se}[\\hat{f}(x_0)]\\).\n\nHow to choose \\(d\\), the polinomial degree? We either fix the degree \\(d\\) at some reasonably low value or use cross-validation to choose \\(d\\)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#polynomial-regression-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Details",
    "text": "Polynomial Regression: Details\n\n\nLogistic regression follows naturally. For example, in the figure we model\n\n\n\\[\n    \\text{Pr}(y_i &gt; 250 \\mid x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}{1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}.\n\\]\n\n\nTo get confidence intervals, compute upper and lower bounds on on the logit scale, and then invert to get on the probability scale.\nCan do separately on several variables—just stack the variables into one matrix, and separate out the pieces afterwards (see GAMs later).\nCaveat: Polynomials have notorious tail behavior — very bad for extrapolation. So, it is not recommended to trust predictions near the end of the data."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\n\nStep functions cut the range of a variable into \\(K\\) distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.\n\\[\nC_1(X) = I(X &lt; 35), \\quad C_2(X) = I(35 \\leq X &lt; 50), \\dots, C_3(X) = I(X \\geq 65)\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-2",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\n\nA step function model partitions the range of a predictor variable \\(X\\) into multiple intervals and creates a set of dummy (0/1) indicators—one for each interval. By fitting a standard linear model with these dummy variables, the resulting function is piecewise constant: within each interval, the fitted value remains the same.\n\nLocal (Step Functions): Because each interval in a step function acts independently, changes in one interval have minimal or no effect on the fitted values in other intervals. Consequently, step functions allow a locally controlled fit, where data in a specific region of \\(X\\) only affect the parameters corresponding to that interval.\nGlobal (Polynomials): In contrast, polynomial models rely on parameters that apply across the entire range of \\(X\\). Consequently, if you alter a single data point or a small set of points in one region, those changes can influence the fitted function everywhere else. This global dependence can lead to dramatic shifts in the estimated curve.\nSimplicity: Step functions are conceptually straightforward and easy to implement, requiring only the definition of intervals and fitting dummy variables.\nLocal Control: Their piecewise nature can be beneficial when the true relationship changes abruptly or when you want to minimize the effect of outliers in one region on the fit elsewhere.\nBut there are some Drawbacks:\n\nBlocky Appearance: The fitted function may appear abrupt or “blocky,” as it is piecewise constant rather than smooth.\n\nBoundary Sensitivity: Choosing the number and location of breakpoints is somewhat subjective and can significantly impact the model’s fit.\n\nOverall, step functions provide an intuitive, locally controlled alternative to global polynomial models. However, the abrupt transitions and the need to specify breakpoints can limit their practical appeal—particularly for applications where a smooth or continuous functional form is desired."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-3",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#step-functions-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step functions",
    "text": "Step functions\n\nEasy to work with. Creates a series of dummy variables representing each group.\nUseful way of creating interactions that are easy to interpret. For example, interaction effect of \\(\\text{Year}\\) and \\(\\text{Age}\\):\n\n\n\\[\n    I(\\text{Year} &lt; 2005) \\cdot \\text{Age}, \\quad I(\\text{Year} \\geq 2005) \\cdot \\text{Age}\n\\]\nwould allow for different linear functions in each age category.\n\n\nChoice of cutpoints or knots can be problematic. For creating nonlinearities, smoother alternatives such as splines are available."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nWhile linear models are simple and often effective, real-world data frequently exhibit non-linear relationships, necessitating more flexible modeling approaches.\n\nNon-Linearity is Common: Most real-world data are not well-represented by simple linear trends, underscoring the importance of non-linear modeling techniques.\n\nPolynomial Regression: Extends linear models by including polynomial terms, enabling the capture of curved relationships. This method offers flexibility but may exhibit instability, especially at the tails, where standard error bands tend to widen due to sparse data.\n\nStep Functions: Provide an alternative approach by dividing continuous variables into discrete intervals, resulting in piecewise constant fits. This makes them particularly useful when natural breakpoints exist in the data.\n\nInteractions with Dummy Variables: Step functions are intuitive to work with and allow for straightforward inclusion of interaction effects through the creation of dummy variables.\n\nChallenges of Step Functions: The performance of step functions heavily depends on the selection of cut points. Poorly chosen boundaries can obscure important patterns or lead to overfitting, highlighting the need for domain expertise when applying this method.\n\nTakeaway: Both polynomial regression and step functions are valuable tools for modeling non-linear relationships. However, their effectiveness depends on thoughtful implementation, particularly in managing standard errors and selecting appropriate cut points."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#regression-splines-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#regression-splines-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nRegression splines are an extension and more flexible than polynomials and step functions.\nThey involve dividing the range of \\(X\\) into \\(K\\) distinct regions. Within each region, a polynomial function is fit to the data.\nHowever, these polynomials are constrained so that they join smoothly at the region boundaries, or knots. Provided that the interval is divided into enough regions, this can produce an extremely flexible fit."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#piecewise-polynomials",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#piecewise-polynomials",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Piecewise Polynomials",
    "text": "Piecewise Polynomials\n\nInstead of a single polynomial in \\(X\\) over its whole domain, we can rather use different polynomials in regions defined by knots.\n\n\n\\[\n    y_i =\n    \\begin{cases}\n    \\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i^2 + \\beta_{31}x_i^3 + \\epsilon_i & \\text{if } x_i &lt; c; \\\\\n    \\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_i^3 + \\epsilon_i & \\text{if } x_i \\geq c.\n    \\end{cases}\n\\]\n\n\nBetter to add constraints to the polynomials, e.g., continuity.\nSplines have the “maximum” amount of continuity."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#splines-visualization",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Splines Visualization",
    "text": "Splines Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop-Left Panel: A third-degree polynomial is fitted to the data on the left side of the knot at \\(X = 50\\), and another (separate) third-degree polynomial is fitted on the right side. There is no continuity constraint imposed at the knot, meaning the two polynomials may not meet at the same function value at \\(X = 50\\).\nTop-Right Panel: Again, a third-degree polynomial is fitted on each side of \\(X = 50\\). However, in this case, the polynomials are forced to be continuous at the knot. In other words, they must share the same function value at \\(X = 50\\).\nBottom-Left Panel: As in the top-right panel, a third-degree polynomial is fitted on each side of \\(X = 50\\), but with an additional constraint that enforces continuity of the first and second derivatives at \\(X = 50\\). This ensures a smoother transition between the left and right segments of the piecewise function.\nBottom-Right Panel: A linear regression model is fitted on each side of \\(X = 50\\). The model is constrained to be continuous at the knot, so both linear segments meet at the same value at \\(X = 50\\)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines",
    "text": "Linear Splines\n\nThe predictor space is partitioned at a set of specified points called knots \\(\\{\\xi_k\\}\\). A linear spline is a piecewise linear polynomial that remains continuous at each knot.\nWe construct linear spline models by augmenting a linear predictor with piecewise components that activate past designated knot locations, yielding a flexible yet interpretable approach to modeling relationships. Specifically:\n\n\n\nBasis-Function Representation: The model is written as\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+1} b_{K+1}(x_i) + \\epsilon_i,\n\\]\nwhere each \\(b_k(\\cdot)\\) is a basis function.\nOne basis function, \\(b_1(x_i)\\), is simply \\(x_i\\). The others, \\(b_{k+1}(x_i) = (x_i - \\xi_k)_+\\), capture local deviations after each knot \\(\\xi_k\\).\nThe notation \\((\\cdot)_+\\) denotes the “positive part,” meaning \\(\\max\\{0, \\cdot\\}\\). Therefore, \\((x_i - \\xi_k)_+ = x_i - \\xi_k\\) if \\(x_i &gt; \\xi_k\\), and 0 otherwise.\n\n\nPiecewise Linear Behavior: Because \\(b_{k+1}(x_i)\\) only becomes non-zero when \\(x_i\\) exceeds the knot \\(\\xi_k\\), the spline behaves like a linear function with additional slopes kicking in after each knot.\n\n\nEssentially, below the smallest knot, the model is a simple linear function of \\(x_i\\). Once \\(x_i\\) passes a knot \\(\\xi_k\\), the corresponding term \\((x_i - \\xi_k)_+\\) begins to contribute, allowing the slope to change. This creates segments of potentially different slopes while maintaining continuity at the knots.\n\n\n\nContinuity at Knots: Despite having distinct linear segments, the spline remains continuous at each \\(\\xi_k\\). The continuity follows naturally from how \\((\\cdot)_+\\) is defined. At a knot, \\((x_i - \\xi_k)_+\\) transitions from 0 to a linear increase, ensuring no jumps in the fitted function.\nRelevance\n\n\nFlexibility: Linear splines allow for piecewise changes in slope rather than forcing a single global linear relationship. This can capture more nuanced relationships between predictors and responses.\nInterpretability: Each knot \\(\\xi_k\\) marks a point where the slope can adjust, making it straightforward to interpret how the effect of \\(x_i\\) differs below and above that knot.\nComparison to Polynomials: Unlike higher-order polynomials, splines can avoid the global distortion that arises from polynomial terms. A single outlier or a data pattern in one region does not overly influence the fit across the entire range of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#lienar-splines-example",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#lienar-splines-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Lienar Splines: Example",
    "text": "Lienar Splines: Example\n\nTo illustrate a linear spline with one knot at \\(x = 50\\), suppose we have a response variable \\(y\\) (e.g., a person’s yearly wage) and a single predictor \\(x\\) (e.g., age):\n\n\n\nDefine the Basis Functions: We choose to place a single knot at \\(x = 50\\). According to the slide’s notation, the basis functions are:\n\n\n\\[\n   b_1(x_i) = x_i, \\quad\n   b_2(x_i) = (x_i - 50)_{+} \\,=\\,\n   \\begin{cases}\n     x_i - 50, & \\text{if } x_i &gt; 50,\\\\\n     0, & \\text{if } x_i \\le 50.\n   \\end{cases}\n\\]\n\n\nSpecify the Model: The corresponding linear spline model is:\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 \\, b_1(x_i) + \\beta_2 \\, b_2(x_i) + \\epsilon_i,\n\\]\n\n\nor more explicitly:\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 \\, x_i + \\beta_2 \\, (x_i - 50)_{+} + \\epsilon_i.\n\\]\n\n\n\n3.For \\(x_i \\le 50\\):\n\n\n\\((x_i - 50)_+ = 0\\). Hence, the model reduces to\n\n\n\\[\n     y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\n\nwhich is a simple linear relationship with slope \\(\\beta_1\\) for ages up to 50.\n\n\n4.For \\(x_i &gt; 50\\):\n\n\n\\((x_i - 50)_+ = x_i - 50\\). Thus, the model becomes\n\n\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - 50) + \\epsilon_i \\\\\n    &= [\\beta_0 - 50 \\beta_2] + (\\beta_1 + \\beta_2) x_i + \\epsilon_i\n\\end{aligned}\n\\]\n\n\nmeaning the slope for ages beyond 50 is \\(\\beta_1 + \\beta_2\\). The intercept adjusts accordingly to ensure the function remains continuous at \\(x=50\\).\n\n\n\n\nLocal Flexibility: Below 50, the effect of age on wage is governed by \\(\\beta_1\\). Above 50, the slope can change to \\(\\beta_1 + \\beta_2\\).\nContinuity at 50: Because the spline is forced to match up at \\(x = 50\\), there is no abrupt jump in the fitted curve.\nSimplicity of Implementation: We only introduced one additional term \\((x_i - 50)_+\\) to capture the potential change in slope after age 50."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines-visualization",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#linear-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines Visualization",
    "text": "Linear Splines Visualization\n\n\n\n\n\n\n\n\n\n\n\nTop plot: shows two linear fits over the domain \\(0 \\le x \\le 1\\). The blue line represents a single global linear function (extending as the dashed line beyond the knot at \\(x = 0.6\\)), whereas the orange line demonstrates how adding a spline basis function allows the slope to change precisely at \\(x = 0.6\\).\nBottom plot: displays the corresponding basis function \\(b(x) = (x - 0.6)_{+}\\), which is defined to be zero for \\(x \\le 0.6\\) and increases linearly for \\(x &gt; 0.6\\). Because \\(b(x)\\) starts at zero at the knot, it does not introduce a jump—thus ensuring continuity—but it permits the slope to differ on either side of \\(x = 0.6\\).\n\nBy including this basis function (and a coefficient for it) in a linear model, one can capture a “bend” or change in slope at the specified knot. More generally, introducing additional such functions at different knots yields a piecewise linear model that remains continuous but adapts its slope in each region."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nA cubic spline with knots at \\(\\xi_k, \\, k = 1, \\dots, K\\) is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.\nAgain we can represent this model with truncated power basis functions:\n\\[\ny_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+3} b_{K+3}(x_i) + \\epsilon_i,\n\\]\n\\(b_1(x_i) = x_i,\\)\n\\(b_2(x_i) = x_i^2,\\)\n\\(b_3(x_i) = x_i^3,\\)\n\\(b_{k+3}(x_i) = (x_i - \\xi_k)_+^3, \\quad k = 1, \\dots, K\\)\n\nwhere\n\\[\n(x_i - \\xi_k)_+^3 =\n\\begin{cases}\n(x_i - \\xi_k)^3 & \\text{if } x_i &gt; \\xi_k, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines-visualization",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#cubic-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines Visualization",
    "text": "Cubic Splines Visualization"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-splines",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\nA natural cubic spline extrapolates linearly beyond the boundary knots. This adds \\(4 = 2 \\times 2\\) extra constraints, and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-spline",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#natural-cubic-spline",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Natural Cubic Spline",
    "text": "Natural Cubic Spline"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#knot-placement",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#knot-placement",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Knot Placement",
    "text": "Knot Placement\n\n\nOne strategy is to decide \\(K\\), the number of knots, and then place them at appropriate quantiles of the observed \\(X\\).\nA cubic spline with \\(K\\) knots has \\(K + 4\\) parameters or degrees of freedom.\nA natural spline with \\(K\\) knots has \\(K\\) degrees of freedom.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure: Comparison of a degree-14 polynomial and a natural cubic spline, each with 15df."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nSplines and piecewise polynomials build on the concept of dividing the predictor domain into segments, offering enhanced flexibility and precision over traditional polynomial models. By using different polynomial functions in various regions and ensuring smooth transitions at knots, these methods provide a powerful approach to capturing non-linear relationships in data.\n\nPiecewise Polynomials: Replace a single global polynomial with multiple polynomials fitted across segments, resulting in a more tailored and adaptable fit.\n\nContinuity Constraints: Enforcing continuity at knots ensures smooth transitions between polynomial segments, avoiding abrupt changes and better representing underlying trends.\n\nNatural Cubic Splines: Add further constraints by requiring smoothness in the function’s second derivative at knots and controlling boundary behavior, ensuring robustness and reducing overfitting at extremes.\n\nFlexibility and Local Adaptation: Piecewise polynomials adapt to local data behavior, making them particularly effective for capturing non-linear trends and abrupt changes in data. This adaptability is essential in real-world datasets that exhibit complex patterns.\n\nStrategic Knot Placement: Properly placing knots, such as at quantiles of the predictor variable, ensures that each segment has adequate data coverage, resulting in more reliable and stable estimates.\n\nImportance of Boundary Constraints: Natural cubic splines excel at controlling the function’s behavior near boundaries, mitigating overfitting and improving interpretability in regions with sparse data.\n\nAdvantages Over Traditional Polynomials: Splines overcome the limitations of global polynomial fitting, such as excessive wiggliness or global influence of outliers, providing smoother, more interpretable fits that align closely with the data’s natural structure.\nConclusion: Splines offer a superior method for non-linear modeling, combining flexibility, local adaptation, and smoothness, making them an indispensable tool in modern statistical analysis."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-adding-mathematical-rigor",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-adding-mathematical-rigor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: Adding Mathematical Rigor",
    "text": "Smoothing Splines: Adding Mathematical Rigor\n\nSmoothing splines are an extension of regression splines but arise in a different context. They are derived by minimizing a residual sum of squares criterion subject to a penalty for roughness, balancing data fidelity and smoothness.\nThe optimization criterion for fitting a smooth function \\(g(x)\\) to data is given by:\n\\[\n\\text{minimize}_{g \\in S}  \\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2 + \\lambda \\int \\left( g''(t) \\right)^2 dt\n\\]\nKey Components:\n\nResidual Sum of Squares (RSS): The first term, \\(\\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2\\), ensures that the function \\(g(x)\\) fits the observed data points \\((x_i, y_i)\\) closely.\nRoughness Penalty: The second term, \\(\\lambda \\int \\left( g''(t) \\right)^2 dt\\), penalizes the curvature of \\(g(x)\\) by integrating the square of its second derivative. This term controls how “wiggly” the function is and enforces smoothness.\nTuning Parameter \\(\\lambda\\): The parameter \\(\\lambda \\geq 0\\) determines the tradeoff between fit and smoothness:\n\nWhen \\(\\lambda = 0\\): The penalty vanishes, and the function \\(g(x)\\) becomes fully flexible, interpolating all data points.\nAs \\(\\lambda \\to \\infty\\): The penalty dominates, and \\(g(x)\\) becomes increasingly smooth, eventually reducing to a simple linear function.\n\n\n\nIntuition: Smoothing splines allow for flexible, smooth fits that adapt to the structure of the data while avoiding excessive overfitting. By tuning \\(\\lambda\\), analysts can strike a balance between capturing meaningful patterns and maintaining a smooth, interpretable curve."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\n\nSmoothing splines yield a natural cubic spline solution with a knot at every unique value of \\(x_i\\). Rather than choosing specific knot locations, practitioners select a single tuning parameter \\(\\lambda\\), which balances data fidelity and smoothness through the roughness penalty.\n\nAutomatic Knot Placement: By placing a knot at each unique \\(x_i\\), smoothing splines avoid the need to manually choose knot positions. The smoothness parameter \\(\\lambda\\) becomes the key lever for controlling model complexity.\nSmoother Matrix: The fitted values can be written as\n\n\n\\[\n    \\hat{g}_\\lambda = \\mathbf{S}_\\lambda\\,\\mathbf{y},\n\\]\nwhere \\(\\mathbf{S}_\\lambda\\) is an \\(n \\times n\\) matrix determined by the locations of the \\(x_i\\) and the penalty \\(\\lambda\\).\n\n\nEffective Degrees of Freedom: The complexity of the smoothing spline is measured by the effective degrees of freedom, computed as\n\n\n\\[\n    \\text{df}_\\lambda = \\sum_{i=1}^n \\{\\mathbf{S}_\\lambda\\}_{ii},\n\\]\ni.e., the trace of the smoother matrix. A larger trace indicates a more flexible fit, while a smaller trace corresponds to a smoother, less flexible curve."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-choosing-lambda",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-choosing-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines — Choosing \\(\\lambda\\)",
    "text": "Smoothing Splines — Choosing \\(\\lambda\\)\n\nSpecifying \\(\\text{df}\\) Instead of \\(\\lambda\\)\n\nA key advantage of smoothing splines is the option to specify the effective degrees of freedom (\\(\\text{df}\\)) directly, rather than choosing \\(\\lambda\\) outright. This allows you to control the spline’s flexibility (or smoothness) in more intuitive terms.\n\n\nLOOCV\n\nAlternatively, \\(\\lambda\\) can be selected to minimize the leave-one-out (LOO) cross-validation error, given by\n\n\n\n\\[\n\\text{RSS}_{\\text{cv}}(\\lambda)\n      = \\sum_{i=1}^n \\Bigl( y_i - \\hat{g}_\\lambda^{(-i)}(x_i) \\Bigr)^2\n      = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2,\n\\]\nwhere \\(\\hat{g}_\\lambda^{(-i)}\\) denotes the fitted function obtained by leaving out the \\(i\\)-th observation, and \\(\\mathbf{S}_\\lambda\\) is the smoother matrix.\nWhether you fix the degrees of freedom directly or optimize \\(\\lambda\\) via cross-validation, the goal is to strike a balance between fidelity to the data and smoothness of the spline."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-specifying-degrees-of-freedom",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-specifying-degrees-of-freedom",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: Specifying Degrees of Freedom",
    "text": "Smoothing Splines: Specifying Degrees of Freedom\n\n\nEffective Degrees of Freedom (df)\n\n\nInstead of selecting \\(\\lambda\\) directly, we choose an equivalent “degrees of freedom” value, \\(\\text{df}_\\lambda\\).\nThe relationship between \\(\\lambda\\) and \\(\\text{df}\\) is such that increasing \\(\\lambda\\) (stronger penalty) lowers \\(\\text{df}\\), leading to a smoother (less flexible) spline.\n\n\nAdvantages\n\n\nIntuitive Interpretation: \\(\\text{df}\\) tells us how many “parameters” (roughly) our model is using, making it easier to grasp complexity.\nDirect Control: Analysts may already have an idea of how flexible their model needs to be, and can set \\(\\text{df}\\) accordingly.\n\n\nPractical Tip\n\n\nEmpirically, one might try a range of df values (e.g., 4, 5, 6, …) and pick the one that balances interpretability and fit (potentially guided by an information criterion or cross-validation)."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-loocv-to-specify-lambda",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-splines-loocv-to-specify-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: LOOCV to Specify \\(\\lambda\\)",
    "text": "Smoothing Splines: LOOCV to Specify \\(\\lambda\\)\n\n\n\n\nWe can use Cross-validation to choose \\(\\lambda\\).\n\n\nFor each candidate \\(\\lambda\\), we leave out one data point at a time, fit the spline, and measure the prediction error on the held-out point.\n\n\nMathematical Form\n\n\n\\[\n     \\text{RSS}_{\\text{cv}}(\\lambda)\n       = \\sum_{i=1}^n \\left(y_i - \\hat{g}_\\lambda^{(-i)}(x_i)\\right)^2,\n\\]\n\n\nwhere \\(\\hat{g}_\\lambda^{(-i)}\\) denotes the spline fitted without the \\(i\\)-th data point.\nIn practice, this can be computed more efficiently via the influence matrix \\(\\mathbf{S}_\\lambda\\):\n\n\n\\[\n       \\text{RSS}_{\\text{cv}}(\\lambda)\n         = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2.\n\\]\n\n\n\nAdvantages\n\n\nData-Driven: No need to guess \\(\\text{df}\\) or \\(\\lambda\\); we let cross-validation find the best trade-off.\nRobust: Tends to choose a value that generalizes well to new data.\n\n\nPractical Tip\n\n\nEvaluate \\(\\text{RSS}_{\\text{cv}}\\) for a grid of \\(\\lambda\\) values (e.g., \\(\\lambda \\in \\{0.01, 0.1, 1, 10\\}\\)) and pick the one that minimizes the cross-validation error."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-spline-degrees-of-freedom",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#smoothing-spline-degrees-of-freedom",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Spline: Degrees of Freedom",
    "text": "Smoothing Spline: Degrees of Freedom"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#local-regression-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#local-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Local Regression",
    "text": "Local Regression\n\nLocal regression is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.\n\n\n\n\n\n\n\n\n\nWith a sliding weight function, we fit separate linear fits over the range of \\(X\\) by weighted least squares.\n\n\nIt is a smart way to fit non-linear functions by fitting local linear functions on the data!"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#generalized-additive-models-1",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#generalized-additive-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\nGeneralized additive models (GAMs) allow us to extend the methods covered in this lecture to deal with multiple predictors.\nGAMs allows for flexible nonlinearities in several variables, but retains the additive structure of linear models.\n\\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\cdots + f_p(x_{ip}) + \\epsilon_i.\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gam-details",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gam-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAM Details",
    "text": "GAM Details\n\nCan fit a GAM simply using, e.g., natural splines:\nCoefficients are not that interesting; fitted functions are.\n\n\n\nCan mix terms — some linear, some nonlinear — and compare models with ANOVA.\nCan use smoothing splines or local regression as well:\n\nGAMs are additive, although low-order interactions can be included in a natural way using, e.g., bivariate smoothers."
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gams-for-classification",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#gams-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAMs for Classification",
    "text": "GAMs for Classification\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p).\n\\]"
  },
  {
    "objectID": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-2",
    "href": "lecture_slides/06_beyond_linearity/06_beyond_linearity.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nPolynomial Regression\n\nExpands a linear model by adding powers of the predictor(s).\n\nSimple to implement but can behave poorly at boundaries.\n\nStep Functions\n\nPartitions the predictor space into intervals; fits piecewise constant segments.\n\nHighly interpretable but can appear “blocky” and requires choosing cutpoints.\n\nRegression Splines\n\nUses piecewise polynomials, constrained to meet smoothly at knots.\n\nGreater flexibility than polynomials or step functions, with continuity at knots.\n\nNatural Cubic Splines\n\nEnforce additional constraints for boundary behavior.\n\nAllow more robust modeling without overfitting at the extremes.\n\n\n\n\n\n\nSmoothing Splines\n\nAutomates knot selection (a knot at each data point); regularizes via a penalty on curvature.\n\nBalances fidelity to data (\\(\\mathrm{RSS}\\)) and smoothness (\\(\\lambda\\)).\n\n\\(\\lambda\\) (or effective degrees of freedom) is chosen via cross-validation or by specifying df directly.\n\nLocal Regression\n\nFits a series of local linear models weighted by proximity to each target \\(x\\).\nUseful for highly variable, nonlinear relationships.\n\nGeneralized Additive Models (GAMs)\n\nAllows non-linear fits in multiple predictors yet maintains an additive structure.\n\nSupports smooth terms (splines, local regression) combined with linear terms.\n\n\nBottom Line\n\nBy moving beyond a strictly linear framework—using polynomials, splines, or GAMs—analysts can more accurately capture complex patterns while retaining interpretability and control over model complexity."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#overview",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\nTree-based Methods\nThe Basics of Decision Trees\nDetails of the tree-building process\nPredictions\nClassification Trees\n\n\n\nTrees Versus Linear Models\nBagging\nRandom Forests\nBoosting\nVariable Importance Measure\nBART\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#tree-based-methods-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#tree-based-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tree-based Methods",
    "text": "Tree-based Methods\n\nWe will discuss tree-based methods for regression and classification.\nThese involve stratifying or segmenting the predictor space into a number of simple regions.\nSince the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision-tree methods."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#pros-and-cons",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#pros-and-cons",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pros and Cons",
    "text": "Pros and Cons\n\nTree-based methods are simple and useful for interpretation.\nHowever, they typically are not competitive with the best supervised learning approaches in terms of prediction accuracy.\nHence we also discuss bagging, random forests, and boosting. These methods grow multiple trees which are then combined to yield a single consensus prediction.\nCombining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#the-basics-of-decision-trees-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#the-basics-of-decision-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Basics of Decision Trees",
    "text": "The Basics of Decision Trees\n\nDecision trees can be applied to both regression and classification problems.\nWe first consider regression problems, and then move on to classification."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-salary-hitters-data-how-would-you-stratify-it",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-salary-hitters-data-how-would-you-stratify-it",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball salary (Hitters) data: how would you stratify it?",
    "text": "Baseball salary (Hitters) data: how would you stratify it?\n\n\n\nBefore examining the structure of a decision tree, let us begin by considering how to stratify the data.\n\n\n\n\n\n\n\n\n\nHere, the response variable is player salary, which is color-coded in the figure from lower salaries (blue and green) to higher salaries (yellow and red).\n\nOur objective is to separate high-salary players from those with lower salaries.\n\nBy inspecting the plot, we observe that players earning higher salaries tend to cluster in the upper portion, whereas players with lower salaries appear in an “L-shaped” cluster below.\nOne straightforward approach is to establish a vertical threshold around five years of career experience, effectively isolating many of the higher-salary players.\nTo refine the partitioning further, we might introduce a horizontal threshold slightly above 100 hits, creating three distinct segments of the feature space.\nThis step-by-step segmentation process is precisely how decision trees operate. By recursively applying threshold-based rules, we isolate increasingly homogeneous subsets of players and, in turn, more accurately predict salary levels."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#decision-tree-for-these-data",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#decision-tree-for-these-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Decision tree for these data",
    "text": "Decision tree for these data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top node represents the full dataset.\nThe first split is based on years of experience:\n\nPlayers with less than 4.5 years → Left branch.\nPlayers with more than 4.5 years → Right branch.\n\nThis split closely aligns with our initial estimate of 5 years.\nAmong players with more than 4.5 years, the tree applies a second split:\n\nFewer than 117.5 hits → Left branch.\nMore than 117.5 hits → Right branch.\n\nThis recursive splitting refines the salary prediction.\nWhat Do the Numbers Represent?\nThe values at the bottom nodes indicate the average log salary of players in that group.\nSince a log transformation was applied, these values represent average log salaries rather than raw salaries.\nFinal Segmentation\nThe decision tree ultimately divides players into three distinct salary groups:\n\n\nHighest salary\nMedium salary\nLowest salary\n\n\nThese categories closely match—though not exactly—the three regions we initially identified.\nBy systematically applying these splits, decision trees segment data into meaningful, homogeneous groups.\nThe final tree has two internal nodes and three terminal nodes (leaves). The number in each leaf is the mean of the response for the observations that fall there."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#results",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\n\n\nThe tree stratifies or segments the players into three regions of predictor space:\n\\[\nR_1 = \\{X \\ | \\ \\text{Years} &lt; 4.5\\}\n\\]\n\\[\nR_2 = \\{X \\ | \\ \\text{Years} \\geq 4.5, \\text{Hits} &lt; 117.5\\}\n\\]\n\\[\nR_3 = \\{X \\ | \\ \\text{Years} \\geq 4.5, \\text{Hits} \\geq 117.5\\}\n\\]"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#terminology-for-trees",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#terminology-for-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Terminology for Trees",
    "text": "Terminology for Trees\n\nIn keeping with the tree analogy, the regions \\(R_1\\), \\(R_2\\), and \\(R_3\\) are known as terminal nodes.\nDecision trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree.\nThe points along the tree where the predictor space is split are referred to as internal nodes.\nIn the Hitters tree, the two internal nodes are indicated by the text \\(\\text{Years} &lt; 4.5\\) and \\(\\text{Hits} &lt; 117.5\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#interpretation-of-results",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#interpretation-of-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\n\nYears is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players.\nGiven that a player is less experienced, the number of Hits that he made in the previous year seems to play little role in his Salary.\nBut among players who have been in the major leagues for five or more years, the number of Hits made in the previous year does affect Salary, and players who made more Hits last year tend to have higher salaries.\nSurely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#details-of-the-tree-building-process-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#details-of-the-tree-building-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of the tree-building process",
    "text": "Details of the tree-building process\n\nWe divide the predictor space — that is, the set of possible values for \\(X_1, X_2, \\dots, X_p\\) — into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\dots, R_J\\).\nFor every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#more-details-of-the-tree-building-process",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#more-details-of-the-tree-building-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More details of the tree-building process",
    "text": "More details of the tree-building process\n\nIn theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predictive model.\nThe goal is to find boxes \\(R_1, \\dots, R_J\\) that minimize the RSS, given by\n\n\n\\[\n\\sum_{j=1}^{J} \\sum_{i \\in R_j} \\left( y_i - \\hat{y}_{R_j} \\right)^2,\n\\]\nwhere \\(\\hat{y}_{R_j}\\) is the mean response for the training observations within the \\(j\\)-th box."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#more-details-of-the-tree-building-process-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#more-details-of-the-tree-building-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More details of the tree-building process",
    "text": "More details of the tree-building process\n\nUnfortunately, it is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes.\nFor this reason, we take a top-down, greedy approach that is known as recursive binary splitting.\nThe approach is top-down because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree.\nIt is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#details-continued",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#details-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details— Continued",
    "text": "Details— Continued\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X | X_j &lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS.\nNext, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.\nHowever, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions.\nAgain, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#predictions-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions",
    "text": "Predictions\nWe predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#predictions-example",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#predictions-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions: Example",
    "text": "Predictions: Example\n\n\nA five-region example\n\n\n\n\n\n\n\n\n\n\n\nTop Left: A partition of two-dimensional feature space that could not result from recursive binary splitting.\nTop Right: The output of recursive binary splitting on a two-dimensional example.\nBottom Left: A tree corresponding to the partition in the top right panel.\nBottom Right: A perspective plot of the prediction surface corresponding to that tree."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#predictions-example-details",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#predictions-example-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions: Example Details",
    "text": "Predictions: Example Details\n\n\nTop right & Bottom left: Regions that can be achieved by a decision tree.\nStep-by-step breakdown:\n\nThe first split occurs at t1, making a vertical partition at \\(x_1\\).\nThe left region is further divided at t2 (a split on \\(x_2\\)), creating Region 1 and Region 2.\nThe right-hand partition is further split at t3, making another vertical split, forming Region 3.\nFinally, a horizontal split at t4 on \\(x_2\\) divides it into two new regions.\n\n\nAt the end of this process, the space is divided into five distinct regions.\n\n\nMaking Predictions with the Decision Tree\n\nEach terminal node approximates the regression function by computing the mean of training observations in that region.\nTo predict a test observation:\n\n\nStart at the top and check its \\(x_1\\) value.\nMove left if \\(x_1 &lt; t1\\), otherwise move right.\nFollow subsequent splits at each internal node until reaching a terminal region.\nThe final prediction is the mean response value in that region.\n\n\nThis results in a piecewise constant function, which is visualized in the plot on the bottom right."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#pruning-a-tree",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#pruning-a-tree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pruning a tree",
    "text": "Pruning a tree\n\nThe process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance.\n\n\n\nA better strategy is to grow a very large tree \\(T_0\\), and then prune it back in order to obtain a subtree.\nCost complexity pruning — also known as weakest link pruning — is used to do this.\nWe consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\), there corresponds a subtree \\(T \\subset T_0\\) such that\n\n\n\\[\n\\sum_{m=1}^{|T|} \\sum_{i : x_i \\in R_m} \\left( y_i - \\hat{y}_{R_m} \\right)^2 + \\alpha |T|\n\\]\nis as small as possible.\nHere \\(|T|\\) indicates the number of terminal nodes of the tree \\(T\\), \\(R_m\\) is the rectangle (i.e., the subset of predictor space) corresponding to the \\(m\\)-th terminal node, and \\(\\hat{y}_{R_m}\\) is the mean of the training observations in \\(R_m\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#choosing-the-best-subtree",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#choosing-the-best-subtree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the best subtree",
    "text": "Choosing the best subtree\n\nThe tuning parameter \\(\\alpha\\) controls a trade-off between the subtree’s complexity and its fit to the training data.\nWe select an optimal value \\(\\hat{\\alpha}\\) using cross-validation.\nWe then return to the full data set and obtain the subtree corresponding to \\(\\hat{\\alpha}\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#tree-algorithm",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#tree-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tree algorithm",
    "text": "Tree algorithm\n\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\).\nUse K-fold cross-validation to choose \\(\\alpha\\). For each \\(k = 1, \\dots, K\\):\n3.1 Repeat Steps 1 and 2 on the \\(\\frac{K-1}{K}\\)-th fraction of the training data, excluding the \\(k\\)-th fold. 3.2 Evaluate the mean squared prediction error on the data in the left-out \\(k\\)-th fold, as a function of \\(\\alpha\\).\nAverage the results, and pick \\(\\alpha\\) to minimize the average error.\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-example-continued",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example continued",
    "text": "Baseball example continued\n\nFirst, we randomly divided the data set in half, yielding 132 observations in the training set and 131 observations in the test set.\nWe then built a large regression tree on the training data and varied \\(\\alpha\\) in order to create subtrees with different numbers of terminal nodes.\nFinally, we performed six-fold cross-validation in order to estimate the cross-validated MSE of the trees as a function of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-example-the-full-tree-before-pruning",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-example-the-full-tree-before-pruning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example: The Full Tree Before Pruning",
    "text": "Baseball example: The Full Tree Before Pruning"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-example-cross-validation-for-the-prune-tree",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#baseball-example-cross-validation-for-the-prune-tree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example: Cross Validation for the Prune Tree",
    "text": "Baseball example: Cross Validation for the Prune Tree\n\n\nAlong the horizontal axis, we have tree size, which is controlled by the alpha parameter (\\(\\alpha\\)). This parameter directly influences the complexity of the decision tree.\n\nWhen \\(\\alpha = 0\\), there is no penalty on tree size, meaning the model grows to its largest possible tree, which in this case contains 12 terminal nodes.\nAs \\(\\alpha\\) increases, a stronger penalty is applied to larger trees, gradually reducing the number of terminal nodes.\nAs \\(\\alpha\\) continues to increase, the model prunes away more splits, simplifying the tree structure.\nAt the extreme, when \\(\\alpha\\) is large enough, the tree is reduced to a single node, meaning no splits occur, and the model collapses into a single global mean prediction.\nThe green curve is what we get from cross validation and it’s minimized at around three terminal nodes!"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#classification-trees-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#classification-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nVery similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#classification-trees-2",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#classification-trees-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nJust as in the regression setting, we use recursive binary splitting to grow a classification tree.\nIn the classification setting, RSS cannot be used as a criterion for making the binary splits.\nA natural alternative to RSS is the classification error rate. This is simply the fraction of the training observations in that region that do not belong to the most common class:\n\n\n\\[\nE = 1 - \\max_k(\\hat{p}_{mk}).\n\\]\nHere \\(\\hat{p}_{mk}\\) represents the proportion of training observations in the \\(m\\)-th region that are from the \\(k\\)-th class.\n\nHowever, classification error is not sufficiently sensitive for tree-growing, and in practice, two other measures are preferable."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#gini-index-and-deviance",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#gini-index-and-deviance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gini index and Deviance",
    "text": "Gini index and Deviance\nThe Gini index is defined by\n\\[\nG = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk}),\n\\]\na measure of total variance across the \\(K\\) classes. The Gini index takes on a small value if all of the \\(\\hat{p}_{mk}\\)’s are close to zero or one.\n\nFor this reason, the Gini index is referred to as a measure of node purity — a small value indicates that a node contains predominantly observations from a single class.\n\n\nDeviance or cross-entropy, given by\n\\[\nD = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}.\n\\]\n\nIt turns out that the Gini index and the cross-entropy are very similar numerically."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-heart-data",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart data",
    "text": "Example: Heart data\n\nThese data contain a binary outcome HD for 303 patients who presented with chest pain.\nAn outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease.\nThere are 13 predictors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements.\nCross-validation yields a tree with six terminal nodes. See next figure."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-heart-data-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-heart-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart data",
    "text": "Example: Heart data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the top, we see the fully grown tree.\n\nThe first split occurs on FEL (a thallium stress test), followed by splits on CA (calcium). The terminal nodes classify observations as “No” (no heart disease) or “Yes” (heart disease) based on majority class.\nSome terminal nodes with the same classification still have splits. This suggests that while both nodes predict “No,” one is purer than the other, as identified by the Gini index.\n\nSince this tree is likely too complex, cross-validation was used to find an optimal size.\n\nThe right panel shows training, validation, and test errors, with the cross-validation error curve guiding the selection of a tree size. A tree with six terminal nodes performed best, balancing complexity and accuracy.\n\nThe pruned tree (size six) is shown on the right, derived using the cost-complexity parameter (\\(\\alpha\\)). This subtree of the original tree achieved an estimated 25% classification error—a significant improvement in generalization."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#trees-versus-linear-models-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#trees-versus-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Trees Versus Linear Models",
    "text": "Trees Versus Linear Models\n\n\nTop Row: True linear boundary;\nBottom row: true non-linear boundary.\nLeft column: Linear model;\nRight column: Tree-based model."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#advantages-and-disadvantages-of-trees",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#advantages-and-disadvantages-of-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advantages and Disadvantages of Trees",
    "text": "Advantages and Disadvantages of Trees\n\nAdvantage: Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\nAdvantage: Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches.\nAdvantage: Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).\nAdvantage: Trees can easily handle qualitative predictors without the need to create dummy variables.\nDisadvantage: Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\nHowever, by aggregating many decision trees, the predictive performance of trees can be substantially improved. We introduce these concepts next."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bagging-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bagging-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging",
    "text": "Bagging\n\n\nBootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. It is particularly useful and frequently used in the context of decision trees.\nRecall that given a set of \\(n\\) independent observations \\(Z_1, \\dots, Z_n\\), each with variance \\(\\sigma^2\\), the variance of the mean \\(\\bar{Z}\\) of the observations is given by \\(\\sigma^2 / n\\).\n\nThis means that as \\(n\\) increases (i.e., we take more independent observations and average them), the variance of the mean decreases. The more independent samples we have, the more stable our estimate becomes.\n\nIn other words, averaging a set of observations reduces variance. In practice, we do not have access to multiple independent training sets, which would allow us to directly apply the above variance reduction principle.\n\nHowever, bagging overcomes this limitation by using bootstrapping—randomly sampling (with replacement) from a single training set to create multiple datasets. These datasets are used to train multiple models, whose predictions are then averaged, effectively reducing variance in the same way that averaging multiple independent observations does.\nThus, bagging approximates the variance-reducing effect of having multiple training sets by repeatedly resampling from the same dataset."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bagging-2",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bagging-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging",
    "text": "Bagging\nWith bootstrap, by taking repeated samples from the (single) training data set, we generate \\(B\\) different bootstrapped training data sets.\nWe then train our method on the \\(b\\)-th bootstrapped training set in order to get \\(\\hat{f}^*_b(x)\\), the prediction at a point \\(x\\). We then average all the predictions to obtain\n\\[\n\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^*_b(x).\n\\]\nThis is called bagging.\n\nThe above prescription applied to regression trees.\nFor classification trees: for each test observation, we record the class predicted by each of the \\(B\\) trees, and take a majority vote: the overall prediction is the most commonly occurring class among the \\(B\\) predictions."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bagging-the-heart-data",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bagging-the-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging the Heart data",
    "text": "Bagging the Heart data\n\n\nBagging and Random Forest results.\n\n\n\n\n\n\n\n\n\n\n\nThe dashed line indicates the test error resulting from a single classification tree.\nThe test error (black and orange) is shown as a function of \\(B\\), the number of bootstrapped training sets used.\nRandom forests were applied with \\(m = \\sqrt{p}\\).\nThe green and blue traces show the Out-of-Bag (OOB) error, which in this case is considerably lower."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#out-of-bag-error-estimation",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#out-of-bag-error-estimation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Out-of-Bag Error Estimation",
    "text": "Out-of-Bag Error Estimation\n\nIt turns out that there is a very straightforward way to estimate the test error of a bagged model.\nRecall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations.\nThe remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.\nWe can predict the response for the \\(i\\)th observation using each of the trees in which that observation was OOB. This will yield around \\(B/3\\) predictions for the \\(i\\)th observation, which we average.\nThis estimate is essentially the LOO cross-validation error for bagging, if \\(B\\) is large."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#test-error-vs.-oob-error-in-bagging",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#test-error-vs.-oob-error-in-bagging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Test Error vs. OOB Error in Bagging",
    "text": "Test Error vs. OOB Error in Bagging\n\n\nData Overlap & Model Correlation\n\nEach model is trained on a bootstrap sample, leaving out some observations for OOB evaluation.\n\nOverlap among bootstrap samples introduces correlation among models, causing OOB error to differ from an independent test error.\n\nSample Size & Representativeness\n\nOOB estimates use only the leftover observations from each bootstrap draw, often fewer and less representative than a true external test set.\n\nA well-chosen test set is typically larger and fully independent, providing a more stable performance estimate.\n\nRandom Fluctuations & Variance\n\nOOB error depends on random sampling; it can have higher variance than test error.\n\nAn external test set, assuming it is independent, generally offers a less biased measure of generalization error.\n\nPossible Bias in OOB Error\n\nWhile OOB error acts like an “internal cross-validation,” it may exhibit slight bias—optimistic or pessimistic—depending on the dataset and model specifics.\n\nCertain data characteristics or model sensitivities can amplify discrepancies between OOB and true test performance."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#random-forests-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#random-forests-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Random Forests",
    "text": "Random Forests\n\nKey Idea\nRandom forests enhance bagged trees by introducing a small modification that decorrelates the individual trees, thus reducing variance when their predictions are averaged.\nConstruction\n\nBootstrapped Samples: As in bagging, multiple decision trees are trained on bootstrapped subsets of the original dataset.\n\nRandom Predictor Selection: At each split in a tree, only a random subset of \\(m\\) predictors (out of \\(p\\) total) is considered. The best split is chosen exclusively from these \\(m\\) predictors.\n\nTypical Parameter Choice\nA new subset of \\(m\\) predictors is drawn at every split. Common practice sets \\(m \\approx \\sqrt{p}\\). For example, with \\(p = 13\\) predictors, 4 might be considered at each split.\nThis random predictor selection helps ensure the trees are less correlated, thereby improving the variance reduction achieved by averaging."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-gene-expression-data",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-gene-expression-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Gene Expression Data",
    "text": "Example: Gene Expression Data\n\nWe applied random forests to a high-dimensional biological data set consisting of expression measurements of 4,718 genes measured on tissue samples from 349 patients.\nThere are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions.\nEach of the patient samples has a qualitative label with 15 different levels: either normal or one of 14 different types of cancer.\nWe use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set.\nWe randomly divided the observations into a training and a test set, and applied random forests to the training set for three different values of the number of splitting variables \\(m\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#results-gene-expression-data",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#results-gene-expression-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results: Gene Expression Data",
    "text": "Results: Gene Expression Data\nResults from random forests for the fifteen-class gene expression data set with \\(p = 500\\) predictors.\n\n\nThe test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of \\(m\\), the number of predictors available for splitting at each interior tree node.\nRandom forests (\\(m &lt; p\\)) lead to a slight improvement over bagging (\\(m = p\\)). A single classification tree has an error rate of 45.7%."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#boosting-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#boosting-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting",
    "text": "Boosting\n\nLike bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification.\nRecall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\nNotably, each tree is built on a bootstrap data set, independent of the other trees.\nBoosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees and it is added to the collection of trees if it contributes to the performance improvement."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#boosting-algorithm-for-regression-trees",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#boosting-algorithm-for-regression-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting Algorithm for Regression Trees",
    "text": "Boosting Algorithm for Regression Trees\n\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\) for all \\(i\\) in the training set.\nFor \\(b = 1, 2, \\dots, B\\), repeat:\n\n\n2.1 Fit a tree \\(\\hat{f}^b\\) with \\(d\\) splits (\\(d + 1\\) terminal nodes) to the training data \\((X, r)\\).\n2.2 Update \\(\\hat{f}\\) by adding in a shrunken version of the new tree:\n\\[\n   \\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x).\n\\]\n2.3 Update the residuals,\n\n\n\\[\n   r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i).\n\\]\n\n\nOutput the boosted model,\n\n\n\\[\n\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x).\n\\]"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#what-is-the-idea-behind-this-procedure",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#what-is-the-idea-behind-this-procedure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the idea behind this procedure?",
    "text": "What is the idea behind this procedure?\n\nUnlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly.\nGiven the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals.\nEach of these trees can be rather small, with just a few terminal nodes, determined by the parameter \\(d\\) in the algorithm.\nBy fitting small trees to the residuals, we slowly improve \\(\\hat{f}\\) in areas where it does not perform well. The shrinkage parameter \\(\\lambda\\) slows the process down even further, allowing more and different shaped trees to attack the residuals.\nBoosting for classification is similar in spirit to boosting for regression, but is a bit more complex. To learn the deatails, check the Elements of Statistical Learning book, chapter 10."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-gene-expression-data-continued",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#example-gene-expression-data-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Gene expression data continued",
    "text": "Example: Gene expression data continued\nResults from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.\n\n\nThe test error is displayed as a function of the number of trees.\nFor the two boosted models, \\(\\lambda = 0.01\\). Depth-1 trees, when a single split where applied, slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.\nThe test error rate for a single tree is 24%."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#tuning-parameters-for-boosting",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#tuning-parameters-for-boosting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tuning Parameters for Boosting",
    "text": "Tuning Parameters for Boosting\n\nThe number of trees \\(B\\). Unlike bagging and random forests, boosting can overfit if \\(B\\) is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select \\(B\\).\nThe shrinkage parameter \\(\\lambda\\). A small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small \\(\\lambda\\) can require using a very large value of \\(B\\) in order to achieve good performance.\nThe number of splits \\(d\\) in each tree, which controls the complexity of the boosted ensemble. Often \\(d = 1\\) works well, in which case each tree is a stump, consisting of a single split and resulting in an additive model. More generally \\(d\\) is the interaction depth, and controls the interaction order of the boosted model, since \\(d\\) splits can involve at most \\(d\\) variables."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#variable-importance-measure-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#variable-importance-measure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Variable Importance Measure",
    "text": "Variable Importance Measure\n\n\n\nFor bagged/RF regression trees:\n\nRecord the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees.\nA large value indicates an important predictor.\n\nFor bagged/RF classification trees:\n\nAdd up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all \\(B\\) trees.\n\n\n\n\n\nVariable Importance Plot for the Heart Data"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#summary",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nDecision trees are simple and interpretable models for regression and classification.\nHowever, they are often not competitive with other methods in terms of prediction accuracy.\nBagging, random forests, and boosting are effective methods for improving the prediction accuracy of trees:\n\nThey work by growing many trees on the training data and then combining the predictions of the resulting ensemble of trees.\n\nRandom forests and boosting are among the state-of-the-art methods for supervised learning, though their results can be difficult to interpret."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-bayesian-additive-regression-trees-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-bayesian-additive-regression-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART: Bayesian Additive Regression Trees",
    "text": "BART: Bayesian Additive Regression Trees\n\nRecall that bagging and random forests make predictions from an average of regression trees, each of which is built using a random sample of data and/or predictors. Each tree is built separately from the others.\nBy contrast, boosting uses a weighted sum of trees, each of which is constructed by fitting a tree to the residual of the current fit. Thus, each new tree attempts to capture signal that is not yet accounted for by the current set of trees.\nBayesian additive regression trees (BART), an ensemble method that uses decision trees as its building blocks, is related to both random forests and boosting:\n\nEach tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.\n\nThe main novelty in BART is the way in which new trees are generated.\nBART can be applied to regression, classification, and other problems; we will focus here just on regression."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-algorithm-intuition",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-algorithm-intuition",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART Algorithm Intuition",
    "text": "BART Algorithm Intuition"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-algorithm-intuition-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-algorithm-intuition-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART Algorithm Intuition",
    "text": "BART Algorithm Intuition\n\n\nMultiple Trees in Parallel\n\nChoose a number of trees, \\(K\\) (often in the hundreds), and a total of \\(B\\) iterations.\n\nAll \\(K\\) trees start as a single root node and are updated in parallel through successive iterations.\n\nRandom Perturbations\n\nAt each iteration, each tree is modified via a “perturbation,” which can involve:\n\nAdding or removing a split.\n\nAdjusting the predicted values in terminal nodes.\n\n\nEach tree is updated based on partial residuals, improving how the ensemble fits the data.\n\nIterative Evolution\n\nOver \\(B\\) iterations, trees evolve—some gain new splits, others lose them, and node predictions adjust.\n\nThis process is akin to a Markov chain over tree configurations, guided by how well each tree explains current residuals.\n\nFinal Prediction by Averaging\n\nAfter \\(B\\) iterations, you have an ensemble of \\(K\\) trees that collectively capture the posterior distribution.\n\nThe final prediction at a point \\(x\\) is typically the average of predictions from all \\(K\\) trees at the final iteration (or across multiple post–burn-in iterations)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bayesian-additive-regression-trees-some-notation",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bayesian-additive-regression-trees-some-notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayesian Additive Regression Trees — Some Notation",
    "text": "Bayesian Additive Regression Trees — Some Notation\n\nWe let \\(K\\) denote the number of regression trees, and \\(B\\) the number of iterations for which the BART algorithm will be run.\nThe notation \\(\\hat{f}^{b}_{k}(x)\\) represents the prediction at \\(x\\) for the \\(k\\)th regression tree used in the \\(b\\)th iteration.\nAt the end of each iteration, the \\(K\\) trees from that iteration will be summed, i.e.,\n\n\n\\[\n  \\hat{f}^{b}(x) = \\sum_{k=1}^{K} \\hat{f}^{b}_{k}(x)\n\\]\nfor \\(b = 1, \\dots, B\\)."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-iterations",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-iterations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART Iterations",
    "text": "BART Iterations\n\nIn the first iteration of the BART algorithm, all trees are initialized to have a single root node, with\n\\[\n  \\hat{f}^{1}_{k}(x) = \\frac{1}{nK} \\sum_{i=1}^{n} y_i\n\\]\nrepresenting the mean of the response values divided by the total number of trees. Thus,\n\\[\n  \\hat{f}^{1}(x) = \\sum_{k=1}^{K} \\hat{f}^{1}_{k}(x) = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n\\]\nIn subsequent iterations, BART updates each of the \\(K\\) trees, one at a time. In the \\(b\\)th iteration, to update the \\(k\\)th tree, we subtract from each response value the predictions from all but the \\(k\\)th tree, in order to obtain a partial residual\n\\[\n  r_i = y_i - \\sum_{k' &lt; k} \\hat{f}^{b}_{k'}(x_i) - \\sum_{k' &gt; k} \\hat{f}^{b-1}_{k'}(x_i), \\quad i = 1, \\dots, n\n\\]"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#new-trees-are-chosen-by-perturbations",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#new-trees-are-chosen-by-perturbations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "New trees are chosen by perturbations",
    "text": "New trees are chosen by perturbations\n\nRather than fitting a fresh tree to this partial residual, BART randomly chooses a perturbation to the tree from the previous iteration \\(\\hat{f}^{b-1}_{k}\\) from a set of possible perturbations, favoring ones that improve the fit to the partial residual.\nThere are two components to this perturbation:\n\nWe may change the structure of the tree by adding or pruning branches.\n\nWe may change the prediction in each terminal node of the tree."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#examples-of-possible-perturbations-to-a-tree",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#examples-of-possible-perturbations-to-a-tree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Examples of possible perturbations to a tree",
    "text": "Examples of possible perturbations to a tree"
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#what-does-bart-deliver",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#what-does-bart-deliver",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What does BART Deliver?",
    "text": "What does BART Deliver?\n\nThe output of BART is a collection of prediction models,\n\\[\n  \\hat{f}^{b}(x) = \\sum_{k=1}^{K} \\hat{f}^{b}_{k}(x), \\quad \\text{for } b = 1,2, \\dots, B.\n\\]\nTo obtain a single prediction, we simply take the average after some \\(L\\) burn-in iterations,\n\\[\n  \\hat{f}(x) = \\frac{1}{B - L} \\sum_{b=L+1}^{B} \\hat{f}^{b}(x).\n\\]\n\nThe perturbation-style moves guard against overfitting since they limit how hard we fit the data in each iteration.\nWe can also compute quantities other than the average: for instance, the percentiles of \\(f^{L+1}(x), \\dots, f^{B}(x)\\) provide a measure of uncertainty of the final prediction."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-applied-to-the-heart-data",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-applied-to-the-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART applied to the Heart data",
    "text": "BART applied to the Heart data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(K = 200\\) trees; the number of iterations is increased to 10,000.\nDuring the initial iterations (in gray), the test and training errors jump around a bit.\nAfter this initial burn-in period, the error rates settle down.\nThe tree perturbation process largely avoids overfitting."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-is-a-bayesian-method",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#bart-is-a-bayesian-method",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART is a Bayesian Method",
    "text": "BART is a Bayesian Method\n\nIt turns out that the BART method can be viewed as a Bayesian approach to fitting an ensemble of trees: each time we randomly perturb a tree in order to fit the residuals, we are in fact drawing a new tree from a posterior distribution.\nFurthermore, the BART algorithm can be viewed as a Markov chain Monte Carlo procedure for fitting the BART model.\nWe typically choose large values for \\(B\\) and \\(K\\), and a moderate value for \\(L\\): for instance, \\(K = 200\\), \\(B = 1,000\\), and \\(L = 100\\) are reasonable choices. BART has been shown to have impressive out-of-box performance — that is, it performs well with minimal tuning."
  },
  {
    "objectID": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#summary-1",
    "href": "lecture_slides/07_tree_based_methods/07_tree_based_methods.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nDecision Trees\n\nPartition the predictor space into simple regions.\nEasy to interpret and visualize.\nProne to overfitting without pruning or regularization.\n\nBagging\n\nTrains multiple trees on bootstrap samples.\nAverages predictions to reduce variance.\nOut-of-Bag (OOB) error provides an internal estimate of test error.\n\nRandom Forests\n\nA variant of bagging that selects a random subset of predictors at each split.\nReduces correlation among trees, improving variance reduction.\nTypically uses \\(m \\approx \\sqrt{p}\\) features at each split.\n\n\n\n\nBoosting\n\nBuilds trees sequentially, each learning from the residuals of the previous trees.\nInvolves a shrinkage parameter (\\(\\lambda\\)) to control the learning rate.\nCan achieve strong predictive performance, but can be less interpretable.\n\nBayesian Additive Regression Trees (BART)\n\nCombines ideas from bagging and boosting in a Bayesian framework.\nMaintains \\(K\\) trees in parallel, updating each by random perturbations.\nCan handle both regression and classification.\nOffers built-in measures of uncertainty and often works well with minimal tuning."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#agenda",
    "href": "lecture_slides/08_time_series/08_time_series.html#agenda",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "Agenda",
    "text": "Agenda\n\nProblem framing & data hygiene for temporal prediction\nBaselines & decomposition\nStationarity, ACF/PACF, and differencing\nClassical models (ARIMA/SARIMA/SARIMAX) for numeric targets\nTime-aware validation: holdout by time, rolling-origin, TimeSeriesSplit, and gaps\nFeature engineering for ML forecasters\nML pipelines for forecasting (Ridge/Lasso, RF, GBM, XGBoost) with leak-safe CV\nMulti-step forecast strategies (recursive, direct, multi-output)\nUncertainty quantification and prediction intervals\nTime-ordered classification (categorical targets): labeling, splits, metrics, calibration\nMonitoring across vintages & model governance\nReferences & further reading"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#problem-framing",
    "href": "lecture_slides/08_time_series/08_time_series.html#problem-framing",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "1) Problem framing",
    "text": "1) Problem framing\n\nGoal: Predict an outcome measured over time.\n\nNumeric response (forecasting): ( y_t )\nCategorical response (time-ordered classification): ( y_t )\n\nData structure: time index ( t ), optional exogenous regressors ( _t ).\nKey risks: data leakage (look-ahead, future-derived features), temporal dependence, non-stationarity.\n\nLeakage taxonomy (examples): - Using ( y_{t+1} ) or features computed with future windows to predict ( y_t ). - Global standardization fitted on full history; must fit only on training window. - Target leakage via engineered features that use labels from the future.\n\nGuiding rule: Any transform, selection, or encoding must be learned only on training data from past to present."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#baselines-decomposition",
    "href": "lecture_slides/08_time_series/08_time_series.html#baselines-decomposition",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "2) Baselines & decomposition",
    "text": "2) Baselines & decomposition\n\nNaïve: (_{t+h|t} = y_t)\nSeasonal naïve (period (m)): ({t+h|t} = y{t+h-m(h-1)/m })\nDrift: ({t+h|t} = y_t + (y_t - y{t_0}))\n\nDecomposition: ( y_t = _t + _t + _t )\n#| label: fig-decompose\n#| echo: true\n#| fig-cap: STL decomposition (example)\nimport pandas as pd, numpy as np\nfrom statsmodels.tsa.seasonal import STL\ny = pd.Series(..., index=...)  # supply your series\nres = STL(y, period=12).fit()\nres.plot()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#stationarity-correlation-structure",
    "href": "lecture_slides/08_time_series/08_time_series.html#stationarity-correlation-structure",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "3) Stationarity & correlation structure",
    "text": "3) Stationarity & correlation structure\n\nWeak stationarity: ( [y_t]=), ( (y_t)=^2 ), ( (y_t,y_{t+k})=_k )\nACF: ( _k = _k/_0 )\nDifferencing: ( y_t = y_t - y_{t-1} ), seasonal ( m y_t = y_t - y*{t-m} )\n\n#| label: fig-acf-pacf\n#| echo: true\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\nfig1 = plot_acf(y.dropna(), lags=48)\nfig2 = plot_pacf(y.dropna(), lags=48)\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#classical-models-for-numeric-targets",
    "href": "lecture_slides/08_time_series/08_time_series.html#classical-models-for-numeric-targets",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "4) Classical models for numeric targets",
    "text": "4) Classical models for numeric targets\nARIMA ((p,d,q))\n\nDifference (d) times, then model: [ (B)(1-B)^d y_t = (B)_t, _t (0,^2) ] where ( (B)=1-_1B--_pB^p ), ( (B)=1+_1B++_qB^q ).\n\nSeasonal ARIMA ((p,d,q)(P,D,Q)_m)\n[(1-B)^d (1-Bm)D (B)(B^m) y_t = (B)(B^m)_t.]\nSARIMAX (with exogenous regressors)\n[(1-B)^d (1-Bm)D (B)(B^m) y_t = (B)(B^m)_t + _t^.]\n#| echo: true\nimport statsmodels.api as sm\nendog = y\nexog  = ...  # pd.DataFrame of aligned regressors\nmodel = sm.tsa.statespace.SARIMAX(endog, exog=exog, order=(p,d,q),\n                                  seasonal_order=(P,D,Q,m), enforce_stationarity=False,\n                                  enforce_invertibility=False)\nres = model.fit()\nforecast = res.get_forecast(steps=H, exog=exog_future).predicted_mean\nDiagnostics: standardized residuals, Ljung–Box, ACF of residuals, normality check."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#time-aware-validation-direct-estimation-of-test-error",
    "href": "lecture_slides/08_time_series/08_time_series.html#time-aware-validation-direct-estimation-of-test-error",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "5) Time-aware validation (direct estimation of test error)",
    "text": "5) Time-aware validation (direct estimation of test error)\nWhy random K-fold is invalid: violates temporal order and leaks future into training.\nSchemes: - Temporal holdout: train on ([t_0, t_1]), test on ((t_1, t_2]). - Rolling-origin (expanding): - Fold (k): train ([t_0, t_k]), test ((t_k, t_{k+1}]) - Sliding window (fixed-length training window). - Blocked CV with gap: leave a gap (g) between train and test to avoid leakage via feature lags/rolling stats. - TimeSeriesSplit (scikit-learn): configurable splits; emulate gap by trimming edges.\n#| echo: true\nfrom sklearn.model_selection import TimeSeriesSplit\nimport numpy as np\n\ntscv = TimeSeriesSplit(n_splits=5, test_size=24)  # e.g., 24 periods per fold\nfor k, (tr, te) in enumerate(tscv.split(np.arange(len(y)))):\n    # fit model on y.iloc[tr], evaluate on y.iloc[te]\n    ...\nMetrics (numeric): RMSE, MAE, sMAPE, MASE.\nMetrics (categorical): ROC-AUC, PR-AUC, Brier score, log loss, calibration."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#feature-engineering-for-ml-forecasters",
    "href": "lecture_slides/08_time_series/08_time_series.html#feature-engineering-for-ml-forecasters",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "6) Feature engineering for ML forecasters",
    "text": "6) Feature engineering for ML forecasters\n\nLags: ( y_{t-1},,y_{t-K} ), and lagged exogenous ( x_{t-j} ).\nRolling stats: mean/median/std/min/max, EWMA.\nCalendar features: DOW, month, quarter, holidays; Fourier terms for seasonality.\nInteractions and regimes (e.g., promo × season).\nRule: compute features using only past data in each training fold.\n\n#| echo: true\nimport pandas as pd\ndef make_features(df, target_col, lags=(1,3,6,12), roll_windows=(3,6,12)):\n    out = df.copy()\n    for L in lags:\n        out[f\"{target_col}_lag{L}\"] = out[target_col].shift(L)\n    for W in roll_windows:\n        out[f\"{target_col}_rollmean{W}\"] = out[target_col].shift(1).rolling(W).mean()\n        out[f\"{target_col}_rollstd{W}\"]  = out[target_col].shift(1).rolling(W).std()\n    # Fourier terms (seasonality m)\n    import numpy as np\n    m = 12\n    for k in range(1, 3):\n        out[f\"fourier_sin_{k}\"] = np.sin(2*np.pi*k*np.arange(len(out))/m)\n        out[f\"fourier_cos_{k}\"] = np.cos(2*np.pi*k*np.arange(len(out))/m)\n    return out"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#ml-pipelines-with-leak-safe-cv",
    "href": "lecture_slides/08_time_series/08_time_series.html#ml-pipelines-with-leak-safe-cv",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "7) ML pipelines with leak-safe CV",
    "text": "7) ML pipelines with leak-safe CV\n#| echo: true\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np, pandas as pd\n\nnum_features = [...]  # numerical exog not derived from future\ncat_features = [...]  # categorical exog\n\npre = ColumnTransformer([\n    (\"num\", StandardScaler(with_mean=True, with_std=True), num_features),\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n])\n\nmodel = Ridge(alpha=1.0)\npipe = Pipeline([(\"prep\", pre), (\"est\", model)])\nWalk-forward evaluator (reusable):\n#| echo: true\ndef walk_forward_eval(X, y, splits, estimator, metrics):\n    records = []\n    for k, (tr, te) in enumerate(splits):\n        est = estimator  # clone if needed\n        est.fit(X.iloc[tr], y.iloc[tr])\n        pred = est.predict(X.iloc[te])\n        rec = {\"fold\": k}\n        for name, fn in metrics.items():\n            rec[name] = fn(y.iloc[te], pred)\n        records.append(rec)\n    return pd.DataFrame(records).assign(mean=lambda d: d.drop(columns=[\"fold\"]).mean(axis=1))"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#multi-step-strategies",
    "href": "lecture_slides/08_time_series/08_time_series.html#multi-step-strategies",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "8) Multi-step strategies",
    "text": "8) Multi-step strategies\n\nRecursive: fit 1-step model; iterate predictions to reach horizon (H).\nDirect: fit (H) separate models for each step (h).\nMulti-output: fit one model that predicts ((y_{t+1},,y_{t+H})).\n\nTrade-offs: - Recursive propagates error; direct can be data-hungry; multi-output captures cross-horizon structure."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#uncertainty-intervals",
    "href": "lecture_slides/08_time_series/08_time_series.html#uncertainty-intervals",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "9) Uncertainty & intervals",
    "text": "9) Uncertainty & intervals\n\nParametric intervals (ARIMA/SARIMAX state-space uncertainty).\nQuantile regression (predict ({}(y{t+h}))).\nConformal prediction with rolling splits: valid, distribution-free intervals under exchangeability assumptions adapted per split."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#time-ordered-classification",
    "href": "lecture_slides/08_time_series/08_time_series.html#time-ordered-classification",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "10) Time-ordered classification",
    "text": "10) Time-ordered classification\nExample label: churn in next (k) periods: ( y_t= ).\nSplits: forward-chaining; optional gap to avoid leakage from label construction.\nMetrics: ROC-AUC, PR-AUC; Calibration: reliability curves, Brier score.\n#| echo: true\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n# y_prob = clf.predict_proba(X_test)[:,1]\n# CalibrationDisplay.from_predictions(y_test, y_prob, n_bins=10)"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#monitoring-model-governance",
    "href": "lecture_slides/08_time_series/08_time_series.html#monitoring-model-governance",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "11) Monitoring & model governance",
    "text": "11) Monitoring & model governance\n\nBacktest stability across vintages.\nDrift checks (level, variance, seasonality pattern).\nRecalibration windows; re-tuning cadence.\nChampion–challenger with rolling evaluation dashboards."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#references",
    "href": "lecture_slides/08_time_series/08_time_series.html#references",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "References",
    "text": "References\n\nHyndman & Athanasopoulos, Forecasting: Principles and Practice (Python edition), OTexts (online).\nBianchi, Time Series Analysis with Python (online book + notebooks).\nstatsmodels documentation: ARIMA/SARIMAX, seasonal decomposition.\nscikit-learn: TimeSeriesSplit, model selection, calibration.\nsktime: forecasting/classification unified framework.\nNixtla: StatsForecast, MLForecast.\nLazzeri, Machine Learning for Time Series Forecasting with Python (Wiley).\nAuffarth, Machine Learning for Time-Series with Python (Packt).\nShmueli & Berger, Practical Time Series Forecasting with Python.\nManning/Peixeiro, Time Series Forecasting in Python."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#appendix-core-formulas",
    "href": "lecture_slides/08_time_series/08_time_series.html#appendix-core-formulas",
    "title": "Time Series for Predictive Analytics: An End-to-End Workflow",
    "section": "Appendix: Core formulas",
    "text": "Appendix: Core formulas\n\nMA(q): ( y_t = + _t +  + + q )\nAR(p): ( y_t = _0 + y{t-1} + + p y{t-p} + _t )\nARIMA(p,d,q) and SARIMAX as defined above.\nsMAPE: ( =_t )\nMASE: ( = )\nBrier score: ( =_t (y_t-_t)^2 )"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html",
    "href": "lecture_slides/08_time_series/08_time_series.html",
    "title": "Overview",
    "section": "",
    "text": "This lecture material was developed with reference to Forecasting: Principles and Practice, the Pythonic Way, sktime documentation, Time Series Analysis with Python Book, scikit-learn documentation, and customized for our course-specific learning objectives. You are welcome to use, share, and expand upon this material.\nIf you want to review the Time Series topic we covered in our Business Statistics course, here you can access the lecture slides.\nHave fun!"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#learning-objectives",
    "href": "lecture_slides/08_time_series/08_time_series.html#learning-objectives",
    "title": "Overview",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDifferentiate forecasting from generic supervised prediction.\n\nSpecify correct evaluation protocols for time series."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#problem-framing-two-worlds",
    "href": "lecture_slides/08_time_series/08_time_series.html#problem-framing-two-worlds",
    "title": "Overview",
    "section": "Problem Framing: Two Worlds",
    "text": "Problem Framing: Two Worlds\n\nGeneric Supervised Prediction\n\nPredict a target from feature columns.\nRows assumed exchangeable; order irrelevant.\nStandard random/stratified splits.\nExamples: tabular regression, classification.\n\n\n\nForecasting\n\nPredict future rows of the same variable from past rows.\nTemporal order is intrinsic; rows are not exchangeable.\nTime-aware splits and backtesting required.\nExamples: demand, traffic, finance, sensor data."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#why-we-get-confused",
    "href": "lecture_slides/08_time_series/08_time_series.html#why-we-get-confused",
    "title": "Overview",
    "section": "Why We Get Confused",
    "text": "Why We Get Confused\nBoth output numeric or categorical predictions, but data-generating processes differ.\nTreating forecasting as generic regression ignores time order and future-unknown constraints.\nThis changes feature construction, model interfaces, and evaluation."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#evaluation-protocols",
    "href": "lecture_slides/08_time_series/08_time_series.html#evaluation-protocols",
    "title": "Overview",
    "section": "Evaluation Protocols",
    "text": "Evaluation Protocols\n\nGeneric Regression\n\nRandom/stratified splits commonly used.\nK-fold CV with row shuffling is acceptable.\n\n\n\nForecasting\n\nTemporal train/test split (hold-out by time).\nRolling/expanding backtests across cutpoints/horizons.\nRandom splits cause information leakage and over-optimistic scores."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#common-pitfalls",
    "href": "lecture_slides/08_time_series/08_time_series.html#common-pitfalls",
    "title": "Overview",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nRow shuffling in time series evaluation.\nAd-hoc lagging/windowing with hidden hyperparameters.\nProducing repeated nowcasts instead of true multi-step forecasts.\nFailing to map predictions back to a properly indexed horizon."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#load-common-libraries-and-settings",
    "href": "lecture_slides/08_time_series/08_time_series.html#load-common-libraries-and-settings",
    "title": "Overview",
    "section": "Load common libraries and settings",
    "text": "Load common libraries and settings\n\n!pip -q install utilsforecast statsmodels seaborn matplotlib cycler\n\n\n!pip install fpppy\n\nRequirement already satisfied: fpppy in /usr/local/lib/python3.12/dist-packages (0.0.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from fpppy) (3.10.0)\nRequirement already satisfied: pre-commit in /usr/local/lib/python3.12/dist-packages (from fpppy) (4.3.0)\nRequirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from fpppy) (8.4.2)\nRequirement already satisfied: pytest-cov in /usr/local/lib/python3.12/dist-packages (from fpppy) (7.0.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from fpppy) (1.6.1)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from fpppy) (0.14.5)\nRequirement already satisfied: utilsforecast in /usr/local/lib/python3.12/dist-packages (from fpppy) (0.2.14)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (1.4.9)\nRequirement already satisfied: numpy&gt;=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (2.0.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (25.0)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (11.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (2.9.0.post0)\nRequirement already satisfied: cfgv&gt;=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (3.4.0)\nRequirement already satisfied: identify&gt;=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (2.6.15)\nRequirement already satisfied: nodeenv&gt;=0.11.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (1.9.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (6.0.3)\nRequirement already satisfied: virtualenv&gt;=20.10.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (20.35.3)\nRequirement already satisfied: iniconfig&gt;=1 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (2.1.0)\nRequirement already satisfied: pluggy&lt;2,&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (1.6.0)\nRequirement already satisfied: pygments&gt;=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (2.19.2)\nRequirement already satisfied: coverage&gt;=7.10.6 in /usr/local/lib/python3.12/dist-packages (from coverage[toml]&gt;=7.10.6-&gt;pytest-cov-&gt;fpppy) (7.10.7)\nRequirement already satisfied: scipy&gt;=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (1.15.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (1.5.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (3.6.0)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels-&gt;fpppy) (2.2.2)\nRequirement already satisfied: patsy&gt;=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels-&gt;fpppy) (1.0.1)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels-&gt;fpppy) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels-&gt;fpppy) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;fpppy) (1.17.0)\nRequirement already satisfied: distlib&lt;1,&gt;=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (0.4.0)\nRequirement already satisfied: filelock&lt;4,&gt;=3.12.2 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (3.20.0)\nRequirement already satisfied: platformdirs&lt;5,&gt;=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (4.5.0)\n\n\n\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    category=UserWarning,\n    message=\".*FigureCanvasAgg is non-interactive.*\"\n)\nimport os\nos.environ[\"NIXTLA_ID_AS_COL\"] = \"true\"\nimport numpy as np\nnp.set_printoptions(suppress=True)\nnp.random.seed(1)\nimport random\nrandom.seed(1)\nimport pandas as pd\npd.set_option(\"max_colwidth\", 100)\npd.set_option(\"display.precision\", 3)\nfrom utilsforecast.plotting import plot_series as plot_series_utils\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nplt.rcParams.update({\n    \"figure.figsize\": (8, 5),\n    \"figure.dpi\": 100,\n    \"savefig.dpi\": 300,\n    \"figure.constrained_layout.use\": True,\n    \"axes.titlesize\": 12,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n    \"legend.title_fontsize\": 10,\n})\nimport matplotlib as mpl\nfrom cycler import cycler\n# mpl.rcParams['axes.prop_cycle'] = cycler(color=[\"#000000\", \"#000000\"])\nfrom fpppy.utils import plot_series\n\n# Then, once anywhere before plotting:\nimport matplotlib as mpl\nmpl.rcParams['axes.prop_cycle'] = mpl.rcParamsDefault['axes.prop_cycle']\n\nfrom scipy.stats import pearsonr\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n\nimport matplotlib as mpl\n# Restore the default Tableau 10 cycle\nmpl.rcParams['axes.prop_cycle'] = mpl.rcParamsDefault['axes.prop_cycle']"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#introduction",
    "href": "lecture_slides/08_time_series/08_time_series.html#introduction",
    "title": "Overview",
    "section": "Introduction",
    "text": "Introduction\nIn any data analysis workflow, the first step is EDA with visualization. Plotting the data reveals critical insights such as:\n\nPatterns and trends across time\nOutliers and anomalies that may distort interpretation\nStructural changes and seasonality in time-dependent data\nRelationships among variables\n\nThese visual features guide the choice and specification of forecasting models. Effective forecasting depends on understanding what the data reveal through visualization."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#time-series-plots",
    "href": "lecture_slides/08_time_series/08_time_series.html#time-series-plots",
    "title": "Overview",
    "section": "Time Series Plots",
    "text": "Time Series Plots\nFor time series data, the most fundamental visualization is the time plot.\nIn a time plot, each observation is displayed against its corresponding time point, and consecutive observations are connected by straight lines to reveal patterns, trends, or irregularities over time.\nThis type of graph enables analysts to identify:\n\nLong-term trends\nSeasonal fluctuations\nSudden shifts or anomalies\n\nFor example, a time plot of weekly economy passenger load between Melbourne and Sydney illustrates how air travel demand evolves through time.\n\nAnsett airlines economy class: Melbourne-Sydney\n\nansett = pd.read_csv(\"ansett.csv\")\nansett[\"ds\"] = pd.to_datetime(ansett[\"ds\"])\nmelsyd_economy = ansett.query('Airports == \"MEL-SYD\" & Class == \"Economy\"').copy()\nmelsyd_economy[\"y\"] = melsyd_economy[\"y\"] / 1000\nsns.set_style(\"whitegrid\")\nplot_series(df=melsyd_economy,\n            id_col=\"Airports\",\n            time_col=\"ds\",\n            target_col=\"y\",\n            ylabel=\"Passengers ('000)\",\n            xlabel=\"Week [1W]\",\n            title=\"Ansett airlines economy class: Melbourne-Sydney\")\n\n\n\n\n\n\n\n\n\n\nAnsett airlines economy class: Melbourne-Sydney\nThis plot reveals some interesting features.\n\nThere was a period in 1989 when no passengers were carried — this was due to an industrial dispute.\nThere was a period of reduced load in 1992. This was due to a trial in which some economy class seats were replaced by business class seats.\nA large increase in passenger load occurred in the second half of 1991.\nThere are some large dips in load around the start of each year. These are due to holiday effects.\nThere is a long-term fluctuation in the level of the series which increases during 1987, decreases in 1989, and increases again through 1990 and 1991.\n\nAny model will need to take all these features into account in order to effectively forecast the passenger load into the future.\n\n\nAustralian antidiabetic drug sales\n\ntotal_cost_df = pd.read_csv(\"total_cost_df.csv\", parse_dates=[\"Month\"])\ntotal_cost_df[\"unique_id\"] = \"total_cost\"\n\nsns.set_style(\"whitegrid\")\nplot_series(total_cost_df,\n            id_col=\"unique_id\",\n            time_col=\"Month\",\n            target_col=\"Cost\",\n            xlabel=\"Month [1M]\",\n            ylabel=\"$ (millions)\",\n            title=\"Australian antidiabetic drug sales\")\n\n\n\n\n\n\n\n\n\n\nAustralian antidiabetic drug sales\nThis example shows a clear upward trend combined with a strong seasonal component whose amplitude increases with the overall level of the series.\nA sharp drop at the beginning of each year results from a government subsidisation policy, which encourages patients to stockpile medications before year-end.\nWhen forecasting such data, the model must capture:\n\nThe seasonal pattern and its changing magnitude\nThe gradual evolution of the trend over time"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#trend",
    "href": "lecture_slides/08_time_series/08_time_series.html#trend",
    "title": "Overview",
    "section": "Trend",
    "text": "Trend\nA trend represents a long-term increase or decrease in the data.\n\nIt may be linear or nonlinear.\nA trend can change direction, shifting from growth to decline or vice versa.\nExample: The antidiabetic drug sales (Figure 2.2) show a steady upward trend."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#seasonal",
    "href": "lecture_slides/08_time_series/08_time_series.html#seasonal",
    "title": "Overview",
    "section": "Seasonal",
    "text": "Seasonal\nA seasonal pattern arises from systematic calendar-based effects such as:\n\nTime of year\nDay of week\nHour of day\n\nSeasonality has a fixed and known period, and a series can exhibit multiple seasonalities (e.g., yearly and weekly). Example: The monthly antidiabetic drug sales display seasonality influenced by year-end cost changes."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#cyclic",
    "href": "lecture_slides/08_time_series/08_time_series.html#cyclic",
    "title": "Overview",
    "section": "Cyclic",
    "text": "Cyclic\nA cycle involves rises and falls with no fixed frequency, often linked to economic conditions such as the business cycle.\n\nCycles usually last two years or more.\nThe magnitude and duration of cycles are less regular than seasonal patterns."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#key-distinction",
    "href": "lecture_slides/08_time_series/08_time_series.html#key-distinction",
    "title": "Overview",
    "section": "Key Distinction",
    "text": "Key Distinction\n\nIf fluctuations occur at a fixed, calendar-based frequency, they are seasonal.\nIf they occur at a variable frequency, they are cyclic."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#practical-insight",
    "href": "lecture_slides/08_time_series/08_time_series.html#practical-insight",
    "title": "Overview",
    "section": "Practical Insight",
    "text": "Practical Insight\nMost real-world time series combine trend, seasonal, and cyclic components.\nWhen selecting a forecasting model, it is essential to first identify these patterns and then apply a method capable of capturing their combined effects."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#four-examples-of-time-series-showing-different-patterns",
    "href": "lecture_slides/08_time_series/08_time_series.html#four-examples-of-time-series-showing-different-patterns",
    "title": "Overview",
    "section": "Four examples of time series showing different patterns",
    "text": "Four examples of time series showing different patterns\n\n\n\n\n\nXXX include fig fourexamples-output-1 XXXX"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#four-examples-of-time-series-showing-different-patterns-1",
    "href": "lecture_slides/08_time_series/08_time_series.html#four-examples-of-time-series-showing-different-patterns-1",
    "title": "Overview",
    "section": "Four Examples of Time Series Showing Different Patterns",
    "text": "Four Examples of Time Series Showing Different Patterns\n\nMonthly Housing Sales (Top Left)\n\nExhibit strong seasonality within each year.\nDisplay cyclic behavior with a period of approximately 6–10 years.\nShow no clear long-term trend over the observed period.\n\nUS Treasury Bill Contracts (Top Right)\n\nReflect data from the Chicago market over 100 trading days in 1981.\nShow no seasonality, but a clear downward trend.\nWith a longer horizon, this trend could represent part of a larger economic cycle.\n\nAustralian Quarterly Electricity Production (Bottom Left)\n\nDemonstrates a strong upward trend coupled with pronounced seasonality.\nNo visible cyclic pattern, suggesting stable periodicity tied to seasonal factors.\n\nDaily Change in Google Closing Stock Price (Bottom Right)\n\nDisplays no trend, no seasonality, and no cyclic behavior.\nCharacterized by random fluctuations, lacking predictable patterns suitable for traditional forecasting."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#lag-plots",
    "href": "lecture_slides/08_time_series/08_time_series.html#lag-plots",
    "title": "Overview",
    "section": "Lag plots",
    "text": "Lag plots\n\nLagged scatterplots for quarterly beer production.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 1) Read and parse dates\naus_production = pd.read_csv(\"aus_production.csv\", parse_dates=[\"ds\"])\n\n# 2) Filter by calendar year (not by string/int compare)\nrecent_production = (\n    aus_production.loc[aus_production[\"ds\"].dt.year &gt;= 2000, [\"ds\", \"Beer\"]]\n    .rename(columns={\"Beer\": \"y\"})\n    .copy()\n)\n\n# 3) Time-derived fields\nrecent_production[\"Quarter\"] = recent_production[\"ds\"].dt.quarter\n\n# 4) Build lags\nfor lag in range(1, 10):\n    recent_production[f\"lag_{lag}\"] = recent_production[\"y\"].shift(lag)\n\nlags = [f\"lag_{i}\" for i in range(1, 10)]\n\n# 5) Drop rows with NaNs (from shifting) for plotting and limits\nplot_df = recent_production[[\"y\", \"Quarter\", *lags]].dropna()\n\n# 6) Limits ignoring NaNs\nlims = [\n    np.nanmin([plot_df[lag].min() for lag in lags] + [plot_df[\"y\"].min()]),\n    np.nanmax([plot_df[lag].max() for lag in lags] + [plot_df[\"y\"].max()]),\n]\n\n# 7) Lag plots\nfig, axes = plt.subplots(3, 3, figsize=(8, 8))\nfor ax, lag in zip(axes.flatten(), lags):\n    ax.scatter(plot_df[lag], plot_df[\"y\"], c=plot_df[\"Quarter\"], cmap=\"viridis\")\n    ax.plot(lims, lims, \"grey\", linestyle=\"--\", linewidth=1)\n    ax.set_title(lag)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipython-input-87298660.py:39: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nLagged Scatterplots for Quarterly Beer Production\nThis plot presents scatterplots of quarterly Australian beer production, where the horizontal axis represents lagged values of the series.\nEach graph displays \\(y_t\\) plotted against \\(y_{t-k}\\) for various lag values \\(k\\).\n\nColor coding indicates the quarter corresponding to the observation on the vertical axis.\nA strong positive relationship appears at lags 4 and 8, revealing the seasonal pattern that repeats annually.\nNegative relationships at lags 2 and 6 occur because peaks (Q4) are compared with troughs (Q2), emphasizing the alternation between seasonal highs and lows."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#autocorrelation",
    "href": "lecture_slides/08_time_series/08_time_series.html#autocorrelation",
    "title": "Overview",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nAutocorrelation quantifies the linear relationship between lagged observations of the same time series, analogous to how standard correlation measures relationships between two different variables.\nFor each lag \\(k\\), there is an autocorrelation coefficient \\(r_k\\):\n\n\\(r_1\\) measures the relationship between \\(y_t\\) and \\(y_{t-1}\\)\n\\(r_2\\) measures the relationship between \\(y_t\\) and \\(y_{t-2}\\)\nand so on\n\nThe general formula for the \\(k\\)-th autocorrelation is:\n\\[\nr_k ;=;\n\\frac{\\displaystyle \\sum_{t=k+1}^{T} (y_t-\\bar{y})(y_{t-k}-\\bar{y})}\n{\\displaystyle \\sum_{t=1}^{T} (y_t-\\bar{y})^2},\n\\]\nwhere \\(T\\) denotes the length of the time series.\nThe complete set of autocorrelation coefficients forms the Autocorrelation Function (ACF), which summarizes how current values relate to past values across different lags.\nFor instance, in the beer production dataset, the ACF can be computed directly using the acf() function.\n\nacf_df = pd.DataFrame(\n    {\"Lag\": range(10), \"ACF\": sm.tsa.acf(recent_production[\"y\"].dropna(), nlags=9,\n        fft=False, bartlett_confint=False)}\n).set_index(\"Lag\")\nacf_df[1:]\n\n\n    \n\n\n\n\n\n\nACF\n\n\nLag\n\n\n\n\n\n1\n-0.053\n\n\n2\n-0.758\n\n\n3\n-0.026\n\n\n4\n0.802\n\n\n5\n-0.077\n\n\n6\n-0.657\n\n\n7\n0.001\n\n\n8\n0.707\n\n\n9\n-0.089\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nThe values in the ACF column are \\(r_1, \\ldots, r_9\\), corresponding to the nine scatterplots in Figure 2.19. We usually plot the ACF to see how the correlations change with the lag \\(k\\). The plot is sometimes known as a correlogram.\n\nAutocorrelation function for quarterly beer production.\n\nfig, ax = plt.subplots()\nsns.set_style(\"whitegrid\")\nplot_acf(recent_production[\"y\"].dropna(), lags=16, ax=ax,\n         zero=False, bartlett_confint=False, auto_ylims=True)\nax.set_title(\"Autocorrelation Function for Beer\")\nax.set_xlabel(\"Lags\")\nax.set_ylabel(\"ACF\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAutocorrelation function for quarterly beer production.\n\n\\(r_4\\) is higher than for the other lags. This is due to the seasonal pattern in the data: the peaks tend to be four quarters apart and the troughs tend to be four quarters apart.\n\\(r_2\\) is more negative than for the other lags because troughs tend to be two quarters behind peaks.\nThe grey area indicate whether the correlations are significantly different from zero (as explained in Section 2.9).\n\n\n\nTrend and Seasonality in ACF Plots\nThe Autocorrelation Function (ACF) reveals distinct patterns that help identify trend and seasonality in time series data.\n\nTrend Effect:\nWhen a series exhibits a trend, nearby observations in time are also close in value. Consequently, autocorrelations for small lags are large and positive, and they decline gradually as the lag increases.\nSeasonality Effect:\nWhen a series is seasonal, autocorrelations spike at multiples of the seasonal period (e.g., lags 4, 8, 12 for quarterly data).\nCombined Effect:\nWhen both trend and seasonality are present, the ACF displays a slow decay due to the trend and a scalloped or wavelike pattern due to seasonality.\n\n\n\nAutocorrelation function for monthly antidiabetic drug sales in Australia\n\ntotal_cost_df = pd.read_csv(\"total_cost_df.csv\", parse_dates=[\"Month\"])\nfig, ax = plt.subplots()\nsns.set_style(\"whitegrid\")\nplot_acf(total_cost_df[\"Cost\"], lags=48, ax=ax,\n         zero=False, bartlett_confint=False, auto_ylims=True)\nax.set_title(\"Australian antidiabetic drug sales\")\nax.set_xlabel(\"Lags\")\nax.set_ylabel(\"ACF\")\nax.set_ylim(bottom=-0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAutocorrelation function for monthly antidiabetic drug sales in Australia\n\nThe slow decline in autocorrelation reflects the trend\nThe repeating scalloped shape indicates seasonal periodicity."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#white-noise",
    "href": "lecture_slides/08_time_series/08_time_series.html#white-noise",
    "title": "Overview",
    "section": "White noise",
    "text": "White noise\nTime series that show no autocorrelation are called white noise.\n\nWhite noise series\n\nnp.random.seed(30)\ny = pd.DataFrame({\"wn\": np.random.normal(0, 1, 50), \"ds\": np.arange(1, 51)})\ny[\"unique_id\"] = \"wn\"\nsns.set_style(\"whitegrid\")\nplot_series(y, target_col=\"wn\", xlabel=\"sample [1]\", title=\"White noise\")\n\n\n\n\n\n\n\n\n\n\nAutocorrelation function for the white noise series\n\nfig, ax = plt.subplots()\nsns.set_style(\"whitegrid\")\nplot_acf(y[\"wn\"], lags=16, ax=ax, zero=False, bartlett_confint=False, auto_ylims=True)\nax.set_title(\"White Noise\")\nax.set_xlabel(\"Lag[1]\")\nax.set_ylabel(\"ACF\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAutocorrelation Function for the White Noise Series\nIn a white noise series, each observation is independent and identically distributed, meaning there is no predictable pattern or correlation across time.\n\nThe autocorrelations are expected to be close to zero, though not exactly zero due to random variation.\nFor such a series, approximately 95% of ACF spikes should lie within the bounds, \\(\\pm \\frac{1.96}{\\sqrt{T}}\\), where \\(T\\) is the length of the time series.\nThese confidence bounds help determine if a series is consistent with white noise.\nIf more than 5% of spikes fall outside these limits, or if large spikes appear beyond them, the series likely exhibits non-random structure.\n\nIn the plot, all but the first autocorrelation fall within the limits, confirming the data behave as white noise."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#load-common-libraries-and-settings-1",
    "href": "lecture_slides/08_time_series/08_time_series.html#load-common-libraries-and-settings-1",
    "title": "Overview",
    "section": "Load common libraries and settings",
    "text": "Load common libraries and settings\n\n!pip -q install utilsforecast statsmodels seaborn matplotlib cycler\n!pip install fpppy\n# Ensure these are available:\n# - boxcox() function — For applying Box–Cox transformations\n# - boxcox_lambda() function — For finding optimal Box–Cox transformation parameters\n# - plot_series() utility — For creating time series visualizations\n\nimport sys, importlib, subprocess\n\ndef _pip_install(pkg):\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", pkg], check=True)\n\n# 1) coreforecast: boxcox, boxcox_lambda\ntry:\n    from coreforecast.scalers import boxcox, boxcox_lambda\nexcept Exception:\n    _pip_install(\"coreforecast\")\n    from coreforecast.scalers import boxcox, boxcox_lambda\n\n# 2) plot_series utility: prefer fpppy.utils, fallback to utilsforecast.plotting\ntry:\n    from fpppy.utils import plot_series  # if you have the FPP(Pythonic) helpers\nexcept Exception:\n    try:\n        from utilsforecast.plotting import plot_series  # fallback\n    except Exception:\n        _pip_install(\"utilsforecast matplotlib pandas numpy\")\n        from utilsforecast.plotting import plot_series\n\n# --- quick smoke test / confirmation ---\ndef _has_callable(obj):\n    try:\n        return callable(obj)\n    except Exception:\n        return False\n\nprint(\"• boxcox() function —\", \"OK\" if _has_callable(boxcox) else \"MISSING\")\nprint(\"• boxcox_lambda() function —\", \"OK\" if _has_callable(boxcox_lambda) else \"MISSING\")\nprint(\"• plot_series() utility —\", \"OK\" if _has_callable(plot_series) else \"MISSING\")\n\n# Optional: tiny demo to verify they actually run (safe no-op)\nimport numpy as np\ny = np.array([1.0, 2.0, 4.0, 8.0])\nlam = boxcox_lambda(y, method=\"guerrero\", season_length=2)\n_ = boxcox(y, lam)  # should not error\n\nRequirement already satisfied: fpppy in /usr/local/lib/python3.12/dist-packages (0.0.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from fpppy) (3.10.0)\nRequirement already satisfied: pre-commit in /usr/local/lib/python3.12/dist-packages (from fpppy) (4.3.0)\nRequirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from fpppy) (8.4.2)\nRequirement already satisfied: pytest-cov in /usr/local/lib/python3.12/dist-packages (from fpppy) (7.0.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from fpppy) (1.6.1)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from fpppy) (0.14.5)\nRequirement already satisfied: utilsforecast in /usr/local/lib/python3.12/dist-packages (from fpppy) (0.2.14)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (1.4.9)\nRequirement already satisfied: numpy&gt;=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (2.0.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (25.0)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (11.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (2.9.0.post0)\nRequirement already satisfied: cfgv&gt;=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (3.4.0)\nRequirement already satisfied: identify&gt;=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (2.6.15)\nRequirement already satisfied: nodeenv&gt;=0.11.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (1.9.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (6.0.3)\nRequirement already satisfied: virtualenv&gt;=20.10.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (20.35.3)\nRequirement already satisfied: iniconfig&gt;=1 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (2.1.0)\nRequirement already satisfied: pluggy&lt;2,&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (1.6.0)\nRequirement already satisfied: pygments&gt;=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (2.19.2)\nRequirement already satisfied: coverage&gt;=7.10.6 in /usr/local/lib/python3.12/dist-packages (from coverage[toml]&gt;=7.10.6-&gt;pytest-cov-&gt;fpppy) (7.10.7)\nRequirement already satisfied: scipy&gt;=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (1.15.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (1.5.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (3.6.0)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels-&gt;fpppy) (2.2.2)\nRequirement already satisfied: patsy&gt;=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels-&gt;fpppy) (1.0.1)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels-&gt;fpppy) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels-&gt;fpppy) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;fpppy) (1.17.0)\nRequirement already satisfied: distlib&lt;1,&gt;=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (0.4.0)\nRequirement already satisfied: filelock&lt;4,&gt;=3.12.2 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (3.20.0)\nRequirement already satisfied: platformdirs&lt;5,&gt;=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (4.5.0)\n• boxcox() function — OK\n• boxcox_lambda() function — OK\n• plot_series() utility — OK\n\n\n\n\nimport sys, subprocess\ndef pip_install(*pkgs):\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", *pkgs], check=True)\n\n# tsfeatures is published by Nixtla. These are commonly used together:\npip_install(\"tsfeatures\", \"statsforecast\", \"coreforecast\")\n\n\n\n# Load common libraries and settings\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    category=UserWarning,\n    message=\".*FigureCanvasAgg is non-interactive.*\"\n)\n\nimport os\nos.environ[\"NIXTLA_ID_AS_COL\"] = \"true\"\n\nimport numpy as np\nnp.set_printoptions(suppress=True)\nnp.random.seed(1)\n\nimport random\nrandom.seed(1)\n\nimport pandas as pd\npd.set_option(\"max_colwidth\", 100)\npd.set_option(\"display.precision\", 3)\n\nfrom utilsforecast.plotting import plot_series as plot_series_utils\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nplt.rcParams.update({\n    \"figure.figsize\": (8, 5),\n    \"figure.dpi\": 100,\n    \"savefig.dpi\": 300,\n    \"figure.constrained_layout.use\": True,\n    \"axes.titlesize\": 12,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n    \"legend.title_fontsize\": 10,\n})\nimport matplotlib as mpl\nfrom cycler import cycler\n#mpl.rcParams['axes.prop_cycle'] = cycler(color=[\"#000000\", \"#000000\"])\n\n# Then, once anywhere before plotting:\nimport matplotlib as mpl\nmpl.rcParams['axes.prop_cycle'] = mpl.rcParamsDefault['axes.prop_cycle']\n\nfrom fpppy.utils import plot_series"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#introduction-1",
    "href": "lecture_slides/08_time_series/08_time_series.html#introduction-1",
    "title": "Overview",
    "section": "Introduction",
    "text": "Introduction\nTime series data often display multiple overlapping patterns, which can be better understood by decomposing the series into separate components representing distinct underlying behaviors."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#introduction-2",
    "href": "lecture_slides/08_time_series/08_time_series.html#introduction-2",
    "title": "Overview",
    "section": "Introduction",
    "text": "Introduction\nWe typically identify three main pattern types:\n\nTrend — long-term movement or direction.\nSeasonality — systematic, calendar-based fluctuations.\nCycles — irregular long-term oscillations, often linked to economic or structural forces."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#introduction-3",
    "href": "lecture_slides/08_time_series/08_time_series.html#introduction-3",
    "title": "Overview",
    "section": "Introduction",
    "text": "Introduction\nIn decomposition, the trend and cycle are usually combined into a single trend–cycle component (commonly referred to simply as the trend). Thus, a time series can be expressed as consisting of:\n\nA trend–cycle component,\nA seasonal component, and\nA remainder component, which captures random or irregular variations.\n\nFor high-frequency data (e.g., daily observations), there may be multiple seasonal components corresponding to different periodicities (daily, weekly, annual, etc.)."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#introduction-4",
    "href": "lecture_slides/08_time_series/08_time_series.html#introduction-4",
    "title": "Overview",
    "section": "Introduction",
    "text": "Introduction\nDecomposition serves two key purposes:\n\nTo enhance interpretability by revealing structural elements of the data.\nTo improve forecasting accuracy by modeling each component separately.\n\nBefore performing decomposition, it is often useful to transform or adjust the series to simplify interpretation and ensure that subsequent analyses are more reliable and stable.\n\n\n# Load additional libraries\nimport calendar\nfrom tsfeatures import *\nfrom statsmodels.tsa.seasonal import STL\nfrom statsforecast import StatsForecast\nfrom coreforecast.scalers import boxcox, boxcox_lambda\nfrom statsmodels.tsa.seasonal import seasonal_decompose"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#time-series-components",
    "href": "lecture_slides/08_time_series/08_time_series.html#time-series-components",
    "title": "Overview",
    "section": "Time Series Components",
    "text": "Time Series Components\nIn time series analysis, the data can be expressed as a combination of underlying components that represent different types of structure.\n\nAdditive Decomposition\n\\[\ny_t = S_t + T_t + R_t,\n\\]\nwhere:\n\n\\(y_t\\) = observed value at time \\(t\\),\n\\(S_t\\) = seasonal component,\n\\(T_t\\) = trend–cycle component,\n\\(R_t\\) = remainder (irregular) component.\n\nThe additive model is appropriate when the amplitude of seasonal and random variations remains constant regardless of the level of the series.\n\n\nMultiplicative Decomposition\n\\[\ny_t = S_t \\times T_t \\times R_t.\n\\]\nThe multiplicative model is suitable when seasonal or irregular variations increase or decrease proportionally with the level of the series — a pattern commonly observed in economic data.\n\n\nLog Transformation Equivalence\nA multiplicative relationship can be converted into an additive one by applying a logarithmic transformation:\n\\[\ny_t = S_t \\times T_t \\times R_t\n\\Longleftrightarrow\n\\log y_t = \\log S_t + \\log T_t + \\log R_t.\n\\]\nThus, stabilizing variance via a transformation enables the use of additive decomposition even for series with proportional variability.\n\n\nExample: Employment in the US Retail Sector\nBefore exploring specific decomposition methods for estimating the components \\(S_t\\), \\(T_t\\), and \\(R_t\\), it is useful to examine a concrete example.\nThe dataset under analysis represents the total monthly number of persons employed (in thousands) in the US retail sector since 1990.\nBy decomposing this series (as illustrated in Figure), we can observe how:\n\n\\(T_t\\) captures the long-term employment trend,\n\\(S_t\\) isolates recurring seasonal hiring patterns, and\n\\(R_t\\) reflects irregular fluctuations not explained by the other components.\n\nThis example provides a practical foundation for understanding how decomposition separates distinct sources of variation in time series data.\n\n\n# US retail employment data and basic plot\nus_employment = pd.read_csv(\"us_employment.csv\", parse_dates=[\"ds\"])\nus_retail_employment = us_employment.query('(unique_id == \"Retail Trade\") & (ds &gt;= \"1990-01-01\")')\nsns.set_style(\"whitegrid\")\nplot_series(us_retail_employment, xlabel=\"Month [1M]\", ylabel=\"Persons (thousands)\", title=\"Total employment in US retail\")\n\n\n\n\n\n\n\n\n\n\nExample: Seasonal and Trend decomposition using Loess (STL) decomposition method\n\n\n# STL decomposition (default)\nstl = STL(us_retail_employment[\"y\"], period=12)\nres = stl.fit()\n\ndcmp = pd.DataFrame({\n    \"ds\": us_retail_employment[\"ds\"],\n    \"data\": us_retail_employment[\"y\"],\n    \"trend\": res.trend,\n    \"seasonal\": res.seasonal,\n    \"remainder\": res.resid\n}).reset_index(drop=True)\n\ndcmp.head()\n\n\n    \n\n\n\n\n\n\nds\ndata\ntrend\nseasonal\nremainder\n\n\n\n\n0\n1990-01-01\n13255.8\n13296.249\n-3.700\n-36.749\n\n\n1\n1990-02-01\n12966.3\n13276.085\n-288.398\n-21.387\n\n\n2\n1990-03-01\n12938.2\n13255.663\n-306.658\n-10.805\n\n\n3\n1990-04-01\n13012.3\n13234.986\n-235.775\n13.089\n\n\n4\n1990-05-01\n13108.3\n13214.071\n-115.399\n9.628\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n\nExample: Seasonal and Trend decomposition using Loess (STL) decomposition method\nThe output shows the components of an STL decomposition. The original data is shown (as data), followed by the estimated components.\nThe trend column (containing the trend-cycle \\(T_t\\)) follows the overall movement of the series, ignoring any seasonality and random fluctuations.\n\n\n# Plot trend vs data\nfig, ax = plt.subplots()\nsns.set_style(\"whitegrid\")\nsns.lineplot(data=dcmp, x=\"ds\", y=\"data\", label=\"Data\", color=\"gray\")\nsns.lineplot(data=dcmp, x=\"ds\", y=\"trend\", label=\"Trend\", color=\"#D55E00\")\nax.set_title(\"Total employment in US retail\")\nax.set_xlabel(\"Month [1M]\")\nax.set_ylabel(\"Persons (thousands)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: Seasonal and Trend decomposition using Loess (STL) decomposition method\n\n\n# Plot all components\nfig, axes = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(8, 6))\nsns.set_style(\"whitegrid\")\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"data\", ax=axes[0])\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"trend\", ax=axes[1])\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"seasonal\", ax=axes[2])\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"remainder\", ax=axes[3])\naxes[0].set_ylabel(\"Employed\")\naxes[1].set_ylabel(\"trend\")\naxes[2].set_ylabel(\"seasonal\")\naxes[3].set_ylabel(\"remainder\")\nfig.suptitle(\"STL decomposition\")\nfig.subplots_adjust(top=0.90)\nfig.text(0.5, 0.95, \"Employed = trend + seasonal + remainder\", ha='center')\nplt.xlabel(\"\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: Seasonal and Trend Decomposition Using Loess (STL)\nThe STL decomposition method separates a time series into three main components — seasonal, trend–cycle, and remainder — which together reconstruct the original data.\nIn the figure, the bottom three panels display each component individually, while the top panel shows the original series.\nKey observations:\n\nThe seasonal component (\\(S_t\\)) varies gradually over time — consecutive years exhibit similar patterns, but seasonality evolves when comparing distant years.\nThe trend–cycle component (\\(T_t\\)) captures the underlying direction of the data, smoothing short-term fluctuations.\nThe remainder component (\\(R_t\\)) represents unexplained irregularities, i.e., what remains after removing both the trend–cycle and seasonal effects.\n\nThese components together illustrate how STL provides a flexible, data-driven approach to isolating dynamic seasonal and trend structures in time series data."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#seasonally-adjusted-data",
    "href": "lecture_slides/08_time_series/08_time_series.html#seasonally-adjusted-data",
    "title": "Overview",
    "section": "Seasonally adjusted data",
    "text": "Seasonally adjusted data\nIf the seasonal component is removed from the original data, the resulting values are the “seasonally adjusted” data. For an additive decomposition, the seasonally adjusted data are given by \\(y_t - S_t\\), and for multiplicative data, the seasonally adjusted values are obtained using \\(y_t / S_t\\).\n\n\n# Seasonally adjusted series\ndcmp[\"season_adjust\"] = dcmp[\"data\"] - dcmp[\"seasonal\"]\n\nfig, ax = plt.subplots()\nsns.set_style(\"whitegrid\")\nsns.lineplot(data=dcmp, x=\"ds\", y=\"data\", label=\"Data\", color=\"gray\")\nsns.lineplot(data=dcmp, x=\"ds\", y=\"season_adjust\", label=\"Seasonally adjusted data\", color=\"#0072B2\")\nax.set_title(\"Total employment in US retail\")\nax.set_xlabel(\"Month [1M]\")\nax.set_ylabel(\"Persons (thousands)\")\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#seasonally-adjusted-data-1",
    "href": "lecture_slides/08_time_series/08_time_series.html#seasonally-adjusted-data-1",
    "title": "Overview",
    "section": "Seasonally Adjusted Data",
    "text": "Seasonally Adjusted Data\nWhen seasonal variation is not the focus of analysis, it is often useful to work with a seasonally adjusted series, which removes predictable seasonal effects.\nFor example, monthly unemployment figures are typically seasonally adjusted to reveal changes in the underlying economic conditions rather than regular seasonal patterns.\n\nAn increase in unemployment due to school leavers entering the labor market represents seasonal variation.\nAn increase due to an economic recession reflects non-seasonal variation, which analysts seek to understand.\n\nBecause of this distinction, many economic indicators, including employment data, are routinely seasonally adjusted before analysis.\n\nKey Insight\nA seasonally adjusted series combines the trend–cycle (\\(T_t\\)) and remainder (\\(R_t\\)) components:\n\\[\n\\text{Seasonally Adjusted Series} = T_t + R_t\n\\]\nHowever, such data are not perfectly smooth, and short-term fluctuations may appear misleading.\n\nUse seasonally adjusted data to examine broad economic patterns.\nUse the trend–cycle component to identify turning points or directional changes with greater reliability."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#moving-averages",
    "href": "lecture_slides/08_time_series/08_time_series.html#moving-averages",
    "title": "Overview",
    "section": "Moving Averages",
    "text": "Moving Averages\nThe classical method of time series decomposition, developed in the 1920s and widely used through the 1950s, remains foundational in modern decomposition techniques.\nThe first step in this classical approach is to estimate the trend–cycle component using a moving average method, which smooths out short-term fluctuations to reveal long-term patterns.\n\nMoving Average Smoothing\nA moving average of order \\(m\\) is defined as:\n\\[\n\\hat{T}*t = \\frac{1}{m}\\sum*{j=-k}^{k} y_{t+j},\n\\]\nwhere \\(m = 2k + 1\\).\nThis means that the estimated trend–cycle \\(\\hat{T}_t\\) at time \\(t\\) is obtained by averaging observations within \\(k\\) periods on either side of \\(t\\).\n\n\nInterpretation\n\nNearby observations are typically similar in value, so averaging them reduces random noise.\nThe result is a smoothed version of the original series, emphasizing the underlying trend–cycle.\nSuch a moving average is referred to as an \\(m\\)-MA, or a moving average of order \\(m\\).\n\n\n\nExample: Australian exports of goods and services (as % of GDP) from 1960–2017\n\n# Moving average smoothing example: Australian exports\nglobal_economy = pd.read_csv(\"global_economy.csv\")\naus_exports = global_economy.query('unique_id == \"Australia\"')[[\"unique_id\", \"ds\", \"Exports\"]]\n\nsns.set_style(\"whitegrid\")\nplot_series(aus_exports, target_col=\"Exports\", xlabel=\"Year [1Y]\", ylabel=\"% of GDP\",\n            title=\"Total Australian exports\")\n\n\n\n\n\n\n\n\n\n\n\nExample: Australian exports of goods and services (as % of GDP) from 1960–2017\n\n# --- Moving average smoothing example: Australian exports (table like the screenshot) ---\n\n# Keep Australia, year, and exports\naus_exports = (\n    global_economy\n    .query('unique_id == \"Australia\"')[[\"ds\", \"Exports\"]]\n    .rename(columns={\"ds\": \"Year\"})\n    .reset_index(drop=True)\n)\n\n# 5-term centered moving average\naus_exports[\"5-MA\"] = (\n    aus_exports[\"Exports\"]\n    .rolling(window=5, center=True)\n    .mean()\n)\n\n# Round for display to match the printed table\ntbl = aus_exports.copy()\ntbl[\"Exports\"] = tbl[\"Exports\"].round(2)\ntbl[\"5-MA\"]    = tbl[\"5-MA\"].round(2)\n\n# Select the same “head … tail” view shown in the chapter:\nhead_part = tbl.iloc[:8]                      # 1960–1967\ntail_part = tbl.iloc[-8:]                     # 2010–2017\n\n# Insert an ellipsis row between head and tail\nellipsis = pd.DataFrame([{\"Year\": \"…\", \"Exports\": \"…\", \"5-MA\": \"…\"}])\n\ndisplay_tbl = pd.concat([head_part, ellipsis, tail_part], ignore_index=True)\n\n# Rename columns to match the header in the figure\ndisplay_tbl = display_tbl.rename(columns={\"5-MA\": \"5–MA\"})  # en-dash as in book\n\n# Show the table\ndisplay_tbl\n\n\n    \n\n\n\n\n\n\nYear\nExports\n5–MA\n\n\n\n\n0\n1960\n12.99\nNaN\n\n\n1\n1961\n12.4\nNaN\n\n\n2\n1962\n13.94\n13.46\n\n\n3\n1963\n13.01\n13.5\n\n\n4\n1964\n14.94\n13.61\n\n\n5\n1965\n13.22\n13.4\n\n\n6\n1966\n12.93\n13.25\n\n\n7\n1967\n12.88\n12.66\n\n\n8\n…\n…\n…\n\n\n9\n2010\n19.84\n21.21\n\n\n10\n2011\n21.47\n21.17\n\n\n11\n2012\n21.52\n20.78\n\n\n12\n2013\n19.99\n20.81\n\n\n13\n2014\n21.08\n20.37\n\n\n14\n2015\n20.01\n20.32\n\n\n15\n2016\n19.25\nNaN\n\n\n16\n2017\n21.27\nNaN\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nExample: Australian Exports of Goods and Services (% of GDP), 1960–2017\nIn this example, a moving average of order 5 is used to estimate the trend–cycle component of Australia’s exports of goods and services as a percentage of GDP.\n\nEach entry in the 5–MA column represents the average of five consecutive observations, centered on the corresponding year.\nFor example:\n\nThe first 5–MA value averages data from 1960–1964.\nThe second 5–MA value averages 1961–1965, and so on.\n\nThese values correspond to \\(\\hat{T}_t\\) with \\(k = 2\\) and \\(m = 2k + 1 = 5\\).\n\nBecause there are no observations two years before 1960 or two years after 2017, no moving averages can be computed at the series endpoints.\n\n\nExample: Australian Exports of Goods and Services (% of GDP), 1960–2017\n\n# Moving average smoothing example: Australian exports\nglobal_economy = pd.read_csv(\"global_economy.csv\")\naus_exports = global_economy.query('unique_id == \"Australia\"')[[\"unique_id\", \"ds\", \"Exports\"]]\n\nplot_series(aus_exports, target_col=\"Exports\", xlabel=\"Year [1Y]\", ylabel=\"% of GDP\",\n            title=\"Total Australian exports\")\n# 5-MA\naus_exports[\"5-MA\"] = aus_exports[\"Exports\"].rolling(window=5, center=True).mean()\n\nfig, ax = plt.subplots()\nsns.set_style(\"whitegrid\")\nsns.lineplot(data=aus_exports, x=\"ds\", y=\"Exports\", label=\"Exports\", color=\"black\")\nsns.lineplot(data=aus_exports, x=\"ds\", y=\"5-MA\", label=\"5-MA\", color=\"#D55E00\")\nax.set_title(\"Total Australian exports\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"% of GDP\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: Australian Exports of Goods and Services (% of GDP), 1960–2017\nThe trend–cycle component (shown in orange) provides a smoothed representation of the original data, effectively capturing the main long-term movements while filtering out short-term noise.\nKey insights:\n\nThe trend–cycle is smoother than the raw series, emphasizing structural changes rather than random variation.\nThe order of the moving average (\\(m\\)) determines the degree of smoothness:\n\nA larger \\(m\\) produces a smoother and more stable curve.\nA smaller \\(m\\) retains more short-term variation.\n\nThe Figure demonstrates how increasing the order of the moving average progressively smooths the Australian export series, clarifying the underlying trend in long-term trade performance.\n\n\n\nExample: Australian Exports of Goods and Services (% of GDP), 1960–2017\n\n# --- Moving average smoothing example: Australian exports (3-, 5-, 7-, 9-MA panels) ---\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\nglobal_economy = pd.read_csv(\"global_economy.csv\")\naus_exports = global_economy.query('unique_id == \"Australia\"')[[\"ds\", \"Exports\"]].reset_index(drop=True)\n\n# Compute centered moving averages for odd windows\nfor m in (3, 5, 7, 9):\n    aus_exports[f\"{m}-MA\"] = aus_exports[\"Exports\"].rolling(window=m, center=True).mean()\n\n# Plot 2x2 grid\nsns.set_style(\"whitegrid\")\n\nfig, axes = plt.subplots(2, 2, figsize=(13, 8), sharex=True, sharey=True)\naxes = axes.ravel()\n\nfor ax, m in zip(axes, (3, 5, 7, 9)):\n    sns.lineplot(data=aus_exports, x=\"ds\", y=\"Exports\", ax=ax, color=\"black\", label=\"Exports\")\n    sns.lineplot(data=aus_exports, x=\"ds\", y=f\"{m}-MA\", ax=ax, color=\"#D55E00\", label=f\"{m}-MA\")\n    ax.set_title(f\"{m}-MA\")\n    ax.set_xlabel(\"\")  # set common label later\n    ax.grid(True, which=\"both\", axis=\"both\")\n\n# Y labels only on left column\naxes[0].set_ylabel(\"% of GDP\")\naxes[2].set_ylabel(\"% of GDP\")\naxes[1].set_ylabel(\"\")\naxes[3].set_ylabel(\"\")\n\n# Common X label\nfig.text(0.5, 0.04, \"Year\", ha=\"center\")\n\nplt.tight_layout(rect=[0, 0.04, 1, 1])\nplt.show()\n\n\n\n\n\n\n\n\nSimple moving averages such as these are usually of an odd order (e.g., 3, 5, 7, etc.). This is so they are symmetric: in a moving average of order \\(m = 2k + 1\\), the middle observation, and \\(k\\) observations on either side, are averaged. But if \\(m\\) was even, it would no longer be symmetric."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#classical-decomposition",
    "href": "lecture_slides/08_time_series/08_time_series.html#classical-decomposition",
    "title": "Overview",
    "section": "Classical Decomposition",
    "text": "Classical Decomposition\nThe classical decomposition method, developed in the 1920s, serves as the foundation for most modern time series decomposition techniques. It provides a straightforward framework for separating a time series into its underlying components.\nThere are two standard forms of classical decomposition:\n\nAdditive decomposition, where components are combined through addition.\nMultiplicative decomposition, where components interact proportionally through multiplication.\n\nFor a time series with seasonal period \\(m\\), typical examples include:\n\n\\(m = 4\\) for quarterly data,\n\\(m = 12\\) for monthly data,\n\\(m = 7\\) for daily data exhibiting a weekly cycle.\n\nIn this approach, the seasonal component is assumed to be constant over time, repeating identically from year to year.\nFor multiplicative seasonality, the \\(m\\) seasonal values are known as seasonal indices, representing the relative influence of each period within the seasonal cycle.\n\nAdditive Decomposition\nThe additive classical decomposition method separates a time series into trend–cycle, seasonal, and remainder components through a structured four-step process.\nStep 1 — Estimate the Trend–Cycle Component (\\(\\hat{T}_t\\))\n\nIf the seasonal period \\(m\\) is even, use a \\(2 \\times m\\)–MA.\nIf \\(m\\) is odd, use an \\(m\\)–MA. This moving average smooths short-term fluctuations to reveal the underlying trend–cycle.\n\nStep 2 — Detrend the Series Compute the detrended values: \\[\ny_t - \\hat{T}_t\n\\] This removes long-term movement, isolating seasonal and irregular variations.\n\n\nAdditive Decomposition\nStep 3 — Estimate the Seasonal Component (\\(\\hat{S}_t\\)) For each season (e.g., month or quarter), average the detrended values across years.\n\nExample: For monthly data, the March seasonal component equals the average of all detrended March values.\nAdjust these averages to ensure they sum to zero.\nReplicate the seasonal pattern across all years to obtain \\(\\hat{S}_t\\).\n\nStep 4 — Compute the Remainder Component (\\(\\hat{R}_t\\)) Subtract both estimated components from the original data: \\[\n\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t\n\\]\n\nExample: A classical additive decomposition of US retail employment\n\n# --- Classical additive decomposition of US retail employment (monthly, period=12) ---\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# 1) Load & filter series\nus_employment = pd.read_csv(\"us_employment.csv\", parse_dates=[\"ds\"])\nus_retail = (\n    us_employment\n    .query('(unique_id == \"Retail Trade\") & (ds &gt;= \"1990-01-01\")')\n    .loc[:, [\"ds\", \"y\"]]\n    .set_index(\"ds\")\n    .asfreq(\"MS\")              # ensure regular monthly frequency\n)\n\n# 2) Classical decomposition (additive) using 2×m MA for trend internally\n#    period = 12 for monthly seasonality\ndecomp = seasonal_decompose(\n    us_retail[\"y\"],\n    model=\"additive\",\n    period=12,\n    two_sided=True,            # centered moving average\n    extrapolate_trend=\"freq\"   # extends ends to plot the trend line\n)\n\n# 3) Collect components\ndcmp = pd.DataFrame({\n    \"data\":     decomp.observed,\n    \"trend\":    decomp.trend,\n    \"seasonal\": decomp.seasonal,\n    \"random\":   decomp.resid\n}, index=us_retail.index)\n\n# 4) Plot four panels to match the figure\nsns.set_style(\"whitegrid\")\nfig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\n\naxes[0].plot(dcmp.index, dcmp[\"data\"], color=\"black\")\naxes[0].set_ylabel(\"Employed\")\n\naxes[1].plot(dcmp.index, dcmp[\"trend\"], color=\"black\")\naxes[1].set_ylabel(\"trend\")\n\naxes[2].plot(dcmp.index, dcmp[\"seasonal\"], color=\"black\")\naxes[2].set_ylabel(\"seasonal\")\n\naxes[3].plot(dcmp.index, dcmp[\"random\"], color=\"black\")\naxes[3].set_ylabel(\"random\")\n\nfig.suptitle(\"Classical additive decomposition of total US retail employment\\n\"\n             \"Employed = trend + seasonal + random\", y=0.97)\naxes[-1].set_xlabel(\"\")  # figure has no explicit x-label in the book figure\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMultiplicative Decomposition\nThe classical multiplicative decomposition follows the same logical structure as the additive version, except that additions and subtractions are replaced by multiplications and divisions.\nStep 1 — Estimate the Trend–Cycle Component (\\(\\hat{T}_t\\))\n\nIf the seasonal period \\(m\\) is even, use a \\(2 \\times m\\)–MA.\nIf \\(m\\) is odd, use an \\(m\\)–MA. This smooths the data to estimate the underlying trend–cycle.\n\nStep 2 — Detrend the Series Remove the estimated trend–cycle by division: \\[\n\\frac{y_t}{\\hat{T}_t}\n\\]\n\n\nMultiplicative Decomposition\nStep 3 — Estimate the Seasonal Component (\\(\\hat{S}_t\\))\n\nFor each season (e.g., each month), compute the average of the detrended values.\nExample: For monthly data, the seasonal index for March equals the average of all detrended March values.\nAdjust the seasonal indexes so that their sum equals \\(m\\).\nReplicate the sequence across all years to form \\(\\hat{S}_t\\).\n\nStep 4 — Compute the Remainder Component (\\(\\hat{R}_t\\)) Remove both the trend–cycle and seasonal effects: \\[\n\\hat{R}_t = \\frac{y_t}{\\hat{T}_t \\hat{S}_t}\n\\]"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#comments-on-classical-decomposition",
    "href": "lecture_slides/08_time_series/08_time_series.html#comments-on-classical-decomposition",
    "title": "Overview",
    "section": "Comments on Classical Decomposition",
    "text": "Comments on Classical Decomposition\nWhile classical decomposition remains conceptually important and historically influential, it is not recommended for practical forecasting today. Modern techniques—such as STL decomposition and model-based approaches—offer greater flexibility, robustness, and accuracy.\nFor a deeper discussion, refer to Chapter 3 of Forecasting: Principles and Practice, the Pythonic Way."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#stl-decomposition",
    "href": "lecture_slides/08_time_series/08_time_series.html#stl-decomposition",
    "title": "Overview",
    "section": "STL Decomposition",
    "text": "STL Decomposition\nSTL (Seasonal and Trend decomposition using Loess) is a flexible and robust method for decomposing time series into trend, seasonal, and remainder components.\nDeveloped by Cleveland et al. (1990) and later extended by Bandara, Hyndman, and Bergmeir (2022), it uses Loess smoothing to estimate nonlinear trends.\n\nKey Advantages of STL\n\nHandles any type of seasonality, not limited to monthly or quarterly data (unlike SEATS and X–11).\nAllows the seasonal component to evolve over time, with user control over the rate of change.\nProvides adjustable smoothness for the trend–cycle component.\nOffers a robust decomposition option, making it resistant to outliers — unusual observations affect only the remainder component, not the trend or seasonality.\n\n\n\nLimitations of STL\n\nDoes not automatically adjust for trading-day or calendar effects.\nSupports only additive decompositions directly.\n\n\n\nTransformations for Alternative Forms\n\nA multiplicative decomposition can be obtained by applying a log transformation before STL and then back-transforming the components.\nIntermediate decompositions can be created using a Box–Cox transformation with parameter \\(0 &lt; \\lambda &lt; 1\\):\n\n\\(\\lambda = 0\\) → multiplicative decomposition\n\\(\\lambda = 1\\) → additive decomposition\n\n\n\n\nPractical Use\nThe most effective way to master STL is through experimentation and visualization.\nWe will se it applied to US retail employment data and check an alternative decomposition where the trend–cycle is more flexible, the seasonal pattern is fixed, and the robust option is enabled to handle outliers.\n\n\nExample: Total US retail employment (top) and its three additive components obtained from a robust STL decomposition with flexible trend-cycle and fixed seasonality.\n\n\n# Robust STL with flexible trend / fixed season\nstl = STL(us_retail_employment[\"y\"], period=12, seasonal=13, trend=21, robust=True)\nres_stl = stl.fit()\n\ndcmp = pd.DataFrame({\n    \"ds\": us_retail_employment[\"ds\"],\n    \"data\": us_retail_employment[\"y\"],\n    \"trend\": res_stl.trend,\n    \"seasonal\": res_stl.seasonal,\n    \"remainder\": res_stl.resid\n}).reset_index(drop=True)\n\nfig, axes = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(8, 6))\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"data\", ax=axes[0])\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"trend\", ax=axes[1])\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"seasonal\", ax=axes[2])\nsns.lineplot(data=dcmp, x=dcmp.index, y=\"remainder\", ax=axes[3])\naxes[0].set_ylabel(\"Employed\")\naxes[1].set_ylabel(\"trend\")\naxes[2].set_ylabel(\"season_year\")\naxes[3].set_ylabel(\"remainder\")\nfig.suptitle(\"STL decomposition\")\nfig.subplots_adjust(top=0.90)\nfig.text(0.5, 0.95, \"Employed = trend + seasonal + random\", ha=\"center\")\nplt.xlabel(\"\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: Total US Retail Employment and Its STL Components\nThe figure shows the total US retail employment series (top) along with its three additive components — trend–cycle, seasonality, and remainder — obtained from a robust STL decomposition with a flexible trend–cycle and fixed seasonality.\n\nKey STL Parameters\nWhen applying STL, two parameters must be defined:\n\nSeason: the length of the seasonal smoother, controlling how quickly the seasonal pattern can evolve.\nTrend: the length of the trend smoother, determining how flexibly the trend–cycle can adjust to structural changes.\n\nBoth parameters must be odd numbers. Smaller values make the corresponding component more responsive to rapid fluctuations.\n\n\nInterpretation of the Example\nIn the default STL output, the trend window was too long, making the trend–cycle too rigid. This caused signal leakage — notably, the 2008 global financial crisis impact appeared in the remainder component instead of the trend. Adjusting to a shorter trend window (as in Figure 3.12) yields a more accurate and responsive trend–cycle estimate."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#setup",
    "href": "lecture_slides/08_time_series/08_time_series.html#setup",
    "title": "Overview",
    "section": "Setup",
    "text": "Setup\n\n!pip -q install utilsforecast statsmodels seaborn matplotlib cycler\n!pip install fpppy\n# Ensure these are available:\n# - boxcox() function — For applying Box–Cox transformations\n# - boxcox_lambda() function — For finding optimal Box–Cox transformation parameters\n# - plot_series() utility — For creating time series visualizations\n\nimport sys, importlib, subprocess\n\ndef _pip_install(pkg):\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", pkg], check=True)\n\n# 1) coreforecast: boxcox, boxcox_lambda\ntry:\n    from coreforecast.scalers import boxcox, boxcox_lambda\nexcept Exception:\n    _pip_install(\"coreforecast\")\n    from coreforecast.scalers import boxcox, boxcox_lambda\n\n# 2) plot_series utility: prefer fpppy.utils, fallback to utilsforecast.plotting\ntry:\n    from fpppy.utils import plot_series  # if you have the FPP(Pythonic) helpers\nexcept Exception:\n    try:\n        from utilsforecast.plotting import plot_series  # fallback\n    except Exception:\n        _pip_install(\"utilsforecast matplotlib pandas numpy\")\n        from utilsforecast.plotting import plot_series\n\n# --- quick smoke test / confirmation ---\ndef _has_callable(obj):\n    try:\n        return callable(obj)\n    except Exception:\n        return False\n\nprint(\"• boxcox() function —\", \"OK\" if _has_callable(boxcox) else \"MISSING\")\nprint(\"• boxcox_lambda() function —\", \"OK\" if _has_callable(boxcox_lambda) else \"MISSING\")\nprint(\"• plot_series() utility —\", \"OK\" if _has_callable(plot_series) else \"MISSING\")\n\n# Optional: tiny demo to verify they actually run (safe no-op)\nimport numpy as np\ny = np.array([1.0, 2.0, 4.0, 8.0])\nlam = boxcox_lambda(y, method=\"guerrero\", season_length=2)\n_ = boxcox(y, lam)  # should not error\n\nRequirement already satisfied: fpppy in /usr/local/lib/python3.12/dist-packages (0.0.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from fpppy) (3.10.0)\nRequirement already satisfied: pre-commit in /usr/local/lib/python3.12/dist-packages (from fpppy) (4.3.0)\nRequirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from fpppy) (8.4.2)\nRequirement already satisfied: pytest-cov in /usr/local/lib/python3.12/dist-packages (from fpppy) (7.0.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from fpppy) (1.6.1)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from fpppy) (0.14.5)\nRequirement already satisfied: utilsforecast in /usr/local/lib/python3.12/dist-packages (from fpppy) (0.2.14)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (1.4.9)\nRequirement already satisfied: numpy&gt;=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (2.0.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (25.0)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (11.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;fpppy) (2.9.0.post0)\nRequirement already satisfied: cfgv&gt;=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (3.4.0)\nRequirement already satisfied: identify&gt;=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (2.6.15)\nRequirement already satisfied: nodeenv&gt;=0.11.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (1.9.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (6.0.3)\nRequirement already satisfied: virtualenv&gt;=20.10.0 in /usr/local/lib/python3.12/dist-packages (from pre-commit-&gt;fpppy) (20.35.3)\nRequirement already satisfied: iniconfig&gt;=1 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (2.1.0)\nRequirement already satisfied: pluggy&lt;2,&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (1.6.0)\nRequirement already satisfied: pygments&gt;=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest-&gt;fpppy) (2.19.2)\nRequirement already satisfied: coverage&gt;=7.10.6 in /usr/local/lib/python3.12/dist-packages (from coverage[toml]&gt;=7.10.6-&gt;pytest-cov-&gt;fpppy) (7.10.7)\nRequirement already satisfied: scipy&gt;=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (1.15.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (1.5.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-&gt;fpppy) (3.6.0)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels-&gt;fpppy) (2.2.2)\nRequirement already satisfied: patsy&gt;=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels-&gt;fpppy) (1.0.1)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels-&gt;fpppy) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels-&gt;fpppy) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;fpppy) (1.17.0)\nRequirement already satisfied: distlib&lt;1,&gt;=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (0.4.0)\nRequirement already satisfied: filelock&lt;4,&gt;=3.12.2 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (3.20.0)\nRequirement already satisfied: platformdirs&lt;5,&gt;=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv&gt;=20.10.0-&gt;pre-commit-&gt;fpppy) (4.5.0)\n• boxcox() function — OK\n• boxcox_lambda() function — OK\n• plot_series() utility — OK\n\n\n\nimport sys, subprocess\ndef pip_install(*pkgs):\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", *pkgs], check=True)\n\n# tsfeatures is published by Nixtla. These are commonly used together:\npip_install(\"tsfeatures\", \"statsforecast\", \"coreforecast\")\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import HistoricAverage\nfrom utilsforecast.plotting import plot_series\n\n\nproduction_df = pd.read_csv(\"aus_production_formatted.csv\", parse_dates=[\"ds\"])\nproduction_df.head()\n\n\n    \n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nBeer\n1956-03-01\n284.0\n\n\n1\nBeer\n1956-06-01\n213.0\n\n\n2\nBeer\n1956-09-01\n227.0\n\n\n3\nBeer\n1956-12-01\n308.0\n\n\n4\nBeer\n1957-03-01\n262.0"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#mean-method",
    "href": "lecture_slides/08_time_series/08_time_series.html#mean-method",
    "title": "Overview",
    "section": "Mean method",
    "text": "Mean method\nHere, the forecasts of all future values are equal to the average (or “mean”) of the historical data. If we let the historical data be denoted by \\(y_1, \\ldots, y_T\\), then we can write the forecasts as\n\\[\\hat{y}_{T+h\\mid T} = \\bar{y} = \\frac{y_1 + \\cdots + y_T}{T}.\\]\nThe notation \\(\\hat{y}_{T+h\\mid T}\\) is a short-hand for the estimate of \\(y_{T+h}\\) based on the data \\(y_1, \\ldots, y_T\\).\navg_method = HistoricAverage()\nsf = StatsForecast(models=[avg_method], freq='QE')\n\nExample: Mean (or average) forecasts applied to clay brick production in Australia.\n\n# Forecast Australian clay brick production (1956–2004) out to 2010\n# Approach: StatsForecast with HistoricAverage(), quarterly frequency\n# Robust to StatsForecast's column naming (uses model-name column)\n# Plot without confidence intervals; y-axis optional\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import HistoricAverage\nfrom pandas.tseries.offsets import MonthEnd\n\n# ----------------------------\n# Load and prepare quarterly data (quarter-end timestamps)\n# ----------------------------\npath = \"aus_production_formatted.csv\"\ndf = pd.read_csv(path, parse_dates=[\"ds\"])\n\nbricks = (\n    df.query('unique_id == \"Bricks\"')\n      .loc[:, [\"unique_id\", \"ds\", \"y\"]]\n      .sort_values(\"ds\")\n      .assign(ds=lambda d: d[\"ds\"] + MonthEnd(0))  # align to quarter-end so freq='QE' is consistent\n)\n\n# Training window (through 2004 inclusive, as in your code)\ntrain = bricks.query('ds &lt;= \"2004-12-31\"').reset_index(drop=True)\nif train.empty:\n    raise ValueError(\"No Bricks observations found through 2004. Check the file and date ranges.\")\n\nlast_ds = train[\"ds\"].max()                 # last observed quarter-end in training\nend_ds  = pd.Timestamp(\"2010-12-31\")        # forecast end (quarter-end)\n\n# ----------------------------\n# Horizon (number of quarters to 2010Q4)\n# ----------------------------\nlast_q = last_ds.to_period(\"Q\")\nend_q  = end_ds.to_period(\"Q\")\nh = (end_q.year - last_q.year) * 4 + (end_q.quarter - last_q.quarter)\nif h &lt;= 0:\n    raise ValueError(\"Computed horizon h &lt;= 0; verify last_ds and end_ds.\")\n\n# ----------------------------\n# StatsForecast: HistoricAverage\n# ----------------------------\navg_method = HistoricAverage()\nsf = StatsForecast(models=[avg_method], freq=\"QE\")  # quarter-end\nfcasts = sf.forecast(df=train, h=h)\n\n# Determine the forecast value column. Newer versions name it after the model.\nvalue_cols = [c for c in fcasts.columns if c not in (\"unique_id\", \"ds\")]\nif not value_cols:\n    raise ValueError(\n        f\"Forecast dataframe has columns {list(fcasts.columns)} but no value column. \"\n        \"Expected a model-named column like 'HistoricAverage'.\"\n    )\nyhat_col = value_cols[0]   # e.g., 'HistoricAverage'\n\n# ----------------------------\n# Plot history + historic-average forecast (ensure non-black colors)\n# ----------------------------\n# Reset any global monochrome override some earlier cell may have set\nmpl.rcParams['axes.prop_cycle'] = mpl.rcParamsDefault['axes.prop_cycle']\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Use explicit non-cycle colors to be robust in Colab\ntrain_label = f'Bricks ({train[\"ds\"].min().year}–{train[\"ds\"].max().year})'\nax.plot(train[\"ds\"], train[\"y\"], label=train_label, linewidth=1.6, color=\"tab:blue\")\nax.plot(fcasts[\"ds\"], fcasts[yhat_col], \"--\", linewidth=2.4,\n        label=\"HistoricAverage (StatsForecast)\", color=\"tab:orange\")\n\nax.set_title(\"Australian Clay Bricks: HistoricAverage Forecast to 2010 (freq='QE')\")\nax.set_xlabel(\"Quarter\")\nax.set_ylabel(\"Production (millions of bricks)\")\n# ax.set_ylim(300, 600)  # optional bound if desired\nax.legend(loc=\"best\")\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\nprint({\n    \"train_window\": [train[\"ds\"].min().date().isoformat(), train[\"ds\"].max().date().isoformat()],\n    \"n_obs\": int(len(train)),\n    \"h_quarters\": int(h),\n    \"last_training_quarter\": str(last_q),\n    \"forecast_end_quarter\": str(end_q),\n    \"forecast_value_column\": yhat_col\n})\n\n\n\n\n\n\n\n\n{'train_window': ['1956-03-31', '2004-12-31'], 'n_obs': 196, 'h_quarters': 24, 'last_training_quarter': '2004Q4', 'forecast_end_quarter': '2010Q4', 'forecast_value_column': 'HistoricAverage'}"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#naïve-method",
    "href": "lecture_slides/08_time_series/08_time_series.html#naïve-method",
    "title": "Overview",
    "section": "Naïve method",
    "text": "Naïve method\nFor naïve forecasts, we simply set all forecasts to be the value of the last observation. That is,\n\\[\\hat{y}_{T+h\\mid T}=y_T.\\]\nThis method works remarkably well for many economic and financial time series.\nnaive_method = Naive()\nsf = StatsForecast(models=[naive_method], freq='QE')\n\nExample: Naïve forecasts applied to clay brick production in Australia.\n\n# Forecast Australian clay brick production (1956–2004) out to 2010\n# Approach: StatsForecast with Naive() at quarterly frequency\n# Plot without confidence intervals; y-axis fixed to [300, 600]\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive\nfrom pandas.tseries.offsets import MonthEnd\n\n# ----------------------------\n# Load and prepare quarterly data (quarter-end timestamps)\n# ----------------------------\npath = \"aus_production_formatted.csv\"\ndf = pd.read_csv(path, parse_dates=[\"ds\"])\n\nbricks = (\n    df.query('unique_id == \"Bricks\"')\n      .loc[:, [\"unique_id\", \"ds\", \"y\"]]\n      .sort_values(\"ds\")\n      .assign(ds=lambda d: d[\"ds\"] + MonthEnd(0))   # align to quarter-end\n)\n\n# Training window 1970–2004 inclusive\n\n# train = bricks.query('ds &gt;= \"1970-01-01\" and ds &lt;= \"2004-12-31\"').reset_index(drop=True)\ntrain = bricks.query('ds &lt;= \"2004-12-31\"').reset_index(drop=True)\n\nif train.empty:\n    raise ValueError(\"No Bricks observations found in 1970–2004. Check the file and date ranges.\")\n\nlast_ds = train[\"ds\"].max()\nend_ds  = pd.Timestamp(\"2010-12-31\")\n\n# ----------------------------\n# Horizon (quarters from last_ds to 2010Q4)\n# ----------------------------\nlast_q = last_ds.to_period(\"Q\")\nend_q  = end_ds.to_period(\"Q\")\nh = (end_q.year - last_q.year) * 4 + (end_q.quarter - last_q.quarter)\nif h &lt;= 0:\n    raise ValueError(\"Computed horizon h &lt;= 0; verify last_ds and end_ds.\")\n\n# ----------------------------\n# StatsForecast: Naive\n# ----------------------------\nnav_method = Naive()\nsf = StatsForecast(models=[nav_method], freq=\"QE\")   # quarter-end frequency\nfcasts = sf.forecast(df=train, h=h)\n\n# Model output column (StatsForecast names it after the model)\nvalue_cols = [c for c in fcasts.columns if c not in (\"unique_id\", \"ds\")]\nif not value_cols:\n    raise ValueError(\n        f\"Forecast dataframe has columns {list(fcasts.columns)} but no value column. \"\n        \"Expected a model-named column like 'Naive'.\"\n    )\nyhat_col = value_cols[0]   # typically 'Naive'\n\n# ----------------------------\n# Plot history + naive forecast (no intervals)\n# ----------------------------\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(train[\"ds\"], train[\"y\"], label=\"Bricks (1956–2004)\", linewidth=1.6)\nax.plot(fcasts[\"ds\"], fcasts[yhat_col], \"--\", linewidth=2, label=\"Naive (StatsForecast)\")\n\nax.set_title(\"Australian Clay Bricks: Naive Forecast to 2010 (freq='QE')\")\nax.set_xlabel(\"Quarter\")\nax.set_ylabel(\"Production (millions of bricks)\")\n# ax.set_ylim(300, 600)\nax.legend(loc=\"best\")\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\nprint({\n    \"train_window\": [train[\"ds\"].min().date().isoformat(), train[\"ds\"].max().date().isoformat()],\n    \"n_obs\": int(len(train)),\n    \"h_quarters\": int(h),\n    \"last_training_quarter\": str(last_q),\n    \"forecast_end_quarter\": str(end_q),\n    \"forecast_value_column\": yhat_col\n})\n\n\n\n\n\n\n\n\n{'train_window': ['1956-03-31', '2004-12-31'], 'n_obs': 196, 'h_quarters': 24, 'last_training_quarter': '2004Q4', 'forecast_end_quarter': '2010Q4', 'forecast_value_column': 'Naive'}\n\n\nBecause a naïve forecast is optimal when data follow a random walk (see Section 9.1), these are also called random walk forecasts."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#seasonal-naïve-method-with-period-m",
    "href": "lecture_slides/08_time_series/08_time_series.html#seasonal-naïve-method-with-period-m",
    "title": "Overview",
    "section": "Seasonal Naïve Method with Period \\(m\\)",
    "text": "Seasonal Naïve Method with Period \\(m\\)\nThe seasonal naïve method is particularly effective for highly seasonal time series. It assumes that each observation repeats the value from the same season in the previous cycle.\nFormally, the forecast for time \\(T + h\\) is defined as:\n\\[\n\\hat{y}*{T+h \\mid T} = y*{T + h - m(k+1)},\n\\]\nwhere:\n\n\\(m\\) = seasonal period (e.g., 12 for monthly data, 4 for quarterly data),\n\\(k\\) = integer part of \\((h - 1)/m\\), representing the number of complete seasons in the forecast horizon prior to \\(T + h\\).\n\n\nInterpretation\n\nFor monthly data, all future February forecasts equal the last observed February value.\nFor quarterly data, all future Q2 forecasts equal the last observed Q2 value.\nThe same logic extends to other seasonal periods.\n\n\n\nExample: Seasonal naïve forecasts applied to clay brick production in Australia.\n\n# Forecast Australian clay brick production (1970–2004) out to 2010\n# Approach: StatsForecast SeasonalNaive (m=4, quarterly)\n# Plot without confidence intervals; y-axis fixed to [300, 600]\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import SeasonalNaive\nfrom pandas.tseries.offsets import MonthEnd\n\n# ----------------------------\n# Load and prepare quarterly data (quarter-end timestamps)\n# ----------------------------\npath = \"aus_production_formatted.csv\"\ndf = pd.read_csv(path, parse_dates=[\"ds\"])\n\nbricks = (\n    df.query('unique_id == \"Bricks\"')\n      .loc[:, [\"unique_id\", \"ds\", \"y\"]]\n      .sort_values(\"ds\")\n      .assign(ds=lambda d: d[\"ds\"] + MonthEnd(0))   # align to quarter-end\n)\n\n# Training window 1970–2004 inclusive\n# train = bricks.query('ds &gt;= \"1970-01-01\" and ds &lt;= \"2004-12-31\"').reset_index(drop=True)\ntrain = bricks.query('ds &lt;= \"2004-12-31\"').reset_index(drop=True)\nif train.empty:\n    raise ValueError(\"No Bricks observations found in 1970–2004. Check the file and date ranges.\")\n\nlast_ds = train[\"ds\"].max()\nend_ds  = pd.Timestamp(\"2010-12-31\")\n\n# ----------------------------\n# Horizon (quarters from last_ds to 2010Q4)\n# ----------------------------\nlast_q = last_ds.to_period(\"Q\")\nend_q  = end_ds.to_period(\"Q\")\nh = (end_q.year - last_q.year) * 4 + (end_q.quarter - last_q.quarter)\nif h &lt;= 0:\n    raise ValueError(\"Computed horizon h &lt;= 0; verify last_ds and end_ds.\")\n\n# ----------------------------\n# StatsForecast: Seasonal Naive (m = 4 for quarterly)\n# ----------------------------\nsnaive = SeasonalNaive(season_length=4)\nsf = StatsForecast(models=[snaive], freq=\"QE\")   # quarter-end frequency\nfcasts = sf.forecast(df=train, h=h)\n\n# Model output column (StatsForecast names it after the model)\nvalue_cols = [c for c in fcasts.columns if c not in (\"unique_id\", \"ds\")]\nif not value_cols:\n    raise ValueError(\n        f\"Forecast dataframe has columns {list(fcasts.columns)} but no value column. \"\n        \"Expected a model-named column like 'SeasonalNaive'.\"\n    )\nyhat_col = value_cols[0]   # typically 'SeasonalNaive'\n\n# ----------------------------\n# Plot history + seasonal naive forecast (no intervals)\n# ----------------------------\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(train[\"ds\"], train[\"y\"], label=\"Bricks (1956–2004)\", linewidth=1.6)\nax.plot(fcasts[\"ds\"], fcasts[yhat_col], \"--\", linewidth=2, label=\"Seasonal Naïve (StatsForecast, m=4)\")\n\nax.set_title(\"Australian Clay Bricks: Seasonal Naïve Forecast to 2010 (freq='QE')\")\nax.set_xlabel(\"Quarter\")\nax.set_ylabel(\"Production (millions of bricks)\")\n# ax.set_ylim(300, 600)\nax.legend(loc=\"best\")\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\nprint({\n    \"train_window\": [train[\"ds\"].min().date().isoformat(), train[\"ds\"].max().date().isoformat()],\n    \"n_obs\": int(len(train)),\n    \"h_quarters\": int(h),\n    \"last_training_quarter\": str(last_q),\n    \"forecast_end_quarter\": str(end_q),\n    \"forecast_value_column\": yhat_col\n})\n\n\n\n\n\n\n\n\n{'train_window': ['1956-03-31', '2004-12-31'], 'n_obs': 196, 'h_quarters': 24, 'last_training_quarter': '2004Q4', 'forecast_end_quarter': '2010Q4', 'forecast_value_column': 'SeasonalNaive'}"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#drift-method",
    "href": "lecture_slides/08_time_series/08_time_series.html#drift-method",
    "title": "Overview",
    "section": "Drift Method",
    "text": "Drift Method\nThe drift method is a variation of the naïve forecasting approach, where forecasts are allowed to increase or decrease linearly over time. The rate of change, known as the drift, equals the average historical change between consecutive observations.\nThe forecast for time \\(T + h\\) is given by:\n\\[\n\\hat{y}*{T+h \\mid T} = y_T + \\frac{h}{T - 1} \\sum*{t=2}^{T}(y_t - y_{t-1}) = y_T + h \\left( \\frac{y_T - y_1}{T - 1} \\right).\n\\]\n\nInterpretation\n\nThe method effectively draws a straight line between the first and last observations of the series.\nFuture values are extrapolated linearly, assuming that the historical rate of change continues.\nIt captures simple trend behavior while maintaining the structure of a random walk.\n\nThis approach is particularly useful for series with a steady trend, as it extends the historical direction of movement into the forecast horizon.\n\n\nExample: Drift forecasts applied to clay brick production in Australia\n\n# Forecast Australian clay brick production (1970–2004) out to 2010\n# Approach: StatsForecast RandomWalkWithDrift() (a.k.a. naïve with drift)\n# Plot without confidence intervals; y-axis fixed to [300, 600]\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import RandomWalkWithDrift\nfrom pandas.tseries.offsets import MonthEnd\n\n# ----------------------------\n# Load and prepare quarterly data (quarter-end timestamps)\n# ----------------------------\npath = \"aus_production_formatted.csv\"\ndf = pd.read_csv(path, parse_dates=[\"ds\"])\n\nbricks = (\n    df.query('unique_id == \"Bricks\"')\n      .loc[:, [\"unique_id\", \"ds\", \"y\"]]\n      .sort_values(\"ds\")\n      .assign(ds=lambda d: d[\"ds\"] + MonthEnd(0))   # align to quarter-end for 'QE'\n)\n\n# Training window 1970–2004 inclusive\n# train = bricks.query('ds &gt;= \"1970-01-01\" and ds &lt;= \"2004-12-31\"').reset_index(drop=True)\ntrain = bricks.query('ds &lt;= \"2004-12-31\"').reset_index(drop=True)\nif train.empty:\n    raise ValueError(\"No Bricks observations found in 1970–2004. Check the file and date ranges.\")\n\nlast_ds = train[\"ds\"].max()\nend_ds  = pd.Timestamp(\"2010-12-31\")\n\n# ----------------------------\n# Horizon (quarters from last_ds to 2010Q4)\n# ----------------------------\nlast_q = last_ds.to_period(\"Q\")\nend_q  = end_ds.to_period(\"Q\")\nh = (end_q.year - last_q.year) * 4 + (end_q.quarter - last_q.quarter)\nif h &lt;= 0:\n    raise ValueError(\"Computed horizon h &lt;= 0; verify last_ds and end_ds.\")\n\n# ----------------------------\n# StatsForecast: Random Walk with Drift\n# ----------------------------\ndrift_method = RandomWalkWithDrift()\nsf = StatsForecast(models=[drift_method], freq=\"QE\")   # quarter-end frequency\nfcasts = sf.forecast(df=train, h=h)                    # columns: unique_id, ds, RandomWalkWithDrift\n\n# Get the forecast value column (named after the model)\nvalue_cols = [c for c in fcasts.columns if c not in (\"unique_id\", \"ds\")]\nif not value_cols:\n    raise ValueError(f\"Unexpected forecast columns: {list(fcasts.columns)}\")\nyhat_col = value_cols[0]  # typically 'RandomWalkWithDrift'\n\n# ----------------------------\n# Plot history + drift forecast\n# ----------------------------\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(train[\"ds\"], train[\"y\"], label=\"Bricks (1956–2004)\", linewidth=1.6)\nax.plot(fcasts[\"ds\"], fcasts[yhat_col], \"--\", linewidth=2, label=\"Naïve with Drift (StatsForecast)\")\n\nax.set_title(\"Australian Clay Bricks: Naïve-with-Drift Forecast to 2010 (freq='QE')\")\nax.set_xlabel(\"Quarter\")\nax.set_ylabel(\"Production (millions of bricks)\")\n# ax.set_ylim(300, 600)\nax.legend(loc=\"best\")\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\nprint({\n    \"train_window\": [train[\"ds\"].min().date().isoformat(), train[\"ds\"].max().date().isoformat()],\n    \"n_obs\": int(len(train)),\n    \"h_quarters\": int(h),\n    \"last_training_quarter\": str(last_q),\n    \"forecast_end_quarter\": str(end_q),\n    \"forecast_value_column\": yhat_col\n})\n\n\n\n\n\n\n\n\n{'train_window': ['1956-03-31', '2004-12-31'], 'n_obs': 196, 'h_quarters': 24, 'last_training_quarter': '2004Q4', 'forecast_end_quarter': '2010Q4', 'forecast_value_column': 'RWD'}"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#example-australian-quarterly-beer-production",
    "href": "lecture_slides/08_time_series/08_time_series.html#example-australian-quarterly-beer-production",
    "title": "Overview",
    "section": "Example: Australian Quarterly Beer Production",
    "text": "Example: Australian Quarterly Beer Production\nThe figure illustrates the application of the first three forecasting methods — naïve, seasonal naïve, and drift — to Australian quarterly beer production data covering the period 1992–2006.\nThe forecasts generated by each method are compared with the actual observations over the subsequent 3.5 years, providing a clear visualization of how each approach captures trend and seasonality in the data.\nThis example highlights the differences in performance between the methods when dealing with seasonal patterns and gradual trends in quarterly production data.\nIn this case, only the seasonal naïve forecasts are close to the observed values from 2007 onwards.\n\n# Forecasts for quarterly beer production (1992–2006) with 3.5-year holdout\n# Models: Naïve, Mean (HistoricAverage), Seasonal Naïve\n# Plot forecasts against actuals for the next 14 quarters with distinct colors\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.tseries.offsets import MonthEnd, QuarterEnd\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive, HistoricAverage, SeasonalNaive\n\n# ----------------------------\n# Load data and prepare quarterly timestamps\n# ----------------------------\npath = \"aus_production_formatted.csv\"\ndf = pd.read_csv(path, parse_dates=[\"ds\"])\n\nbeer = (\n    df.query('unique_id == \"Beer\"')\n      .loc[:, [\"unique_id\", \"ds\", \"y\"]]\n      .sort_values(\"ds\")\n      .assign(ds=lambda d: d[\"ds\"] + MonthEnd(0))  # align to quarter-end for freq='QE'\n)\n\n# ----------------------------\n# Train window and horizon\n# ----------------------------\ntrain = beer.query('ds &gt;= \"1992-01-01\" and ds &lt;= \"2006-12-31\"').reset_index(drop=True)\nif train.empty:\n    raise ValueError(\"No Beer observations found in 1992–2006.\")\n\nlast_train_ds = train[\"ds\"].max()   # 2006Q4 quarter-end\nh = 14                              # 3.5 years = 14 quarters\nend_ds = last_train_ds + QuarterEnd(h)\n\n# ----------------------------\n# StatsForecast models\n# ----------------------------\nsf = StatsForecast(\n    models=[Naive(), HistoricAverage(), SeasonalNaive(season_length=4)],\n    freq=\"QE\"\n)\nfcasts = sf.forecast(df=train, h=h)\n\n# Normalize model column names\nrename_map = {}\nfor c in fcasts.columns:\n    if c in (\"unique_id\", \"ds\"):\n        continue\n    cl = c.lower()\n    if \"historicaverage\" in cl or cl == \"mean\":\n        rename_map[c] = \"Mean\"\n    elif cl == \"naive\":\n        rename_map[c] = \"Naïve\"\n    elif \"seasonalnaive\" in cl:\n        rename_map[c] = \"Seasonal naïve\"\nfcasts = fcasts.rename(columns=rename_map)\n\n# Actuals in holdout horizon\nactual_future = beer.query('ds &gt; @last_train_ds and ds &lt;= @end_ds')\n\n# ----------------------------\n# Plot\n# ----------------------------\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Historical and holdout actuals in black\nax.plot(train[\"ds\"], train[\"y\"], color=\"black\", linewidth=1.6)\nif not actual_future.empty:\n    ax.plot(actual_future[\"ds\"], actual_future[\"y\"], color=\"black\", linewidth=1.6)\n\n# Forecasts with distinct colors\nax.plot(fcasts[\"ds\"], fcasts[\"Mean\"],           color=\"tab:orange\", linewidth=2.5, label=\"Mean\")\nax.plot(fcasts[\"ds\"], fcasts[\"Naïve\"],          color=\"tab:blue\",   linewidth=2.5, label=\"Naïve\")\nax.plot(fcasts[\"ds\"], fcasts[\"Seasonal naïve\"], color=\"tab:green\",  linewidth=2.5, label=\"Seasonal naïve\")\n\nax.set_title(\"Forecasts for quarterly beer production\")\nax.set_xlabel(\"Quarter\")\nax.set_ylabel(\"Megalitres\")\nax.legend(title=\"Forecast\", loc=\"best\", frameon=True)\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#example-googles-daily-closing-stock-price",
    "href": "lecture_slides/08_time_series/08_time_series.html#example-googles-daily-closing-stock-price",
    "title": "Overview",
    "section": "Example: Google’s Daily Closing Stock Price",
    "text": "Example: Google’s Daily Closing Stock Price\nThe non-seasonal forecasting methods are applied to Google’s daily closing stock prices for the year 2015, generating one-month-ahead forecasts.\nBecause stock prices are recorded only on trading days, not on calendar days, the analysis begins by creating a custom time index aligned with the sequence of trading days.\nThis ensures that temporal spacing in the data reflects actual market activity, allowing the forecasting models to properly capture short-term price dynamics without distortion from non-trading days.\n\n# GOOG daily close in 2015 → non-seasonal forecasts (Mean, Naive, Drift) one month ahead\n# Robust to StatsForecast variants where RandomWalkWithDrift column is named 'RWD'\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.tseries.offsets import MonthEnd\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import HistoricAverage, Naive, RandomWalkWithDrift\n\n# ----------------------------\n# Load and subset Google Close\n# ----------------------------\ndf = pd.read_csv(\"gafa_stock.csv\", parse_dates=[\"ds\"])\n\ngoog = (\n    df.query('unique_id == \"GOOG_Close\"')\n      .loc[:, [\"unique_id\", \"ds\", \"y\"]]\n      .sort_values(\"ds\")\n      .reset_index(drop=True)\n)\n\n# ----------------------------\n# Training window: 2015 trading days\n# ----------------------------\ntrain = goog.query('ds &gt;= \"2015-01-01\" and ds &lt;= \"2015-12-31\"').reset_index(drop=True)\nif train.empty:\n    raise ValueError(\"No GOOG_Close observations in 2015.\")\nlast_train_date = train[\"ds\"].max()\n\n# Horizon: trading days in the next calendar month (after last training day)\nnext_month_end = last_train_date + MonthEnd(1)\nfuture_dates = goog.loc[(goog[\"ds\"] &gt; last_train_date) & (goog[\"ds\"] &lt;= next_month_end), \"ds\"].tolist()\nif len(future_dates) == 0:  # fallback if needed\n    future_dates = goog.loc[goog[\"ds\"] &gt; last_train_date, \"ds\"].head(21).tolist()\nh = len(future_dates)\n\n# ----------------------------\n# Trading-day index for StatsForecast (freq=1)\n# ----------------------------\ntrain_idx = train.copy()\ntrain_idx[\"ds\"] = np.arange(len(train_idx), dtype=int)  # 0..N-1\n\n# ----------------------------\n# Models and forecast\n# ----------------------------\navg_method   = HistoricAverage()\nnaive_method = Naive()\ndrift_method = RandomWalkWithDrift()\n\nsf = StatsForecast(models=[drift_method, avg_method, naive_method], freq=1)\nfcasts = sf.forecast(df=train_idx, h=h)  # columns: unique_id, ds, &lt;drift&gt;, HistoricAverage, Naive\n\n# Map forecast step index back to real trading dates\nfcasts_plot = fcasts.copy()\nfcasts_plot[\"ds\"] = future_dates[:len(fcasts_plot)]\n\n# ----------------------------\n# Robust rename of model columns\n# ----------------------------\nrename = {}\nfor c in fcasts_plot.columns:\n    if c in (\"unique_id\", \"ds\"):\n        continue\n    lc = c.lower()\n    if (\"randomwalkwithdrift\" in lc) or (\"rw\" in lc and \"d\" in lc) or (lc == \"rwd\") or (\"drift\" in lc):\n        rename[c] = \"Drift\"\n    elif \"historicaverage\" in lc or lc == \"mean\":\n        rename[c] = \"Mean\"\n    elif lc == \"naive\":\n        rename[c] = \"Naive\"\nfcasts_plot = fcasts_plot.rename(columns=rename)\n\n# If a short alias like 'RWD' remains unmapped, catch it\nif \"Drift\" not in fcasts_plot.columns:\n    drift_like = [c for c in fcasts_plot.columns if c not in (\"unique_id\", \"ds\", \"Mean\", \"Naive\")]\n    if drift_like:\n        fcasts_plot = fcasts_plot.rename(columns={drift_like[0]: \"Drift\"})\n\n# Actuals within the forecast month (for comparison)\nactual_future = goog.loc[goog[\"ds\"].isin(future_dates)].copy()\n\n# ----------------------------\n# Plot\n# ----------------------------\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Actuals (2015 train + next month)\nax.plot(train[\"ds\"], train[\"y\"], color=\"black\", linewidth=1.4, label=\"Actuals\")\nif not actual_future.empty:\n    ax.plot(actual_future[\"ds\"], actual_future[\"y\"], color=\"black\", linewidth=1.4)\n\n# Forecast lines (distinct colors)\nif \"Mean\" in fcasts_plot:\n    ax.plot(fcasts_plot[\"ds\"], fcasts_plot[\"Mean\"],  color=\"tab:orange\", linewidth=2.5, label=\"Mean\")\nif \"Naive\" in fcasts_plot:\n    ax.plot(fcasts_plot[\"ds\"], fcasts_plot[\"Naive\"], color=\"tab:blue\",   linewidth=2.5, label=\"Naive\")\nif \"Drift\" in fcasts_plot:\n    ax.plot(fcasts_plot[\"ds\"], fcasts_plot[\"Drift\"], color=\"tab:green\",  linewidth=2.5, label=\"Drift\")\n\nax.set_title(\"Google daily closing stock prices (Jan 2015-Jan 2016)\")\nax.set_xlabel(\"Date (trading days)\")\nax.set_ylabel(\"Closing Price (USD)\")\nax.legend(title=\"Method\", loc=\"best\", frameon=True)\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#benchmark-role-of-simple-forecasting-methods",
    "href": "lecture_slides/08_time_series/08_time_series.html#benchmark-role-of-simple-forecasting-methods",
    "title": "Overview",
    "section": "Benchmark Role of Simple Forecasting Methods",
    "text": "Benchmark Role of Simple Forecasting Methods\nIn some situations, one of these simple forecasting methods may provide the most effective and practical solution available.\nHowever, in most applications, these methods primarily serve as benchmark models. Any newly developed forecasting technique should be evaluated against these benchmarks to confirm its added value.\nIf a new model does not outperform these simple alternatives, it is not justified for use, regardless of its complexity.\nEstablishing such benchmarks ensures that model development remains grounded in empirical performance rather than unnecessary sophistication."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#load-additional-libraries",
    "href": "lecture_slides/08_time_series/08_time_series.html#load-additional-libraries",
    "title": "Overview",
    "section": "Load additional libraries",
    "text": "Load additional libraries\n\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    category=UserWarning,\n    message=\".*FigureCanvasAgg is non-interactive.*\"\n)\n\nimport os\nos.environ[\"NIXTLA_ID_AS_COL\"] = \"true\"\n\nimport numpy as np\nnp.set_printoptions(suppress=True)\nnp.random.seed(1)\n\nimport random\nrandom.seed(1)\n\nimport pandas as pd\npd.set_option(\"max_colwidth\", 100)\npd.set_option(\"display.precision\", 3)\n\nfrom utilsforecast.plotting import plot_series as plot_series_utils\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nplt.rcParams.update({\n    \"figure.figsize\": (8, 5),\n    \"figure.dpi\": 100,\n    \"savefig.dpi\": 300,\n    \"figure.constrained_layout.use\": True,\n    \"axes.titlesize\": 12,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n    \"legend.title_fontsize\": 10,\n})\n\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['axes.prop_cycle'] = cycler(color=[\"#000000\", \"#000000\"])\n\nfrom fpppy.utils import plot_series\n\n\nfrom IPython.display import Image\nfrom functools import partial\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom utilsforecast.evaluation import evaluate\nfrom utilsforecast.feature_engineering import pipeline, trend\n# from utilsforecast.losses import rmse, mae, mape as _mape, mase, quantile_loss, mq\nfrom utilsforecast.losses import rmse, mae, mape as _mape, mase, quantile_loss\n\ndef mape(df, models, id_col = \"unique_id\", target_col = \"y\"):\n    df_mape = _mape(df, models, id_col=id_col, target_col=target_col)\n    df_mape.loc[:, df_mape.select_dtypes(include='number').columns] *= 100\n    return df_mape\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import SklearnModel\nfrom statsforecast.utils import ConformalIntervals\nfrom statsforecast.models import (\n    WindowAverage,\n    Naive,\n    SeasonalNaive,\n    RandomWalkWithDrift,\n    HistoricAverage,\n)\nfrom fpppy.models import LinearRegression"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#example-naïve-forecasts-of-the-seasonally-adjusted-data-obtained-from-an-stl-decomposition-of-the-total-us-retail-employment.",
    "href": "lecture_slides/08_time_series/08_time_series.html#example-naïve-forecasts-of-the-seasonally-adjusted-data-obtained-from-an-stl-decomposition-of-the-total-us-retail-employment.",
    "title": "Overview",
    "section": "Example: Naïve forecasts of the seasonally adjusted data obtained from an STL decomposition of the total US retail employment.",
    "text": "Example: Naïve forecasts of the seasonally adjusted data obtained from an STL decomposition of the total US retail employment.\n\n# Seasonally adjust US retail employment, forecast the adjusted series with Naive(),\n# and plot with Matplotlib. Fix: pass a *single* 'y' column to StatsForecast\n# (avoid duplicate 'y' caused by keeping both raw and adjusted series).\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import STL\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive\n\n# ----------------------------\n# Load & filter\n# ----------------------------\nus_employment_df = pd.read_csv(\"us_employment_formatted.csv\", parse_dates=[\"ds\"])\nus_retail_employment_df = (\n    us_employment_df.query(\"Title == 'Retail Trade' and ds.dt.year &gt;= 1992\")\n    .sort_values(\"ds\")\n    .reset_index(drop=True)\n    .drop(columns=[\"Title\"])\n)\n\n# ----------------------------\n# STL seasonal adjustment (monthly -&gt; period=12)\n# ----------------------------\nstl = STL(us_retail_employment_df[\"y\"].to_numpy(), period=12, robust=True, trend_deg=7)\nres = stl.fit()\nus_retail_employment_df[\"seasonal\"] = res.seasonal\nus_retail_employment_df[\"y_adjusted\"] = (\n    us_retail_employment_df[\"y\"] - us_retail_employment_df[\"seasonal\"]\n)\n\n# ----------------------------\n# Build the dataframe for StatsForecast with a SINGLE 'y' column\n# ----------------------------\nadj_train = (\n    us_retail_employment_df.loc[:, [\"unique_id\", \"ds\", \"y_adjusted\"]]\n    .rename(columns={\"y_adjusted\": \"y\"})\n)\n\n# ----------------------------\n# Forecast the seasonally-adjusted series with Naive\n# ----------------------------\nsf = StatsForecast(models=[Naive()], freq=\"M\")\nadj_fc = sf.forecast(df=adj_train, h=24, level=[80, 95], fitted=True)\nfitted_vals = sf.forecast_fitted_values()\n\n# Detect model column (e.g., 'Naive')\nmodel_col = next(c for c in adj_fc.columns if c not in (\"unique_id\", \"ds\"))\n\n# Optional: rename back for clarity in plotting variables\nyhat = adj_fc[model_col]\nlo80, hi80 = adj_fc.get(f\"{model_col}-lo-80\"), adj_fc.get(f\"{model_col}-hi-80\")\nlo95, hi95 = adj_fc.get(f\"{model_col}-lo-95\"), adj_fc.get(f\"{model_col}-hi-95\")\n\n# ----------------------------\n# Plot with Matplotlib\n# ----------------------------\nfig, ax = plt.subplots(figsize=(11, 5))\n\n# Historical seasonally-adjusted series\nax.plot(\n    us_retail_employment_df[\"ds\"],\n    us_retail_employment_df[\"y_adjusted\"],\n    color=\"black\",\n    linewidth=1.6,\n    label=\"Adjusted (history)\",\n)\n\n# Prediction intervals (if present)\nif lo95 is not None and hi95 is not None:\n    ax.fill_between(adj_fc[\"ds\"], lo95, hi95, alpha=0.18, label=\"95% PI\")\nif lo80 is not None and hi80 is not None:\n    ax.fill_between(adj_fc[\"ds\"], lo80, hi80, alpha=0.28, label=\"80% PI\")\n\n# Point forecast\nax.plot(adj_fc[\"ds\"], yhat, \"--\", linewidth=2.2, label=f\"{model_col} forecast\")\n\nax.set_title(\"US retail employment (seasonally adjusted)\")\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Number of people\")\nax.legend(loc=\"best\", frameon=True)\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nForecasting with STL Decomposition\nThe Figure presents naïve forecasts of the seasonally adjusted US retail employment data.\nThese forecasts are then “reseasonalised” by adding the seasonal naïve forecasts of the seasonal component, effectively reconstructing the full series.\n\n\nAutomated Forecasting with STL()\nThe STL() function in statsmodels simplifies this process by:\n\nAllowing forecasts from any additive decomposition.\nLetting users specify different forecasting models for each component.\nAutomatically applying SeasonalNaive() for the seasonal component if no alternative is provided.\nPerforming the reseasonalisation automatically, ensuring that forecasts of the original series are recovered correctly.\n\nThe next Figure illustrates the final reseasonalised forecasts, showing how the STL framework integrates decomposition and forecasting into a cohesive, automated workflow.\n\n\nExample: Forecasts of the total US retail employment data based on a naïve forecast of the seasonally adjusted data and a seasonal naïve forecast of the seasonal component, after an STL decomposition of the data.\n\n# Re-seasonalise the adjusted forecast by adding a SeasonalNaive forecast of the seasonal component\n# and PLOT with Matplotlib.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive, SeasonalNaive   # &lt;-- Naive was missing\n\n# --- helper: keep unique column names (avoids target being returned as a DataFrame) ---\ndef _dedupe_cols(df: pd.DataFrame) -&gt; pd.DataFrame:\n    if df.columns.duplicated().any():\n        df = df.loc[:, ~df.columns.duplicated()]\n    return df\n\n# 1) Seasonally-adjusted level -&gt; Naive\nadj_train = (\n    us_retail_employment_df[[\"unique_id\", \"ds\"]].copy()\n    .assign(y=us_retail_employment_df[\"y_adjusted\"])\n)\nadj_train = _dedupe_cols(adj_train)\nadj_train[\"y\"] = pd.to_numeric(adj_train[\"y\"], errors=\"coerce\")\nadj_train = adj_train.dropna(subset=[\"y\"])\n\nsf_adj = StatsForecast(models=[Naive()], freq=\"M\")\nadjusted_fcasts = sf_adj.forecast(df=adj_train, h=24, level=[80, 95], fitted=True)\nadjusted_fcasts_fitted = sf_adj.forecast_fitted_values()\n\n# 2) Seasonal component -&gt; SeasonalNaive\nseas_train = (\n    us_retail_employment_df[[\"unique_id\", \"ds\"]].copy()\n    .assign(y=us_retail_employment_df[\"seasonal\"])\n)\nseas_train = _dedupe_cols(seas_train)\nseas_train[\"y\"] = pd.to_numeric(seas_train[\"y\"], errors=\"coerce\")\nseas_train = seas_train.dropna(subset=[\"y\"])\n\nsf_seas = StatsForecast(models=[SeasonalNaive(season_length=12)], freq=\"M\")\nseasonal_fcasts = sf_seas.forecast(df=seas_train, h=24, level=[80, 95], fitted=True)\nseasonal_fitted = sf_seas.forecast_fitted_values()\n\n# 3) Combine (re-seasonalise): detect the model-value columns\nadj_col = [c for c in adjusted_fcasts.columns if c not in (\"unique_id\", \"ds\") and \"-lo-\" not in c and \"-hi-\" not in c][0]\nsea_col = [c for c in seasonal_fcasts.columns if c not in (\"unique_id\", \"ds\") and \"-lo-\" not in c and \"-hi-\" not in c][0]\n\nmerged = adjusted_fcasts.merge(seasonal_fcasts, on=[\"unique_id\", \"ds\"], how=\"inner\")\nfinal_df = merged[[\"unique_id\", \"ds\"]].copy()\nfinal_df[\"combined\"] = merged[adj_col] + merged[sea_col]\nfor a in (80, 95):\n    final_df[f\"combined-lo-{a}\"] = merged[f\"{adj_col}-lo-{a}\"] + merged[f\"{sea_col}-lo-{a}\"]\n    final_df[f\"combined-hi-{a}\"] = merged[f\"{adj_col}-hi-{a}\"] + merged[f\"{sea_col}-hi-{a}\"]\n\n# 3b) Residuals (align lengths by merging back, not assigning by position)\nmerged_fitted = (\n    adjusted_fcasts_fitted.merge(seasonal_fitted, on=[\"unique_id\", \"ds\"], how=\"inner\")\n    .merge(us_retail_employment_df[[\"unique_id\", \"ds\", \"y\"]], on=[\"unique_id\", \"ds\"], how=\"inner\")\n)\nresids = merged_fitted.assign(\n    resid = merged_fitted[\"y\"] - merged_fitted[adj_col] - merged_fitted[sea_col]\n)[[\"unique_id\", \"ds\", \"resid\"]]\n\n# attach residuals (left-join) to the full dataset; rows without fitted values get NaN\nus_retail_employment_df = us_retail_employment_df.merge(resids, on=[\"unique_id\", \"ds\"], how=\"left\")\n\n# 4) Plot with Matplotlib\nfig, ax = plt.subplots(figsize=(11, 5))\n\n# History (original, not adjusted)\nax.plot(\n    us_retail_employment_df[\"ds\"], us_retail_employment_df[\"y\"],\n    color=\"black\", linewidth=1.6, label=\"History\"\n)\n\n# Bands\nax.fill_between(\n    final_df[\"ds\"], final_df[\"combined-lo-95\"], final_df[\"combined-hi-95\"],\n    alpha=0.18, label=\"95% PI\"\n)\nax.fill_between(\n    final_df[\"ds\"], final_df[\"combined-lo-80\"], final_df[\"combined-hi-80\"],\n    alpha=0.28, label=\"80% PI\"\n)\n\n# Point forecast\nax.plot(\n    final_df[\"ds\"], final_df[\"combined\"],\n    \"--\", linewidth=2.2, label=\"Re-seasonalised forecast\"\n)\nsns.set_style(\"whitegrid\")\nax.set_title(\"US retail employment (reseasonalised forecasts)\")\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Number of people\")\nax.legend(loc=\"best\", frameon=True)\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPrediction Intervals and Residual Analysis\nThe prediction intervals in this figure above are generated using the same reseasonalisation process as the point forecasts.\n\nThe upper and lower bounds of the intervals for the seasonally adjusted series are reseasonalised by adding the corresponding seasonal component forecasts.\nThis ensures that both point forecasts and interval forecasts reflect the same seasonal dynamics of the original data.\n\n\n\nResidual Diagnostics\nThe ACF of the residuals (Figure 5.21) reveals significant autocorrelation, indicating that:\n\nThe naïve method fails to capture the evolving trend in the seasonally adjusted series.\nThis persistence in the residuals suggests that a more advanced forecasting model is required to account for the underlying trend changes and reduce serial dependence.\n\nA convenient shortcut for producing these residual diagnostic graphs is the plot_diagnostics() function, which will produce a time plot, ACF plot and histogram of the residuals.\n\ndef plot_diagnostics(data):\n    fig = plt.figure(figsize=(8, 5))\n\n    ax1 = fig.add_subplot(2, 2, (1, 2))\n    ax1.plot(data['ds'], data[\"resid\"])\n    ax1.set_title(\"Innovation Residuals\")\n\n    ax2 = fig.add_subplot(2, 2, 3)\n    plot_acf(data[\"resid\"].dropna(), ax=ax2, zero=False,\n             bartlett_confint=False, auto_ylims=True)\n    ax2.set_title(\"ACF Plot\")\n    ax2.set_xlabel('lag[1]')\n\n    ax3 = fig.add_subplot(2, 2, 4)\n    ax3.hist(data[\"resid\"], bins=20)\n    ax3.set_title(\"Histogram\")\n    ax3.set_xlabel(\".resid\")\n    ax3.set_ylabel(\"Count\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n# --- Build a diagnostics-ready training dataframe with residuals ---\n\n# residuals that we already computed on the fitted window\n# (keys: ['unique_id','ds','resid'])\n# resids = merged_fitted.assign(\n#     resid = merged_fitted[\"y\"] - merged_fitted[adj_col] - merged_fitted[sea_col]\n# )[[\"unique_id\", \"ds\", \"resid\"]]\n\n# keep only training rows (the ones you modeled)\ntrain_df = (\n    us_retail_employment_df.loc[:, [\"unique_id\", \"ds\", \"y\"]]   # base training info\n    .merge(resids, on=[\"unique_id\", \"ds\"], how=\"left\")         # attach residuals by key\n    .dropna(subset=[\"resid\"])                                  # keep rows that have residuals\n    .reset_index(drop=True)\n)\n\n# (Optional) make sure types are numeric\ntrain_df[\"y\"] = pd.to_numeric(train_df[\"y\"], errors=\"coerce\")\ntrain_df[\"resid\"] = pd.to_numeric(train_df[\"resid\"], errors=\"coerce\")\n\n# now this has a 'resid' column and will work with your diagnostic plotter\nplot_diagnostics(train_df)"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#training-and-test-sets",
    "href": "lecture_slides/08_time_series/08_time_series.html#training-and-test-sets",
    "title": "Overview",
    "section": "Training and Test Sets",
    "text": "Training and Test Sets\nTo properly evaluate forecast accuracy, it is essential to assess model performance on genuine forecasts, not on the residuals from the fitted model.\nResiduals reflect in-sample fit, which may underestimate the magnitude of true forecast errors. Reliable accuracy assessment requires testing the model on unseen data.\n\nData Partitioning for Model Evaluation\nIt is standard practice to divide the available data into two segments:\n\nTraining Set:\n\nUsed to fit the model and estimate parameters.\n\nTest Set:\n\nUsed exclusively to evaluate forecast performance on data not used during model fitting.\n\n\nBecause the test data are withheld from model training, performance metrics calculated on this portion offer a realistic measure of out-of-sample accuracy — indicating how well the model generalizes to future observations.\n\n\nData Partitioning for Model Evaluation\n\n\n\n\n\nXXX include figure train_test_split XXX"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#test-set-size-and-key-considerations",
    "href": "lecture_slides/08_time_series/08_time_series.html#test-set-size-and-key-considerations",
    "title": "Overview",
    "section": "Test Set Size and Key Considerations",
    "text": "Test Set Size and Key Considerations\nThe test set typically represents about 20% of the total sample, though the exact proportion depends on:\n\nThe length of the dataset, and\nThe forecast horizon of interest.\n\nIdeally, the test set should be at least as long as the maximum forecast horizon being evaluated.\n\nImportant Guidelines\n\nA model that fits the training data well does not guarantee good forecast performance.\nA perfect in-sample fit can always be achieved with enough parameters — but this leads to overfitting.\nOverfitting is as detrimental as underfitting, as it captures noise rather than genuine patterns."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#percentage-errors",
    "href": "lecture_slides/08_time_series/08_time_series.html#percentage-errors",
    "title": "Overview",
    "section": "Percentage Errors",
    "text": "Percentage Errors\nThe percentage error is defined as:\n\\[\np_t = 100 \\frac{e_t}{y_t},\n\\]\nwhere \\(e_t\\) is the forecast error.\nPercentage errors are unit-free, allowing for easy comparison of forecasting performance across different datasets.\n\nMean Absolute Percentage Error (MAPE)\n\\[\n\\text{MAPE} = \\text{mean}(|p_t|).\n\\]\nLimitations:\n\nUndefined when \\(y_t = 0\\) and unstable when \\(y_t\\) is close to zero.\nAssumes a meaningful zero point in the measurement scale (not applicable for temperatures in °C or °F).\nPenalizes negative errors more heavily than positive ones.\n\n\n\nSymmetric Mean Absolute Percentage Error (sMAPE)\nProposed by Armstrong (1978): \\[\n\\text{sMAPE} = \\text{mean}\\left(200 \\frac{|y_t - \\hat{y}_t|}{y_t + \\hat{y}_t}\\right).\n\\]\nHowever:\n\nStill unstable for small \\(y_t\\) and \\(\\hat{y}_t\\).\nCan produce negative values.\nHyndman and Koehler (2006) recommend avoiding sMAPE due to these issues."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#scaled-errors",
    "href": "lecture_slides/08_time_series/08_time_series.html#scaled-errors",
    "title": "Overview",
    "section": "Scaled Errors",
    "text": "Scaled Errors\nTo overcome the limitations of percentage errors, scaled errors (Hyndman & Koehler, 2006) standardize forecast errors relative to a simple benchmark method (typically the naïve forecast).\n\nNon-Seasonal Scaled Error\n\\[\nq_j = \\frac{e_j}\n{\\dfrac{1}{T-1} \\sum_{t=2}^{T} |y_t - y_{t-1}| }.\n\\]\n\nIndependent of the data scale.\n\\(|q_j| &lt; 1\\) indicates a better forecast than the average one-step naïve forecast.\n\\(|q_j| &gt; 1\\) indicates a worse forecast.\n\n\n\nScaled Errors for Seasonal Time Series\nFor seasonal data, define the scaled error as: \\[\nq_j =\n\\frac{e_j}\n{\\dfrac{1}{T - m} \\sum_{t = m + 1}^{T} |y_t - y_{t - m}| }.\n\\]\n\n\nMean Absolute Scaled Error (MASE)\n\\[\n\\text{MASE} = \\text{mean}(|q_j|).\n\\]\n\n\nRoot Mean Squared Scaled Error (RMSSE)\n\\[\n\\text{RMSSE} = \\sqrt{\\text{mean}(q_j^2)},\n\\]\nwhere \\[\nq_j^2 =\n\\frac{e_j^2}\n{\\dfrac{1}{T - m} \\sum_{t = m + 1}^{T} (y_t - y_{t - m})^2 }.\n\\]\nFor non-seasonal data, set \\(m = 1\\).\nScaled errors like MASE and RMSSE provide reliable, comparable, and scale-independent measures of forecast accuracy across different datasets and models."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#example-forecasts-of-australian-quarterly-beer-production-using-data-up-to-the-end-of-2007",
    "href": "lecture_slides/08_time_series/08_time_series.html#example-forecasts-of-australian-quarterly-beer-production-using-data-up-to-the-end-of-2007",
    "title": "Overview",
    "section": "Example: Forecasts of Australian quarterly beer production using data up to the end of 2007",
    "text": "Example: Forecasts of Australian quarterly beer production using data up to the end of 2007\n\n# Forecasts for quarterly beer production (train ≤ 2007; test &gt; 2007)\n# Each method plotted with a distinct color. Actuals rendered in gray dashed style.\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import HistoricAverage, Naive, RandomWalkWithDrift, SeasonalNaive\n\n# Ensure default color cycle (in case a global monochrome was set earlier)\nmpl.rcParams['axes.prop_cycle'] = mpl.rcParamsDefault['axes.prop_cycle']\n\nbeers_df = beer\ntrain_df = beers_df.query(\"ds.dt.year &lt;= 2007\").reset_index(drop=True)\ntest_df  = beers_df.query(\"ds.dt.year &gt; 2007\").reset_index(drop=True)\n\nmean_method   = HistoricAverage()\nnaive_method  = Naive()\ndrift_method  = RandomWalkWithDrift()\nseasonal_nav  = SeasonalNaive(season_length=4)\n\nsf = StatsForecast(models=[drift_method, mean_method, naive_method, seasonal_nav], freq=\"Q\")\npreds = sf.forecast(df=train_df, h=len(test_df)).reset_index(drop=True)\npreds[\"y_actual\"] = test_df[\"y\"].to_numpy()\n\n# Robust mapping from model columns to legend labels\nlabel_map = {\n    \"RandomWalkWithDrift\": \"Drift\", \"RWD\": \"Drift\",\n    \"HistoricAverage\": \"Mean\",\n    \"Naive\": \"Naïve\",\n    \"SeasonalNaive\": \"Seasonal naïve\",\n}\n# Palette to force non-black method colors\npalette = {\n    \"Drift\": \"tab:orange\",\n    \"Mean\": \"tab:blue\",\n    \"Naïve\": \"tab:green\",\n    \"Seasonal naïve\": \"tab:purple\",\n}\nmodel_cols = [c for c in preds.columns if c not in (\"unique_id\", \"ds\", \"y_actual\")]\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Actuals in gray, dashed (applies to full history line)\nax.plot(\n    beers_df[\"ds\"], beers_df[\"y\"],\n    #color=\"0.45\", linestyle=\"--\", linewidth=1.6, label=\"Actuals\", zorder=1\n    color=\"0.45\", linestyle=\"-\", linewidth=1, label=\"Actuals\", zorder=1\n)\n\n# Forecasts with explicit colors\nfor c in model_cols:\n    lbl = label_map.get(c, c)\n    ax.plot(\n        preds[\"ds\"], preds[c],\n        linewidth=2.4, color=palette.get(lbl, None), label=lbl, zorder=2\n    )\n\n# Optional: overlay test actuals (same gray/dashed, no extra legend entry)\n#ax.plot(test_df[\"ds\"], test_df[\"y\"], color=\"0.45\", linestyle=\"--\", linewidth=1.6, zorder=1)\nax.plot(test_df[\"ds\"], test_df[\"y\"], color=\"0.45\", linewidth=1.6, zorder=1)\n\nax.set_title(\"Forecasts for quarterly beer production\")\nax.set_xlabel(\"Quarter\")\nax.set_ylabel(\"Megalitres\")\nax.legend(title=\"Forecast\", loc=\"best\", frameon=True)\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nExample: Forecast accuracy measures for the four forecast methods applied to the quarterly Australian beer production\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# 1) Prepare predictions frame\n# ----------------------------\npreds_eval = preds.copy()\n# Your frame likely has the actuals in 'y_actual'; rename to 'y' if needed\nif \"y\" not in preds_eval and \"y_actual\" in preds_eval:\n    preds_eval = preds_eval.rename(columns={\"y_actual\": \"y\"})\n\n# Identify model columns (everything except id/time/actuals)\nfixed_cols = {\"unique_id\", \"ds\", \"y\"}\nmodel_cols = [c for c in preds_eval.columns if c not in fixed_cols]\n\n# ----------------------------\n# 2) Helper: groupwise metrics\n# ----------------------------\ndef _rmse(y, yhat):\n    return float(np.sqrt(np.mean((y - yhat) ** 2)))\n\ndef _mae(y, yhat):\n    return float(np.mean(np.abs(y - yhat)))\n\ndef _mape(y, yhat):\n    y = np.asarray(y, dtype=float)\n    yhat = np.asarray(yhat, dtype=float)\n    mask = y != 0\n    return float(np.mean(np.abs((y[mask] - yhat[mask]) / y[mask])) * 100.0)\n\ndef _mase(y_true, y_hat, y_train, seasonality: int = 1):\n    \"\"\"\n    Mean Absolute Scaled Error\n    MASE = MAE(y_true, y_hat) / MAE of seasonal naive on training set\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_hat  = np.asarray(y_hat,  dtype=float)\n    y_train = np.asarray(y_train, dtype=float)\n\n    m = int(seasonality)\n    if m &lt;= 0 or len(y_train) &lt;= m:\n        return np.nan\n    denom = np.mean(np.abs(y_train[m:] - y_train[:-m]))\n    if denom == 0 or not np.isfinite(denom):\n        return np.nan\n    return float(np.mean(np.abs(y_true - y_hat)) / denom)\n\n# Pre-compute MASE denominators per series (id)\nseasonality = 4  # quarterly; set to 1 for non-seasonal data\ntrain_mae_denom = (\n    train_df.sort_values([\"unique_id\", \"ds\"])\n            .groupby(\"unique_id\", as_index=True)[\"y\"]\n            .apply(lambda y: np.mean(np.abs(y.values[seasonality:] - y.values[:-seasonality]))\n                   if len(y) &gt; seasonality else np.nan)\n            .to_dict()\n)\n\n# ----------------------------\n# 3) Compute metrics per model\n# ----------------------------\nrows = []\nfor model in model_cols:\n    # group by series id and aggregate; average (weighted by number of obs) across ids\n    per_id = []\n    for uid, g in preds_eval.groupby(\"unique_id\"):\n        y_true = g[\"y\"].values\n        y_hat  = g[model].values\n        rmse_i = _rmse(y_true, y_hat)\n        mae_i  = _mae(y_true, y_hat)\n        mape_i = _mape(y_true, y_hat)\n        denom  = train_mae_denom.get(uid, np.nan)\n        mase_i = (mae_i / denom) if denom and np.isfinite(denom) else np.nan\n        per_id.append((len(g), rmse_i, mae_i, mape_i, mase_i))\n\n    # length-weighted average across ids\n    n = sum(k[0] for k in per_id)\n    rmse_w = sum(k[0]*k[1] for k in per_id) / n\n    mae_w  = sum(k[0]*k[2] for k in per_id) / n\n    mape_w = sum(k[0]*k[3] for k in per_id) / n\n    mase_w = (sum(k[0]*k[4] for k in per_id if np.isfinite(k[4])) /\n              sum(k[0]       for k in per_id if np.isfinite(k[4]))) if any(np.isfinite(k[4]) for k in per_id) else np.nan\n\n    rows.append({\"model\": model, \"rmse\": rmse_w, \"mae\": mae_w, \"mape\": mape_w, \"mase\": mase_w})\n\nevaluation = pd.DataFrame(rows).set_index(\"model\").sort_index()\nevaluation\n\n\n    \n\n\n\n\n\n\nrmse\nmae\nmape\nmase\n\n\nmodel\n\n\n\n\n\n\n\n\nHistoricAverage\n36.484\n24.414\n5.521\n1.543\n\n\nNaive\n62.693\n57.400\n14.184\n3.629\n\n\nRWD\n66.978\n60.230\n14.938\n3.808\n\n\nSeasonalNaive\n14.311\n13.400\n3.169\n0.847\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nEvaluating Forecast Accuracy\nThe evaluate() function automatically selects the relevant time periods from the data (e.g., production values) to align with the corresponding forecast horizon when calculating multiple accuracy metrics.\n\n\nComparing Forecasting Methods\nFrom the analysis:\n\nThe seasonal naïve method provides the best performance for these data, as clearly shown in the graph.\nAlthough effective, this method can still be refined further for improved accuracy.\nIn some cases, different accuracy measures may yield different rankings of forecasting methods.\n\nHere, however, all measures consistently identify the seasonal naïve approach as the best among the four tested methods.\n\n\n\n\nExample: Non-Seasonal Example — Google Stock Price\nA separate example uses Google’s daily closing stock prices from 2015, with forecasts for January 2016 generated from three non-seasonal methods.\nThe results illustrate how forecast accuracy evaluation helps identify which approach best captures short-term market movements in non-seasonal data.\n\nimport matplotlib.pyplot as plt\n\n# Select all of 2015 and January 2016 (inclusive)\nmask = (goog[\"ds\"] &gt;= \"2015-01-01\") & (goog[\"ds\"] &lt;= \"2016-01-31\")\ngoog_df = goog.loc[mask].sort_values(\"ds\").copy()\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(goog_df[\"ds\"], goog_df[\"y\"], linewidth=1.6)\nax.set_title(\"Google daily closing stock prices: 2015 and Jan 2016\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"$USD\")\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: Non-Seasonal Example — Google Stock Price\n\n# Google — 2015 train, Jan 2016 test; show forecasts in color (not black)\n# Fix: avoid pandas .query with .dt accessors; use boolean masks instead.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import HistoricAverage, Naive, RandomWalkWithDrift\n\ngoog_df = goog.loc[(goog[\"ds\"] &gt;= \"2015-01-01\") & (goog[\"ds\"] &lt;= \"2016-01-31\")].copy()\n\n# --- Prepare contiguous trading-day index for plotting ---\ntrain_df = (\n    goog_df.loc[goog_df[\"ds\"].dt.year == 2015, [\"unique_id\", \"ds\", \"y\"]]\n    .reset_index(drop=True)\n    .copy()\n)\ntest_df = (\n    goog_df.loc[(goog_df[\"ds\"].dt.year == 2016) & (goog_df[\"ds\"].dt.month == 1),\n                [\"unique_id\", \"ds\", \"y\"]]\n    .reset_index(drop=True)\n    .copy()\n)\n\n# integer index for StatsForecast\ntrain_df[\"ds\"] = np.arange(len(train_df), dtype=int)\n# align test dates right after the training window for plotting/overlay\ntest_df[\"ds\"] = np.arange(len(train_df), len(train_df) + len(test_df), dtype=int)\n\n# --- Fit models & forecast for the Jan 2016 horizon ---\nsf = StatsForecast(models=[RandomWalkWithDrift(), HistoricAverage(), Naive()], freq=1)\npreds = sf.forecast(df=train_df, h=len(test_df))\n\n# Add actual Jan 2016 values for reference (optional)\npreds = preds.merge(test_df[[\"unique_id\", \"ds\", \"y\"]], on=[\"unique_id\", \"ds\"], how=\"left\")\n\n# Normalize (model) column names across StatsForecast versions\nmodel_cols = [c for c in preds.columns if c not in (\"unique_id\", \"ds\", \"y\")]\nrename = {}\nfor c in model_cols:\n    cl = c.lower()\n    if cl in (\"rwd\", \"randomwalkwithdrift\"): rename[c] = \"Drift\"\n    elif \"historicaverage\" in cl or cl == \"mean\": rename[c] = \"Mean\"\n    elif cl == \"naive\": rename[c] = \"Naive\"\npreds = preds.rename(columns=rename)\n\n# --- Plot (actuals in gray; forecasts in color) ---\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Training and holdout actuals\nax.plot(train_df[\"ds\"], train_df[\"y\"], color=\"0.45\", linestyle=\"--\", linewidth=1.6, label=\"Actuals (2015)\")\nax.plot(test_df[\"ds\"],  test_df[\"y\"],  color=\"0.45\", linestyle=\"--\", linewidth=1.6, label=\"Actuals (Jan 2016)\")\n\n# Colored forecasts\ncolor_map = {\"Drift\": \"tab:orange\", \"Mean\": \"tab:blue\", \"Naive\": \"tab:green\"}\nfor name, color in color_map.items():\n    if name in preds.columns:\n        ax.plot(preds[\"ds\"], preds[name], linewidth=2.4, label=name, color=color)\n\nax.set_title(\"Google closing stock prices: 2015 (train) and Jan 2016 (test)\")\nax.set_xlabel(\"day index\")\nax.set_ylabel(\"$US\")\nax.legend(title=\"Forecast\", loc=\"best\", frameon=True)\nax.grid(True, linewidth=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExample: Non-Seasonal Example — Google Stock Price\n\n# --- Accuracy table for 2016 holdout (RMSE, MAE, MAPE, MASE) ---\n\nimport numpy as np\nimport pandas as pd\n\n# Keep only the 2016 horizon\nh_start, h_end = test_df[\"ds\"].min(), test_df[\"ds\"].max()\nholdout = preds.loc[(preds[\"ds\"] &gt;= h_start) & (preds[\"ds\"] &lt;= h_end)].copy()\n\n# Map model display names to whatever column name exists in `preds`\nmethod_columns = {\n    \"RWD\": [\"RWD\", \"RandomWalkWithDrift\", \"Drift\"],\n    \"HistoricAverage\": [\"HistoricAverage\", \"Mean\"],\n    \"Naive\": [\"Naive\"],\n}\nresolved_cols = {}\nfor disp, candidates in method_columns.items():\n    for c in candidates:\n        if c in holdout.columns:\n            resolved_cols[disp] = c\n            break\n\n# In-sample denominator for MASE (m=1 for daily prices)\ny_tr = train_df[\"y\"].to_numpy()\nmase_denom = np.mean(np.abs(y_tr[1:] - y_tr[:-1]))\n\ndef _rmse(y, yhat):\n    return float(np.sqrt(np.mean((y - yhat) ** 2)))\n\ndef _mae(y, yhat):\n    return float(np.mean(np.abs(y - yhat)))\n\ndef _mape(y, yhat):\n    # guard (not needed for prices, but keeps it robust)\n    y_safe = np.where(y == 0, np.nan, y)\n    return float(np.nanmean(np.abs((y - yhat) / y_safe)) * 100.0)\n\nrows = []\ny_true = holdout[\"y\"].to_numpy()\n\nfor disp_name in [\"RWD\", \"HistoricAverage\", \"Naive\"]:\n    if disp_name not in resolved_cols:\n        continue\n    yhat = holdout[resolved_cols[disp_name]].to_numpy()\n    mae = _mae(y_true, yhat)\n    row = {\n        \"Method\": disp_name,\n        \"RMSE\": _rmse(y_true, yhat),\n        \"MAE\": mae,\n        \"MAPE\": _mape(y_true, yhat),\n        \"MASE\": mae / mase_denom if mase_denom &gt; 0 else np.nan,\n    }\n    rows.append(row)\n\nacc_df = pd.DataFrame(rows, columns=[\"Method\", \"RMSE\", \"MAE\", \"MAPE\", \"MASE\"])\n\n# Format to 3 decimals to match the screenshot\nacc_df[[\"RMSE\", \"MAE\", \"MAPE\", \"MASE\"]] = acc_df[[\"RMSE\", \"MAE\", \"MAPE\", \"MASE\"]].round(3)\n\n# (Optional) pretty print similar to the attachment\nacc_df_style = (\n    acc_df.style\n    .hide(axis=\"index\")\n    .format({\"RMSE\": \"{:.3f}\", \"MAE\": \"{:.3f}\", \"MAPE\": \"{:.3f}\", \"MASE\": \"{:.3f}\"})\n    .set_table_styles(\n        [\n            {\"selector\": \"th\", \"props\": [(\"background-color\", \"#8c8c8c\"), (\"color\", \"white\"), (\"font-weight\", \"bold\")]},\n            {\"selector\": \"td\", \"props\": [(\"background-color\", \"#f6f6f6\")]}\n        ],\n        overwrite=False,\n    )\n)\nacc_df, acc_df_style  # display(acc_df_style) in notebooks\ndisplay(acc_df_style)\n\n\n\n\n\n\nMethod\nRMSE\nMAE\nMAPE\nMASE\n\n\n\n\nRWD\n53.070\n49.824\n6.992\n6.990\n\n\nHistoricAverage\n118.032\n116.945\n16.235\n16.406\n\n\nNaive\n43.432\n40.384\n5.673\n5.666"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#example-forecast-horizon-accuracy-with-cross-validation",
    "href": "lecture_slides/08_time_series/08_time_series.html#example-forecast-horizon-accuracy-with-cross-validation",
    "title": "Overview",
    "section": "Example: Forecast Horizon Accuracy with Cross-Validation",
    "text": "Example: Forecast Horizon Accuracy with Cross-Validation\nThe google_2015 subset of the gafa_stock dataset contains daily closing stock prices for Google Inc. from the NASDAQ exchange throughout 2015.\nIn this example, the code evaluates the forecasting performance of drift forecasts across 1- to 8-step-ahead horizons using time series cross-validation.\nHere, horizons are the lead times we are evaluating—i.e., the number of business days ahead each forecast targets, given freq=\"B\" and a daily rolling origin."
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#example-forecast-horizon-accuracy-with-cross-validation-1",
    "href": "lecture_slides/08_time_series/08_time_series.html#example-forecast-horizon-accuracy-with-cross-validation-1",
    "title": "Overview",
    "section": "Example: Forecast Horizon Accuracy with Cross-Validation",
    "text": "Example: Forecast Horizon Accuracy with Cross-Validation\nConcretely:\n\nh = 1 → next-business-day forecasts (e.g., cutoff 2015-01-06 → predict ds 2015-01-07).\nh = 2 → two-business-days-ahead forecasts (cutoff t → predict ds t+2B).\n…\nh = 8 → eight-business-days-ahead forecasts.\n\nFor each h in np.arange(1, 9), cross_validation(h=h, step_size=1, test_size=249) rolls the origin one business day at a time across 2015, computes all h-step-ahead predictions with RandomWalkWithDrift, and evaluate(..., metrics=[rmse]) aggregates the RMSE across those folds for that specific horizon. The scatter then plots RMSE vs. forecast horizon (1–8 business days ahead).\nThe resulting plot demonstrates that forecast error increases with the forecast horizon, which is the expected outcome — as longer-term forecasts naturally accumulate more uncertainty and modeling error over time.\n\nrmses = []\ndrift_model = RandomWalkWithDrift()\nhorizons = np.arange(1, 9)\n\nfor horizon in horizons:\n    sf = StatsForecast(models=[drift_model], freq=\"B\")\n    cv_df = sf.cross_validation(h=horizon, df=goog_2015, step_size=1, test_size=249)\n    cv_evaluation = evaluate(cv_df, metrics=[rmse], models=[\"RWD\"])\n    rmses.append(cv_evaluation[\"RWD\"].iloc[0])\n\nfig, ax = plt.subplots()\nax.scatter(horizons, rmses)\nax.set_xlabel(\"h\")\nax.set_ylabel(\"RMSE\")\nax.set_title(\"RMSE vs horizon for drift method (Google 2015)\")\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#synthetic-time-series-for-demonstration",
    "href": "lecture_slides/08_time_series/08_time_series.html#synthetic-time-series-for-demonstration",
    "title": "Overview",
    "section": "Synthetic Time Series for Demonstration",
    "text": "Synthetic Time Series for Demonstration\nWe create a simple seasonal signal with noise to illustrate temporal splits, naive baselines, direct/recursive reduction, and rolling backtests.\nSynthetic data and seasonal period\n\nThe series is daily and length 400.\nSignal = sinusoid \\(+;\\) noise \\(+;\\) linear trend:\n\\[\ny_t=\\sin(t/12)+0.1,\\varepsilon_t+0.01,t,\n\\]\nwhich creates a smooth seasonal‐like cycle and a mild upward trend.\nSP = 75 is the assumed season length (approximately the sinusoid’s cycle in days).\n\n\n# -----------------------------\n# Synthetic data\nn = 400\nidx = pd.date_range(\"2015-01-01\", periods=n, freq=\"D\")\ny = pd.Series(\n    np.sin(np.arange(n) / 12.0) + 0.1 * np.random.randn(n) + 0.01 * np.arange(n),\n    index=idx,\n    name=\"y\",\n)\n\nSP = 75  # seasonal period\n\ny.head(), y.tail()\n\n\n(2015-01-01    0.104\n 2015-01-02    0.026\n 2015-01-03    0.298\n 2015-01-04    0.206\n 2015-01-05    0.350\n Freq: D, Name: y, dtype: float64,\n 2016-01-31    4.970\n 2016-02-01    4.902\n 2016-02-02    4.751\n 2016-02-03    5.126\n 2016-02-04    5.009\n Freq: D, Name: y, dtype: float64)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=y)\nplt.title(\"Synthetic Time Series Data (y)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Value\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#temporal-split-and-naive-benchmark",
    "href": "lecture_slides/08_time_series/08_time_series.html#temporal-split-and-naive-benchmark",
    "title": "Overview",
    "section": "Temporal Split and Naive Benchmark",
    "text": "Temporal Split and Naive Benchmark\nWe use a last-value naive forecaster as a baseline and compute MAPE on a 30-step horizon.\nGoal Set up a fair, time-aware test for forecasting. Start with an easy-to-understand baseline (the “last value” forecast) and measure how far off it is using MAPE.\n1) Split by time (no leakage)\n\ntemporal_train_test_split(y, test_size=30) cuts the series into two parts in order: train = everything up to the split; test = the last 30 points.\nWe never let the model see the test part while training. This prevents look-ahead bias (using future info to predict the future).\n\n2) Tell the model how far ahead to predict (the horizon fh)\n\nfh = np.arange(1, len(y_test) + 1) creates the steps we want: 1, 2, …, 30 days ahead.\nThis is a relative horizon: “from the last training day, predict 1 step ahead, then 2, …”. It works regardless of whether the index is dates or simple integers.\n\n3) Baseline forecast: “last value”\n\nNaiveForecaster(strategy=\"last\") always predicts the last value from the training set for every future step.\nfit(y_train) just remembers that last value; no parameters are learned.\npredict(fh) returns a flat line (same number repeated for all 30 steps).\nWhy use it? It’s a sanity check. If a fancy model can’t beat this, it’s not adding value.\n\n4) How we score it: MAPE\n\nmean_absolute_percentage_error(y_test, y_pred_last) computes MAPE = average of \\(|y_t - \\hat y_t| / |y_t| \\times 100%\\).\nInterpretation: on average, how many percent off are our forecasts on the 30 test points?\nCaution: if any true value is 0 (or very close), MAPE can blow up. In those cases also report MAE or RMSE, or use sMAPE.\n\n5) Why this setup matters\n\nFairness: time-ordered split avoids peeking into the future.\nBenchmarking: gives a clear bar to beat (mape_last).\nReproducibility: explicit horizon + explicit metric = results others can replicate.\n\n6) Helpful variations\n\nSeasonal data: NaiveForecaster(strategy=\"seasonal_last\", sp=m) uses the last value from the same season (e.g., same month last year if m=12).\nTrend: strategy=\"drift\" extrapolates a straight line from the first to the last training points.\nDiagnostics: check error by step ahead (1-step vs 30-step) to see where accuracy decays.\n\nBottom line You build a clean time-based test, create a simple carry-forward forecast, and compute its MAPE. Any model you try next must deliver a lower MAPE than this baseline to be worth using.\n\n# Temporal split\ny_train, y_test = temporal_train_test_split(y, test_size=30)\nfh = np.arange(1, len(y_test) + 1)  # forecast horizon steps ahead\n\n# Naive benchmark\nf_last = NaiveForecaster(strategy=\"last\")\nf_last.fit(y_train)\ny_pred_last = f_last.predict(fh)\n\nmape_last = mean_absolute_percentage_error(y_test, y_pred_last)\nmape_last\n\nnp.float64(0.2887089356544459)\n\n\n\ny_train.tail()\n\n\n\n\n\n\n\n\ny\n\n\n\n\n2016-01-01\n2.753\n\n\n2016-01-02\n2.904\n\n\n2016-01-03\n2.848\n\n\n2016-01-04\n3.204\n\n\n2016-01-05\n2.989\n\n\n\n\ndtype: float64\n\n\n\ny_test.head()\n\n\n\n\n\n\n\n\ny\n\n\n\n\n2016-01-06\n3.227\n\n\n2016-01-07\n3.189\n\n\n2016-01-08\n3.468\n\n\n2016-01-09\n3.215\n\n\n2016-01-10\n3.306\n\n\n\n\ndtype: float64\n\n\n\ny_pred_last.head()\n\n\n\n\n\n\n\n\ny\n\n\n\n\n2016-01-06\n2.989\n\n\n2016-01-07\n2.989\n\n\n2016-01-08\n2.989\n\n\n2016-01-09\n2.989\n\n\n2016-01-10\n2.989\n\n\n\n\ndtype: float64\n\n\n\n# Actual vs Predicted time series plot\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sktime.utils.plotting import plot_series\n\n# Ensure forecast is indexed on the test horizon (safe for any forecaster)\ny_pred_last = pd.Series(y_pred_last, index=y_test.index, name=\"Predicted\")\n\n# Pure matplotlib\nfig, ax = plt.subplots(figsize=(10, 4))\ny_train.plot(ax=ax, label=\"Train\")\ny_test.plot(ax=ax, label=\"Actual\")\ny_pred_last.plot(ax=ax, label=\"Predicted\", linestyle=\"--\")\nax.set_title(\"Actual vs Predicted\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"y\")\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#theta-forecasting-model",
    "href": "lecture_slides/08_time_series/08_time_series.html#theta-forecasting-model",
    "title": "Overview",
    "section": "Theta forecasting model",
    "text": "Theta forecasting model\nThe code fits a Theta forecasting model (without deseasonalizing) and compares it to a naïve baseline using MAPE on a held-out test window. It avoids “multiplicative seasonality” because the series can take zero/negative values (multiplicative seasonal models require strictly positive data).\n\n\nKey objects assumed from earlier cells\n\ny_train, y_test: time-ordered split of the original series (y), typically produced by temporal_train_test_split.\nfh: the forecasting horizon, e.g., fh = [1,2,…,H], indicating how many steps ahead to predict from the end of y_train.\ny_pred_last, mape_last: predictions and MAPE from the naïve benchmark (carry-forward “last value”).\n\nWhy split by time? We must preserve ordering to avoid look-ahead bias. Training uses only past data; evaluation is on future points.\n\n\n\nBaseline metric: MAPE\nThe mean absolute percentage error is\n\\[\n\\text{MAPE} = \\frac{100%}{H}\\sum_{h=1}^{H}\\left|\\frac{y_{T+h}-\\hat y_{T+h}}{y_{T+h}}\\right|,\n\\]\nwhere:\n\n\\(T\\) = last index in the training set,\n\\(H = |fh|\\) = number of forecast steps,\n\\(y_{T+h}\\) = actual value (h) steps ahead,\n\\(\\hat{y}_{T+h}\\) = forecast.\n\nLower is better. MAPE becomes unstable if any (y_{T+h}).\n\n\n\nWhy “multiplicative seasonality” is problematic here\nA seasonal component can enter a model additively or multiplicatively:\n\nAdditive: \\(y_t = \\text{level}_t + \\text{trend}_t + s_t + \\varepsilon_t\\)\nMultiplicative: \\(y_t = \\text{level}_t \\times \\text{trend}_t \\times s_t \\times (1+\\varepsilon_t)\\)\n\nMultiplicative seasonality needs strictly positive (y_t). If the series can be zero/negative (common after centering or with noise), statsmodels raises the error you saw. The code therefore does not use multiplicative deseasonalization.\n\n\n\nModel A — ThetaForecaster (no deseasonalization)\ntheta = ThetaForecaster(sp=1, deseasonalize=False)\ntheta.fit(y_train)\ny_pred_theta = theta.predict(fh)\nmape_theta = mean_absolute_percentage_error(y_test, y_pred_theta)\nWhat is the Theta method? The Theta method (Assimakopoulos & Nikolopoulos, 2000) modifies the local curvature of the series by applying theta lines to the second differences. It blends two extrapolations:\n\nA local level/trend component (similar to Simple Exponential Smoothing with drift),\nA long-term mean reversion component.\n\nA useful way to view Theta is its equivalence (under conditions) to SES with drift (Hyndman & Billah, 2003):\n\nSES update:\n\\[\n\\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}\n\\]\n(h)-step forecast with drift:\n\\[\n\\hat y_{t+h|t} = \\ell_t + d\\cdot h,\n\\]\nwhere (d) is a data-driven drift term capturing trend.\n\nSetting sp=1 tells ThetaForecaster that there is no seasonal period to remove internally. deseasonalize=False ensures the model does not attempt multiplicative/seasonal decomposition (the source of the earlier error). The forecaster then extrapolates level + trend only.\nIntuition:\nTheta smooths the series to estimate a stable current level and a gentle slope, then projects that slope forward. It’s robust for many series with mild trends.\n\n\n\nComparing models and choosing the winner\nprint({\"MAPE_last\": float(mape_last), \"MAPE_theta\": float(mape_theta)})\n\nbest_name, best_mape, best_pred = min(\n    [(\"naive\", mape_last, y_pred_last), (\"theta_nodeseason\", mape_theta, y_pred_theta)],\n    key=lambda x: x[1]\n)\nprint({\"best_model\": best_name, \"best_mape\": float(best_mape)})\n\nWe report the baseline error (MAPE_last) and Theta’s error (MAPE_theta).\nWe select the model with the lower MAPE. If Theta &lt; naïve, we have demonstrated value over a trivial benchmark.\n\n\n\n\nWhere ETS fits conceptually (mentioned in comments)\nAlthough not executed in this snippet, Exponential Smoothing (ETS) models combine:\n\nError type (additive/multiplicative),\nTrend (none/additive/damped),\nSeasonality (none/additive/multiplicative) with period (s).\n\nA common additive ETS specification is:\n\\[\n\\begin{aligned}\ny_t &= \\ell_{t-1} + b_{t-1} + s_{t-s} + \\varepsilon_t,\\\n\\ell_t &= \\ell_{t-1} + b_{t-1} + \\alpha \\varepsilon_t,\\\nb_t &= b_{t-1} + \\beta \\varepsilon_t,\\\ns_t &= s_{t-s} + \\gamma \\varepsilon_t,\n\\end{aligned}\n\\]\nwith smoothing parameters (,,(0,1)). ETS is appropriate for additive seasonality and can complement Theta by explicitly modeling seasonal cycles.\n\n\n\nPractical checklist\n\nAlways build a naïve baseline and compute its MAPE.\nUse time-ordered splits and an explicit horizon.\nIf your series can be zero/negative, avoid multiplicative seasonality. Prefer additive seasonality or set sp=1 and handle trend only.\nCompare models on the same horizon and same test window.\nVisualize actuals vs. forecasts and inspect step-ahead errors to understand where accuracy degrades.\n\nTakeaway: the code fits a trend-focused Theta model that sidesteps multiplicative-seasonality restrictions, evaluates it with MAPE on a proper time split, and programmatically selects the better performer relative to the naïve standard.\n\n# Robust alternative that avoids the multiplicative-seasonality error.\n# Use non-deseasonalized Theta (trend-focused) plus ETS with additive seasonality.\n\nimport numpy as np\nimport pandas as pd\n\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n\nfrom sktime.forecasting.theta import ThetaForecaster\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\n\n# ----- Model A: Theta without internal deseasonalization (avoids multiplicative issue) -----\n# Focuses on level/trend; we'll let ETS handle seasonality.\ntheta = ThetaForecaster(sp=1, deseasonalize=False)\ntheta.fit(y_train)\ny_pred_theta = theta.predict(fh)\nmape_theta = mean_absolute_percentage_error(y_test, y_pred_theta)\n\nprint({\n    \"MAPE_last\": float(mape_last),\n    \"MAPE_theta\": float(mape_theta),\n})\n\n# Choose the better of the two candidate models\nbest_name, best_mape, best_pred = min(\n    [(\"naive\", mape_last, y_pred_last), (\"theta_nodeseason\", mape_theta, y_pred_theta)],\n    key=lambda x: x[1]\n)\nprint({\"best_model\": best_name, \"best_mape\": float(best_mape)})\n\n{'MAPE_last': 0.2887089356544459, 'MAPE_theta': 0.2658110643420696}\n{'best_model': 'theta_nodeseason', 'best_mape': 0.2658110643420696}\n\n\n\n# Plot actuals and forecasts (naive, theta, ETS) with distinct colors/styles\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Ensure all forecasts are Series aligned to the test index\ny_pred_last  = pd.Series(y_pred_last,  index=y_test.index, name=\"Naive\")\ny_pred_theta = pd.Series(y_pred_theta, index=y_test.index, name=\"Theta\")\n\nfig, ax = plt.subplots(figsize=(12, 4))\n\n# Actuals\ny_train.plot(ax=ax, color=\"0.7\", label=\"Train\")\ny_test.plot(ax=ax, color=\"black\", linewidth=2, label=\"Actual\")\n\n# Predictions\ny_pred_last.plot(ax=ax,  color=\"#1f77b4\", linestyle=\"--\", label=\"Naive\")\ny_pred_theta.plot(ax=ax, color=\"#ff7f0e\", linestyle=\"--\", label=\"Theta\")\n\n# Split marker\nax.axvline(y_test.index[0], color=\"0.5\", linestyle=\":\", linewidth=1)\n\nax.set_title(\"Actual vs Forecasts (Naive, Theta)\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"y\")\nax.legend(loc=\"best\", ncols=3)\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#exponential-smoothing-ets-model",
    "href": "lecture_slides/08_time_series/08_time_series.html#exponential-smoothing-ets-model",
    "title": "Overview",
    "section": "Exponential Smoothing (ETS) model",
    "text": "Exponential Smoothing (ETS) model\nWhat this code does Fits an Exponential Smoothing (ETS) model with additive trend and additive seasonality to the training data, generates multi-step forecasts for the test horizon, evaluates accuracy with MAPE, and programmatically selects the better model between Theta and ETS based on the lower MAPE.\n\n\nWhy ETS here\nThe data have a smooth sinusoidal pattern (seasonality) plus a slow upward drift (trend). ETS explicitly models level, trend, and seasonality with exponential smoothing updates. The configuration\nExponentialSmoothing(trend=\"add\", seasonal=\"add\", sp=75, damped_trend=True)\nmeans:\n\ntrend=“add”: an additive trend component (\\(+\\) slope).\nseasonal=“add”: additive seasonal pattern (appropriate when seasonal amplitude does not scale with the level).\nsp=75: seasonal period of 75 time steps (daily data with a ~75-day cycle).\ndamped_trend=True: trend impact decays over the forecast horizon via a damping factor \\(\\phi \\in (0,1)\\), preventing unrealistic runaway growth.\n\n\n\n\nThe ETS(A,A,A) with damping: state-space equations\nAn additive error/additive trend/additive seasonality model with damping (often denoted ETS(A,Ad,A)) has latent states:\n\nLevel: \\(\\ell_t\\)\nTrend (slope): \\(b_t\\)\nSeasonality: \\(s_t\\) with period \\(s=\\text{sp}\\)\n\nObservation (data) equation \\[\ny_t = \\ell_{t-1} + \\phi b_{t-1} + s_{t-s} + \\varepsilon_t,\n\\] where \\(\\varepsilon_t\\) is the one-step-ahead error.\nState update equations \\[\n\\ell_t = \\ell_{t-1} + \\phi b_{t-1} + \\alpha \\varepsilon_t,\n\\] \\[\nb_t = \\phi b_{t-1} + \\beta \\varepsilon_t,\n\\] \\[\ns_t = s_{t-s} + \\gamma \\varepsilon_t,\n\\] with smoothing parameters \\(\\alpha,\\beta,\\gamma \\in (0,1)\\) estimated from the data, and damping \\(\\phi \\in (0,1)\\).\n\\(h\\)-step-ahead forecast (produced by predict(fh)) \\[\n\\hat y_{t+h|t} = \\ell_t + \\left(\\sum_{j=1}^{h} \\phi^j\\right) b_t + s_{t+h-s\\left\\lfloor\\frac{h-1}{s}\\right\\rfloor}.\n\\] When \\(\\phi&lt;1\\), the contribution of the trend term \\(\\sum_{j=1}^{h}\\phi^j\\) tapers off as \\(h\\) increases (damped trend).\nAdditive vs. multiplicative\n\nAdditive seasonality adds a fixed seasonal effect: \\(+,s_{t-s}\\).\nMultiplicative seasonality scales with level: \\(\\times,s_{t-s}\\) (requires strictly positive values; not used here).\n\n\n\n\nFit, forecast, and horizon\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\n\nfit(y_train) estimates \\(\\alpha,\\beta,\\gamma,\\phi\\) and initializes \\(\\ell_t, b_t, s_t\\) using only the training portion (no leakage).\nfh is the forecasting horizon (e.g., steps 1…30 ahead from the training endpoint). predict(fh) produces the corresponding vector \\({\\hat y_{T+1|T},\\dots,\\hat y_{T+H|T}}\\).\n\n\n\n\nAccuracy metric: MAPE\nmape_ets = mean_absolute_percentage_error(y_test, y_pred_ets)\nMean Absolute Percentage Error: \\[\n\\text{MAPE} = \\frac{100%}{H}\\sum_{h=1}^{H}\\left|\\frac{y_{T+h}-\\hat y_{T+h}}{y_{T+h}}\\right|.\n\\]\n\nInterprets error in percent terms (lower is better).\nBe careful when \\(y_{T+h}\\) is zero or very small (instability).\n\nThe printout compares:\n{\"MAPE_last\": ..., \"MAPE_theta\": ..., \"MAPE_ets\": ...}\n\nMAPE_last: naïve baseline (carry the last value forward).\nMAPE_theta: Theta model (trend-focused, no seasonality).\nMAPE_ets: ETS with additive trend/seasonality and damping.\n\n\n\n\nModel selection by metric\nbest_name, best_mape, best_pred = min(\n    [(\"theta_nodeseason\", mape_theta, y_pred_theta), (\"ets_add\", mape_ets, y_pred_ets)],\n    key=lambda x: x[1]\n)\n\nBuilds a small leaderboard of candidates with their MAPE.\nmin(..., key=lambda x: x[1]) picks the tuple with the smallest second element (the MAPE).\nReturns the name, score, and predictions of the winner for downstream use.\n\n\n\n\nPractical intuition\n\nETS learns three moving pieces: today’s baseline level, the slope of change, and a repeating seasonal pattern.\nAdditive seasonality says “add a seasonal bump” rather than “scale by a seasonal factor,” which is safer when values can be near zero or negative.\nDamped trend avoids forecasts that grow too fast by shrinking the trend effect step by step with \\(\\phi^h\\).\nThe selected model must beat the naïve MAPE to justify its complexity on this dataset.\n\n\n# ----- Model B: ETS with additive trend/seasonality (handles sine + trend) -----\nets = ExponentialSmoothing(trend=\"add\", seasonal=\"add\", sp=75, damped_trend=True)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\nmape_ets = mean_absolute_percentage_error(y_test, y_pred_ets)\n\nprint({\n    \"MAPE_last\": float(mape_last),\n    \"MAPE_theta\": float(mape_theta),\n    \"MAPE_ets\": float(mape_ets),\n})\n\n# Choose the better of the two candidate models\nbest_name, best_mape, best_pred = min(\n    [(\"theta_nodeseason\", mape_theta, y_pred_theta), (\"ets_add\", mape_ets, y_pred_ets)],\n    key=lambda x: x[1]\n)\nprint({\"best_model\": best_name, \"best_mape\": float(best_mape)})\n\n{'MAPE_last': 0.2887089356544459, 'MAPE_theta': 0.2658110643420696, 'MAPE_ets': 0.03130788494131025}\n{'best_model': 'ets_add', 'best_mape': 0.03130788494131025}\n\n\n\n# Plot actuals and forecasts (naive, theta, ETS) with distinct colors/styles\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Ensure all forecasts are Series aligned to the test index\ny_pred_last  = pd.Series(y_pred_last,  index=y_test.index, name=\"Naive\")\ny_pred_theta = pd.Series(y_pred_theta, index=y_test.index, name=\"Theta\")\ny_pred_ets   = pd.Series(y_pred_ets,   index=y_test.index, name=\"ETS\")\n\nfig, ax = plt.subplots(figsize=(12, 4))\n\n# Actuals\ny_train.plot(ax=ax, color=\"0.7\", label=\"Train\")\ny_test.plot(ax=ax, color=\"black\", linewidth=2, label=\"Actual\")\n\n# Predictions\ny_pred_last.plot(ax=ax,  color=\"#1f77b4\", linestyle=\"--\", label=\"Naive\")\ny_pred_theta.plot(ax=ax, color=\"#ff7f0e\", linestyle=\"--\", label=\"Theta\")\ny_pred_ets.plot(ax=ax,   color=\"#2ca02c\", linestyle=\"--\", label=\"ETS\")\n\n# Split marker\nax.axvline(y_test.index[0], color=\"0.5\", linestyle=\":\", linewidth=1)\n\nax.set_title(\"Actual vs Forecasts (Naive, Theta, ETS)\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"y\")\nax.legend(loc=\"best\", ncols=3)\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lecture_slides/08_time_series/08_time_series.html#time-series-cross-validation-2",
    "href": "lecture_slides/08_time_series/08_time_series.html#time-series-cross-validation-2",
    "title": "Overview",
    "section": "Time-Series Cross Validation",
    "text": "Time-Series Cross Validation\nEvaluate and compare three forecasters—Naive, Theta, and ETS—using time-series cross-validation (rolling/forward-chaining). The code also records, for every fold, the calendar dates used in training and testing, computes fold-level errors, and summarizes performance.\n\n\nTime-series cross-validation (rolling origin)\ncv = SlidingWindowSplitter(window_length=180, fh=np.arange(1, 31), step_length=7)\n\nTraining window: last 180 observations before each cutoff.\nForecasting horizon: fh = 1..30 means produce 30-step-ahead forecasts from each cutoff.\nStep length: move the cutoff forward 7 observations at a time (weekly).\nThis is leakage-free because each fold trains on past data and tests on strictly future data.\n\nAbsolute vs. relative horizon. Later, each fold converts fh to absolute timestamps so forecasts align to the calendar:\nfh_abs = ForecastingHorizon(y.index[test_idx], is_relative=False)\n\n\n\nCalendar of folds (what dates were used)\nfold_meta = []\nfor fold_id, (tr_idx, te_idx) in enumerate(cv.split(y), start=1):\n    fold_meta.append({\n        \"fold\": fold_id,\n        \"train_start\": y.index[tr_idx[0]],\n        \"train_end\":   y.index[tr_idx[-1]],\n        \"test_start\":  y.index[te_idx[0]],\n        \"test_end\":    y.index[te_idx[-1]],\n        \"n_train\":     len(tr_idx),\n        \"n_test\":      len(te_idx),\n    })\ncv_folds_calendar = pd.DataFrame(fold_meta)\n\nFor each fold, the code stores the train and test start/end dates and counts.\nThis table documents the rolling windows used in CV—critical for auditability.\n\n\n\n\nCandidate forecasters\nmodels = {\n    \"naive_last\": NaiveForecaster(strategy=\"last\"),\n    \"theta_nodeseason\": ThetaForecaster(sp=1, deseasonalize=False),\n    \"ets_add\": ExponentialSmoothing(\n        trend=\"add\", seasonal=\"add\", sp=SP, damped_trend=True,\n        initialization_method=\"estimated\"\n    ),\n}\n\nNaive (last): \\(\\hat y_{t+h|t}=y_t\\) for all \\(h\\).\nTheta (no deseasonalization): trend-focused extrapolation (related to SES with drift), robust when values may be near zero since no multiplicative seasonality is applied.\nETS(A,Ad,A): additive error, additive trend (damped), additive seasonality with period \\(s=SP\\). State-space form:\nObservation:\n\\[\ny_t=\\ell_{t-1}+\\phi b_{t-1}+s_{t-s}+\\varepsilon_t\n\\]\nUpdates:\n\\[\n\\ell_t=\\ell_{t-1}+\\phi b_{t-1}+\\alpha\\varepsilon_t,\\quad\nb_t=\\phi b_{t-1}+\\beta\\varepsilon_t,\\quad\ns_t=s_{t-s}+\\gamma\\varepsilon_t,\n\\]\nwith \\(\\alpha,\\beta,\\gamma\\in(0,1)\\) and damping \\(\\phi\\in(0,1)\\).\n\\(h\\)-step forecast:\n\\[\n\\hat y_{t+h|t}=\\ell_t+\\Big(\\sum_{j=1}^{h}\\phi^j\\Big)b_t+s_{t+h-s\\lfloor (h-1)/s\\rfloor}.\n\\]\ninitialization_method=\"estimated\" lets the model estimate initial level/trend/seasonals rather than relying on heuristic rules requiring two full seasonal cycles.\n\n\n\n\nCross-validated backtest routine\ndef cv_backtest(y, forecaster, cv):\n    preds_list, ape_records, fold_scores = [], [], []\n    for fold_id, (train_idx, test_idx) in enumerate(cv.split(y), start=1):\n        y_train, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n        fh_abs = ForecastingHorizon(y.index[test_idx], is_relative=False)\n\n        f = clone(forecaster)    # fresh copy per fold\n        f.fit(y_train)           # learn only from the current training window\n        y_pred_fold = f.predict(fh=fh_abs)\n\n        preds_list.append(y_pred_fold)\n\n        # Fold-level error: MAPE\n        mape_fold = mean_absolute_percentage_error(y_test_fold, y_pred_fold)\n        fold_scores.append({\"fold\": fold_id, \"MAPE\": float(mape_fold)})\n\n        # Pointwise errors for diagnostics: APE = |y - ŷ| / |y|\n        ape = (y_test_fold - y_pred_fold).abs() / y_test_fold.abs()\n        for t, val in ape.items():\n            ape_records.append({\"time\": t, \"APE\": float(val), \"fold\": fold_id})\n\n    # “Stitched” predictions across folds (later folds overwrite overlaps)\n    preds = pd.concat(preds_list).sort_index()\n    preds = preds[~preds.index.duplicated(keep=\"last\")]\n    return preds, pd.DataFrame(fold_scores), pd.DataFrame(ape_records)\n\nMAPE per fold: \\[\\text{MAPE}*\\text{fold}=\\frac{100%}{H}\\sum*{h=1}^{H}\\left|\\frac{y_{T+h}-\\hat y_{T+h}}{y_{T+h}}\\right|\\] where \\(T\\) is the fold cutoff and \\(H=30\\).\nAPE (pointwise) feeds diagnostic plots: \\[\\text{APE}_{t}=\\frac{|y_t-\\hat y_t|}{|y_t|}.\\]\n\nBacktest (time-series forecasting)\nA backtest is a walk-forward evaluation routine that simulates how a forecaster would have performed in real time by repeatedly training on past data and forecasting future points, then comparing forecasts with the actuals that occurred later. It is leakage-free and time-ordered (no random shuffling).\n\nWhy use it\n\nEstimates generalization error under realistic deployment conditions.\nDetects concept drift and horizon-specific degradation.\nProvides a robust basis for model selection and hyperparameter tuning.\n\n\n\nCore concepts\n\nCutoff \\(T_j\\): the last timestamp available to the model at backtest step \\(j\\).\nTraining window \\(\\mathcal{D}_{\\le T_j}\\): either growing (expanding) or sliding (fixed length).\nForecast horizon \\(fh={h_1,\\dots,h_H}\\): steps ahead predicted from \\(T_j\\) (e.g., \\(1{:}30\\)).\nStep length \\(\\Delta\\): how far the cutoff moves forward each iteration (e.g., 7 days).\nRefit policy: refit the model at every cutoff (typical) vs. reuse fitted parameters.\n\n\n\nDesign choices\n\nWindowing\n\nGrowing window: uses all history up to \\(T_j\\); stable for slow-changing series.\nSliding window: fixed length (e.g., last 180 points); better for drift or seasonal recency.\n\nHorizon definition\n\nRelative (\\(1{:}H\\) steps from \\(T_j\\)) or absolute (calendar dates). Consistency is critical.\n\nStep length \\(\\Delta\\)\n\nSmaller \\(\\Delta\\) = more folds, smoother estimates, higher compute.\n\nExogenous features\n\nMust be available at prediction time for the horizon; otherwise restrict features or forecast them first.\n\nTransformations\n\nScaling, differencing, and feature engineering must be fit inside each fold to avoid leakage.\n\n\n\n\nOne-line definition\nA backtest routine is a time-ordered, walk-forward evaluation that repeatedly fits on past, predicts the future horizon, and scores against realized outcomes, aggregating errors across multiple rolling cutoffs to estimate true out-of-sample forecasting performance.\n\n\n\n\nRunning CV for all models and joining fold dates\nfor name, model in models.items():\n    preds, fold_scores_df, ape_df = cv_backtest(y, model, cv)\n    preds.name = name\n    cv_preds[name] = preds\n    fold_scores_df[\"model\"] = name\n    per_fold_scores.append(fold_scores_df)\n    if not ape_df.empty:\n        ape_df[\"model\"] = name\n        ape_all_models.append(ape_df)\n\nper_fold_scores = pd.concat(per_fold_scores, ignore_index=True)\nper_fold_scores = per_fold_scores.merge(cv_folds_calendar, on=\"fold\", how=\"left\")\n\nProduces:\n\ncv_preds: stitched forecast series per model.\nper_fold_scores: table with columns fold, MAPE, model, plus the calendar columns merged in (train_start, train_end, test_start, test_end, n_train, n_test).\nape_all_models: long table of per-point APEs with model and fold labels.\n\n\n\n\n\nModel-level summary\nresults_table = (\n    per_fold_scores.groupby(\"model\")[\"MAPE\"]\n    .agg(MAPE_mean=\"mean\", MAPE_std=\"std\", MAPE_median=\"median\", MAPE_min=\"min\", MAPE_max=\"max\", folds=\"count\")\n    .reset_index()\n    .sort_values(\"MAPE_mean\")\n)\n\nAggregates fold-level MAPE to compare models. Lower mean MAPE indicates better average performance across rolling test windows; the standard deviation reflects stability.\n\n\n\n\nDeliverables printed\nprint(results_table)        # model comparison (mean±std, etc.)\nprint(cv_folds_calendar)    # calendar of folds (dates and sizes)\nprint(per_fold_scores.head())\n\nresults_table: concise leaderboard.\ncv_folds_calendar: audit log of the rolling CV windows (start/end dates).\nper_fold_scores: per-fold MAPE joined with the corresponding calendar dates.\n\n\n\n\nWhy this setup is correct for forecasting\n\nNo leakage: each fold trains on past, validates on future.\nMultiple cutoffs: tests robustness by varying the information set.\nCalendar traceability: retained dates allow reproduction and debugging.\nAppropriate baselines and models: Naive sets the floor; Theta handles level/trend; ETS adds explicit seasonality with damping.\n\nInterpretation guideline\nPick the model with the lowest mean MAPE and acceptable MAPE_std. Then visualize cv_preds against y to confirm qualitative fit and check where errors spike using per-point APEs.\n\n# Cross-validated comparison (Naive, Theta, ETS) + calendar of folds (train/test start & end dates)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.base import clone\n\nfrom sktime.forecasting.model_selection import SlidingWindowSplitter\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.forecasting.theta import ThetaForecaster\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n\n# -----------------------------\n# Synthetic data (ensure enough history for seasonal ETS)\nn = 400\nidx = pd.date_range(\"2015-01-01\", periods=n, freq=\"D\")\ny = pd.Series(\n    np.sin(np.arange(n) / 12.0) + 0.1 * np.random.randn(n) + 0.01 * np.arange(n),\n    index=idx,\n    name=\"y\",\n)\n\nSP = 75  # seasonal period\n\n# -----------------------------\n# Cross-validation scheme (&gt;= 2*SP inside each training window)\ncv = SlidingWindowSplitter(\n    window_length=180,            # &gt;= 150\n    fh=np.arange(1, 31),          # 30-step horizon\n    step_length=7                 # weekly rolling\n)\n\n# -----------------------------\n# Build a dataset with the calendar (start/end) for each CV fold\nfold_meta = []\nfor fold_id, (tr_idx, te_idx) in enumerate(cv.split(y), start=1):\n    fold_meta.append({\n        \"fold\": fold_id,\n        \"train_start\": y.index[tr_idx[0]],\n        \"train_end\":   y.index[tr_idx[-1]],\n        \"test_start\":  y.index[te_idx[0]],\n        \"test_end\":    y.index[te_idx[-1]],\n        \"n_train\":     len(tr_idx),\n        \"n_test\":      len(te_idx),\n    })\ncv_folds_calendar = pd.DataFrame(fold_meta)\n\n# print(cv_folds_calendar.head())           # &lt;-- calendar of folds\n\n# -----------------------------\n# Candidate forecasters\nmodels = {\n    \"naive_last\": NaiveForecaster(strategy=\"last\"),\n    \"theta_nodeseason\": ThetaForecaster(sp=1, deseasonalize=False),\n    \"ets_add\": ExponentialSmoothing(\n        trend=\"add\",\n        seasonal=\"add\",\n        sp=SP,\n        damped_trend=True,\n        initialization_method=\"estimated\",\n    ),\n}\n\n# -----------------------------\ndef cv_backtest(y, forecaster, cv):\n    \"\"\"Run sliding-window CV for one forecaster and collect predictions, metrics, and per-fold scores.\"\"\"\n    preds_list = []\n    ape_records = []\n    fold_scores = []\n\n    for fold_id, (train_idx, test_idx) in enumerate(cv.split(y), start=1):\n        y_train = y.iloc[train_idx]\n        y_test_fold = y.iloc[test_idx]\n        fh_abs = ForecastingHorizon(y.index[test_idx], is_relative=False)\n\n        f = clone(forecaster)\n        f.fit(y_train)\n        y_pred_fold = f.predict(fh=fh_abs)\n\n        preds_list.append(y_pred_fold)\n\n        mape_fold = mean_absolute_percentage_error(y_test_fold, y_pred_fold)\n        fold_scores.append({\"fold\": fold_id, \"MAPE\": float(mape_fold)})\n\n        ape = (y_test_fold - y_pred_fold).abs() / y_test_fold.abs()\n        for t, val in ape.items():\n            ape_records.append({\"time\": t, \"APE\": float(val), \"fold\": fold_id})\n\n    preds = pd.concat(preds_list).sort_index()\n    preds = preds[~preds.index.duplicated(keep=\"last\")]\n    return preds, pd.DataFrame(fold_scores), pd.DataFrame(ape_records)\n\n# -----------------------------\n# Run CV for all models\ncv_preds = {}                 # Series of stitched CV forecasts per model\nper_fold_scores = []          # list of DataFrames with columns: fold, MAPE, model\nape_all_models = []           # per-point APE for boxplots\n\nfor name, model in models.items():\n    preds, fold_scores_df, ape_df = cv_backtest(y, model, cv)\n    preds.name = name\n    cv_preds[name] = preds\n    fold_scores_df[\"model\"] = name\n    per_fold_scores.append(fold_scores_df)\n    if not ape_df.empty:\n        ape_df[\"model\"] = name\n        ape_all_models.append(ape_df)\n\nper_fold_scores = pd.concat(per_fold_scores, ignore_index=True)\n# Attach calendar dates to each fold\nper_fold_scores = per_fold_scores.merge(cv_folds_calendar, on=\"fold\", how=\"left\")\n\n# -----------------------------\n# Summary comparison table (mean ± std MAPE)\nresults_table = (\n    per_fold_scores\n    .groupby(\"model\")[\"MAPE\"]\n    .agg(MAPE_mean=\"mean\", MAPE_std=\"std\", MAPE_median=\"median\", MAPE_min=\"min\", MAPE_max=\"max\", folds=\"count\")\n    .reset_index()\n    .sort_values(\"MAPE_mean\")\n)\nprint(results_table)\n\n\n# -----------------------------\n# Artifacts produced:\n# - cv_folds_calendar : dataset with start/end dates per fold\n# - per_fold_scores   : per-fold MAPE joined with fold calendar\n# - results_table     : model-level summary (mean/std/median/min/max MAPE)\n\nprint(\"\\nCalendar of CV folds:\")\nprint(cv_folds_calendar)\n\nprint(\"\\nPer-fold scores (with dates):\")\nprint(per_fold_scores.head())\n\n              model  MAPE_mean  MAPE_std  MAPE_median  MAPE_min  MAPE_max  \\\n0           ets_add      0.051     0.022        0.042     0.025     0.105   \n1        naive_last      0.285     0.149        0.266     0.070     0.757   \n2  theta_nodeseason      0.297     0.169        0.263     0.077     0.836   \n\n   folds  \n0     28  \n1     28  \n2     28  \n\nCalendar of CV folds:\n    fold train_start  train_end test_start   test_end  n_train  n_test\n0      1  2015-01-01 2015-06-29 2015-06-30 2015-07-29      180      30\n1      2  2015-01-08 2015-07-06 2015-07-07 2015-08-05      180      30\n2      3  2015-01-15 2015-07-13 2015-07-14 2015-08-12      180      30\n3      4  2015-01-22 2015-07-20 2015-07-21 2015-08-19      180      30\n4      5  2015-01-29 2015-07-27 2015-07-28 2015-08-26      180      30\n5      6  2015-02-05 2015-08-03 2015-08-04 2015-09-02      180      30\n6      7  2015-02-12 2015-08-10 2015-08-11 2015-09-09      180      30\n7      8  2015-02-19 2015-08-17 2015-08-18 2015-09-16      180      30\n8      9  2015-02-26 2015-08-24 2015-08-25 2015-09-23      180      30\n9     10  2015-03-05 2015-08-31 2015-09-01 2015-09-30      180      30\n10    11  2015-03-12 2015-09-07 2015-09-08 2015-10-07      180      30\n11    12  2015-03-19 2015-09-14 2015-09-15 2015-10-14      180      30\n12    13  2015-03-26 2015-09-21 2015-09-22 2015-10-21      180      30\n13    14  2015-04-02 2015-09-28 2015-09-29 2015-10-28      180      30\n14    15  2015-04-09 2015-10-05 2015-10-06 2015-11-04      180      30\n15    16  2015-04-16 2015-10-12 2015-10-13 2015-11-11      180      30\n16    17  2015-04-23 2015-10-19 2015-10-20 2015-11-18      180      30\n17    18  2015-04-30 2015-10-26 2015-10-27 2015-11-25      180      30\n18    19  2015-05-07 2015-11-02 2015-11-03 2015-12-02      180      30\n19    20  2015-05-14 2015-11-09 2015-11-10 2015-12-09      180      30\n20    21  2015-05-21 2015-11-16 2015-11-17 2015-12-16      180      30\n21    22  2015-05-28 2015-11-23 2015-11-24 2015-12-23      180      30\n22    23  2015-06-04 2015-11-30 2015-12-01 2015-12-30      180      30\n23    24  2015-06-11 2015-12-07 2015-12-08 2016-01-06      180      30\n24    25  2015-06-18 2015-12-14 2015-12-15 2016-01-13      180      30\n25    26  2015-06-25 2015-12-21 2015-12-22 2016-01-20      180      30\n26    27  2015-07-02 2015-12-28 2015-12-29 2016-01-27      180      30\n27    28  2015-07-09 2016-01-04 2016-01-05 2016-02-03      180      30\n\nPer-fold scores (with dates):\n   fold   MAPE       model train_start  train_end test_start   test_end  \\\n0     1  0.757  naive_last  2015-01-01 2015-06-29 2015-06-30 2015-07-29   \n1     2  0.538  naive_last  2015-01-08 2015-07-06 2015-07-07 2015-08-05   \n2     3  0.215  naive_last  2015-01-15 2015-07-13 2015-07-14 2015-08-12   \n3     4  0.271  naive_last  2015-01-22 2015-07-20 2015-07-21 2015-08-19   \n4     5  0.375  naive_last  2015-01-29 2015-07-27 2015-07-28 2015-08-26   \n\n   n_train  n_test  \n0      180      30  \n1      180      30  \n2      180      30  \n3      180      30  \n4      180      30  \n\n\n\n# -----------------------------\n# Plots\n\n# 1) Actual vs stitched CV forecasts\nfig, ax = plt.subplots(figsize=(12, 4))\ny.plot(ax=ax, label=\"Actual\", linewidth=2)\nfor name, preds in cv_preds.items():\n    preds.plot(ax=ax, linestyle=\"--\", label=name)\nax.set_title(\"Actual vs Cross-Validated Forecasts (stitched across folds)\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"y\")\nax.legend()\nax.grid(True, alpha=0.25)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# 2) Mean MAPE by model (with std error bars)\nfig, ax = plt.subplots(figsize=(8, 4))\nx = np.arange(len(results_table))\nax.bar(x, results_table[\"MAPE_mean\"], yerr=results_table[\"MAPE_std\"], capsize=4)\nax.set_xticks(x, results_table[\"model\"])\nax.set_title(\"Cross-Validated Mean MAPE by Model\")\nax.set_ylabel(\"MAPE\")\nax.grid(True, axis=\"y\", alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# -----------------------------\n# Best model by mean MAPE\nbest_row = results_table.iloc[0]\nprint({\n    \"best_model\": best_row[\"model\"],\n    \"best_mape_mean\": best_row[\"MAPE_mean\"],\n    \"best_mape_std\": best_row[\"MAPE_std\"],\n})\n\n{'best_model': 'ets_add', 'best_mape_mean': np.float64(0.05079799448537462), 'best_mape_std': np.float64(0.021624320334035486)}\n\n\n#\n\n\n#\n\nHave fun!\n\n\n# !jupyter nbconvert _08-py-wrangling-basics-practice-solutions.ipynb --to html --template classic --output 08-py-wrangling-basics-practice-solutions.html"
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#introduction",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#introduction",
    "title": "Introduction to time series analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this lecture we will cover the following topics:\n\nDefinition of time series data.\nIntroduction to time series analysis and application examples.\nThe main components of a time series.\nTime series decomposition.\n\n\n#Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose, STL\nfrom scipy.fft import fft\nnp.random.seed(0)  # for reproducibility"
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#basics",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#basics",
    "title": "Introduction to time series analysis",
    "section": "Basics",
    "text": "Basics\n\nWhat is a time series?\n\nA time series is a sequence of data points organized in time order.\nUsually, the time signal is sampled at equally spaced points in time.\nThese can be represented as the sequence of the sampled values.\nWe refer to this setting as discrete time.\n\n\n\nIrregularly sampled time signals can still be represented as a time series.\nIt is necessary to encode this additional information into an additional data structure.\nWe refer to this setting as continuous time.\n\n\n\nThis setting is less common. We focus on discrete time.\n\n\n\nWhat data are represented as time series?\n\nTime series are found in a myriad of natural phenomena, industrial and engineering applications, business, human activities, and so on.\n\n\n\n\n\n\n\nOther examples include data from:\n\nFinance: stock prices, asset prices, macroeconomic factors.\nE-Commerce: page views, new users, searches.\nBusiness: transactions, revenue, inventory levels.\nNatural language: machine translation, chatbots.\n\n\n\n\nTime series analysis\nThe main pruposes of time series analysis are: 1. To understand and characterize the underlying process that generates the observed data. 2. To forecast the evolution of the process, i.e., predict the next observed values.\n\nThere are two main different perspectives to look at a time series.\nEach perspective leads to different time series analysis approaches\n\n\nStatistics perspective\n\nA time series is a sequence of random variables that have some correlation or other distributional relationship between them.\n\n\n\nThe sequence is a realization (observed values) of a stochastic process.\nStatistical time series approaches focus on finding the parameters of the stochastic process that most likely produced the observed time series.\n\n\n\nDynamical system perspective\n\nThis perspective assumes that there is a system governed by unknown variables \\(\\{x_1, x_2, x_3, \\dots ,x_𝑁\\}\\).\nGenerally, we only observe one time series \\(y\\) generated by the system.\nWhat can \\(y\\) be?\n\nOne of the system variables.\nA function \\(f\\) of system variables.\n\nThe objective of the analysis is to reconstruct the dynamics of the entire system from \\(y\\).\n\n\n\n\n\nApplications\nTime series analysis is applied in many real world applications, including - Economic forecasting - Stock market analysis - Demand planning and forecasting - Anomaly detection - … And much more\nEconomic Forecasting\n\nTime series analysis is used in macroeconomic predictions.\nWorld Trade Organization does time series forecasting to predict levels of international trade [source].\nFederal Reserve uses time series forecasts of the economy to set interest rates [source].\n\n\nDemand forecasting\n\nTime series analysis is used to predict demand at different levels of granularity.\nAmazon and other e commerce companies use time series modeling to predict demand at a product geography level [source].\nHelps meet customer needs (fast shipping) and reduce inventory waste\n\n\nAnomaly detection\n\nUsed to detect anomalous behaviors in the underlying system by looking at unusual patterns in the time series.\nWidely used in manufacturing to detect defects and target preventive maintenance [source].\nWith new IoT devices, anomaly detection is being used in machinery heavy industries, such as petroleum and gas [source]."
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#time-series-components",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#time-series-components",
    "title": "Introduction to time series analysis",
    "section": "Time series components",
    "text": "Time series components\n\nA time series is often assumed to be composed of three components:\n\nTrend: the long-term direction.\nSeasonality: the periodic behavior.\nResiduals: the irregular fluctuations.\n\n\n\nTrend\n\nTrend captures the general direction of the time series.\nFor example, increasing number of passengers over the years despite seasonal fluctuations.\nTrend can be increasing, decreasing, or constant.\nIt can increase/decrease in different ways over time (linearly, exponentially, etc…).\n\n\n\nLet’s create a trend from scratch to understand how it looks like.\n\n\ntime = np.arange(144)\ntrend = time * 2.65 +100\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax.plot(time, trend, color='tab:red')\nax.set_xlabel(\"Months\")\nax.set_ylabel(\"Passengers\")\nplt.grid()\nplt.title(\"Trend vs Time\");\n\n\n\n\n\n\n\n\n\n\nSeasonality\n\nPeriodic fluctuations in time series data that occur at regular intervals due to seasonal factors.\nIt is characterized by consistent and predictable patterns over a specific period (e.g., daily, monthly, quarterly, yearly).\n\nIt can be driven by many factors. - Naturally occurring events such as weather fluctuations caused by time of year. - Business or administrative procedures, such as start and end of a school year. - Social or cultural behavior, e.g., holidays or religious observances.\n\nLet’s generate the seasonal component.\n\n\nseasonal = 20 + np.sin( time * 0.5) * 20\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax.plot(time, seasonal, color='tab:orange')\nax.set_xlabel(\"Months\")\nax.set_ylabel(\"Passengers\")\nplt.grid()\nplt.title(\"Seasonality vs Time\");\n\n\n\n\n\n\n\n\n\n\nResiduals\n\nResiduals are the random fluctuations left over after trend and seasonality are removed from the original time series.\nOne should not see a trend or seasonal pattern in the residuals.\nThey represent short term, rather unpredictable fluctuations.\n\n\nresiduals = np.random.normal(loc=0.0, scale=3, size=len(time))\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax.plot(time, residuals, color='tab:green')\nax.set_xlabel(\"Months\")\nax.set_ylabel(\"Passengers\")\nplt.grid()\nplt.title(\"Residuals vs Time\");"
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#decomposition-models",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#decomposition-models",
    "title": "Introduction to time series analysis",
    "section": "Decomposition Models",
    "text": "Decomposition Models\n\nTime series components can be decomposed with the following models:\n\nAdditive decomposition\nMultiplicative decomposition\nPseudoadditive decomposition\n\n\n\nAdditive model\n\nAdditive models assume that the observed time series is the sum of its components:\n\n\\[X(t) = T(t) + S(t) + R(t)\\]\n\nwhere\n\n\\(X(t)\\) is the time series\n\n\\(T(t)\\) is the trend\n\n\\(S(t)\\) is the seasonality\n\n\\(R(t)\\) is the residual\n\n\nAdditive models are used when the magnitudes of the seasonal and residual values do not depend on the level of the trend.\n\n\nadditive = trend + seasonal + residuals\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax.plot(time, additive, 'tab:blue')\nax.set_xlabel(\"Months\")\nax.set_ylabel(\"Passengers\")\nplt.title(\"Additive Time Series\")\nplt.grid();\n\n\n\n\n\n\n\n\n\n\nMultiplicative Model\n\nAssumes that the observed time series is the product of its components:\n\n\\[X(t) = T(t) \\cdot S(t) \\cdot R(t)\\]\n\nIt is possible to transform a multiplicative model to an additive one by applying a log transformation:\n\n\\[\\log \\left( T(t) \\cdot S(t) \\cdot R(t) \\right) = \\log(T(t)) + \\log(S(t)) + \\log(R(t))\\]\n\nMultiplicative models are used when the magnitudes of seasonal and residual values depends on trend.\n\n\nmultiplicative = trend * seasonal # we do not include residuals to make the pattern more clear\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax.plot(time, multiplicative, 'tab:blue')\nax.set_xlabel(\"Months\")\nax.set_ylabel(\"Passengers\")\nplt.title(\"Multiplicative Time Series\")\nplt.grid();\n\n\n\n\n\n\n\n\n\n\nPseudoadditive Model\n\nPseudoadditive models combine elements of the additive and multiplicative models.\nUseful when:\n\nTime series values are close to or equal to zero. Multiplicative models struggle with zero values, but you still need to model multiplicative seasonality.\nSome features are multiplicative (e.g., seasonal effects) and other are additive (e.g., residuals).\nComplex seasonal patterns or data that do not completely align with additive or multiplicative model.\n\nFor example, this model is particularly relevant for modeling series that:\n\nare extremely weather-dependent,\nhave sharply pronounced seasonal fluctuations and trend-cycle movements.\n\nFormulation:\n\n\\[X(t) = T(t) + T(t)\\cdot(S(t) - 1) + T(t)\\cdot(R(t) - 1) = T(t)\\cdot(S(t) + R(t) -1)\\]\n\npseudoadditive = trend * (seasonal + residuals - 1)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax.plot(time, pseudoadditive, 'tab:blue')\nax.set_xlabel(\"Months\")\nax.set_ylabel(\"Passengers\")\nplt.title(\"Pseudoadditive Time Series\")\nplt.grid();"
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#time-series-decomposition",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#time-series-decomposition",
    "title": "Introduction to time series analysis",
    "section": "Time Series Decomposition",
    "text": "Time Series Decomposition\n\nNow let’s reverse directions.\nWe have additive and multiplicative data.\nLet’s decompose them into their three components.\nA very simple, yet often useful, approach is to estimate a linear trend.\nA detrended time series is obtained by subtracting the linear trend from the data.\nThe linear trend is computed as a 1st order polynomial.\n\n\nslope, intercept = np.polyfit(np.arange(len(additive)), additive, 1) # estimate line coefficient\ntrend = np.arange(len(additive)) * slope + intercept # linear trend\ndetrended = additive - trend # remove the trend\n\n\nplt.figure(figsize=(10, 3))\nplt.plot(additive, label='Original')\nplt.plot(trend, label='Trend')\nplt.plot(detrended, label='Detrended')\nplt.grid()\nplt.legend();\n\n\n\n\n\n\n\n\n\nNext, we will use seasonal_decompose (more information here) to isolate the main time series components.\nThis is a simple method that requires us to specify the type of model (additive or multiplicative) and the main period.\n\n\nAdditive Decomposition\n\nWe need to specify an integer that represents the main seasonality of the data.\nBy looking at the seasonal component, we see that the period is approximately \\(12\\) time steps long.\nSo, we set period=12.\n\n\nadditive_decomposition = seasonal_decompose(x=additive, model='additive', period=12)\n\n\n# Utility function to make the plots\ndef seas_decomp_plots(original, decomposition):\n    _, axes = plt.subplots(4, 1, sharex=True, sharey=False, figsize=(7, 5))\n    axes[0].plot(original, label='Original')\n    axes[0].legend(loc='upper left')\n    axes[1].plot(decomposition.trend, label='Trend')\n    axes[1].legend(loc='upper left')\n    axes[2].plot(decomposition.seasonal, label='Seasonality')\n    axes[2].legend(loc='upper left')\n    axes[3].plot(decomposition.resid, label='Residuals')\n    axes[3].legend(loc='upper left')\n    plt.show()\n\n\nseas_decomp_plots(additive, additive_decomposition)\n\n\n\n\n\n\n\n\n\nThe blue line in each plot representes the decomposition.\nThere is a legend in the upper left corner of each plot to let you know what each plot represents.\nYou can see the decomposition is not perfect with regards to seasonality and residuals, but it’s pretty close.\nYou may notice both trend and residuals are missing data towards the beginning and end.\nThis has to do with how trend is calculated (beyond the scope of this lesson).\nThe residuals are missing simply because \\(R_{t} = Y_{t} - T_{t} - S_{t}\\), so missing trend values mean missing residual values as well.\nIn other words, there is nothing wrong with these graphs.\n\n\n\nMultiplicative Decomposition\n\nWe use the same function as before, but on the multiplicative time series.\nSince we know this is a multiplicative time series, we declare model='multiplicative' in seasonal_decompose.\n\n\nmultiplicative_decomposition = seasonal_decompose(x=multiplicative, model='multiplicative', \n                                                  period=12)\nseas_decomp_plots(multiplicative, multiplicative_decomposition)\n\n\n\n\n\n\n\n\n\nAgain, the decomposition does a relatively good job picking up the overall trend and seasonality.\nWe can see the shapes follow the patterns we expect.\n\n\n\nLocally estimated scatterplot smoothing (LOESS)\n\nNext, we try a second method called STL (Seasonal and Trend decomposition using LOESS).\nWe start with the additive model.\n\n\nstl_decomposition = STL(endog=additive, period=12, robust=True).fit()\nseas_decomp_plots(additive, stl_decomposition)\n\n\n\n\n\n\n\n\n\nThe STL decomposition does a very good job on the additive time series.\nNext, we try with the multiplicative one.\n\n\nstl_decomposition = STL(endog=multiplicative, period=12, robust=True).fit()\nseas_decomp_plots(multiplicative, stl_decomposition)\n\n\n\n\n\n\n\n\n\nThis decomposition is not as good as the previous one.\n\n\n\nWhich method to use?\nUse seasonal_decompose when:\n\nYour time series data has a clear and stable seasonal pattern and trend.\nYou prefer a simpler model with fewer parameters to adjust.\nThe seasonal amplitude is constant over time (suggesting an additive model) or varies proportionally with the trend (suggesting a multiplicative model).\n\nUse STL when:\n\nYour time series exhibits complex seasonality that may change over time.\nYou need to handle outliers effectively without them distorting the trend and seasonal components.\nYou are dealing with non-linear trends and seasonality, and you need more control over the decomposition process."
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#identify-the-dominant-periodfrequency",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#identify-the-dominant-periodfrequency",
    "title": "Introduction to time series analysis",
    "section": "Identify the dominant period/frequency",
    "text": "Identify the dominant period/frequency\n\nseasonal_decompose expects the dominant period as a parameter.\nIn this example, we generated the seasonal component by hand as follows:\n\nseasonal = 20 + np.sin( time * 0.5) * 20\n\nWe said that the period was approximately \\(12\\).\nBut, in general, how do we find it out 🤔?\nYou can use one of the following techniques:\n\nPlot the data and try to figure out after how many steps the cycle repeats.\nDo an Autocorrelation Plot (more on this later).\nUse the Fast Fourier Transform on a signal without trend.\n\nWe will look more into FFT later on.\nFor now, you can use the following function to compute the dominant period in the data.\n\n\ndef fft_analysis(signal):\n    \n    # Linear detrending\n    slope, intercept = np.polyfit(np.arange(len(signal)), signal, 1)\n    trend = np.arange(len(signal)) * slope + intercept \n    detrended = signal - trend \n    \n    fft_values = fft(detrended)\n    frequencies = np.fft.fftfreq(len(fft_values))\n\n    # Remove negative frequencies and sort\n    positive_frequencies = frequencies[frequencies &gt; 0]\n    magnitudes = np.abs(fft_values)[frequencies &gt; 0]\n\n    # Identify dominant frequency\n    dominant_frequency = positive_frequencies[np.argmax(magnitudes)]\n    print(f\"Dominant Frequency: {dominant_frequency:.3f}\")\n\n    # Convert frequency to period (e.g., days, weeks, months, etc.)\n    dominant_period = 1 / dominant_frequency\n    print(f\"Dominant Period: {dominant_period:.2f} time units\")\n    \n    return dominant_period, positive_frequencies, magnitudes\n\n\nperiod, freqs, magnitudes = fft_analysis(seasonal)\n\nDominant Frequency: 0.076\nDominant Period: 13.09 time units\n\n\n\n# Plotting the spectrum\nplt.figure(figsize=(10, 3))\nplt.stem(freqs, magnitudes)\nplt.title('Frequency Spectrum')\nplt.xlabel('Frequency')\nplt.ylabel('Magnitude')\nplt.show()\n\n\n\n\n\n\n\n\n\nIt turns out that the main seasonality was not exactly \\(12\\).\nIf we want to generate a periodic signal with seasonality \\(12\\), we have to do as follows.\n\n\nseasonal_12 = 20 + np.sin(2*np.pi*time/12)*20\n\n\nfft_analysis(seasonal_12);\n\nDominant Frequency: 0.083\nDominant Period: 12.00 time units"
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#summary",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#summary",
    "title": "Introduction to time series analysis",
    "section": "Summary",
    "text": "Summary\nIn this lecture we covered the following topics. - The definition of a time series and examples of time series from the real world. - The definition of time series analysis and examples of its application in different fields. - A practical understanding of the three components of time series data. - The additive, multiplicative, and pseudo-additive models. - Standard approaches to decompose a time series in its constituent parts."
  },
  {
    "objectID": "lecture_slides/08_time_series/introduction_to_time_series.html#exercises",
    "href": "lecture_slides/08_time_series/introduction_to_time_series.html#exercises",
    "title": "Introduction to time series analysis",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\n\nConsider as the seasonal component the periodic signal with period 12\n\ntime = np.arange(144)\nseasonal_12 = 20 + np.sin(2*np.pi*time/12)*20\n\nUse seasonal_12 and the trend and residual components below to define and plot the additive and the multiplicative models\n\ntrend = time * 2.65 + 100\nresiduals = np.random.normal(loc=0.0, scale=3, size=len(time))\n\nPerform the seasonal decomposition with seasonal_decompose and STL on the new signals and compare the results with the ones obtained in class, where we used an approximate period.\n\n\n\nExercise 2\nLoad the two different time series as follows.\n\nts_A = sm.datasets.get_rdataset(\"AirPassengers\", \"datasets\").data[\"value\"].values\nprint(len(ts_A))\nts_B = sm.datasets.get_rdataset(\"CO2\", \"datasets\").data[\"value\"].values\nprint(len(ts_B))\n\n144\n468\n\n\n\nPlot the two time series.\nDetermine if the time series looks additive or multiplicative models.\nDetermine the main period of the seasonal component in the two time series.\n\n\n\nExercise 3\n\nDecompose ts_A and ts_B using seasonal_decompose and STL.\nComment on the results you obtain. from statsmodels.tsa.seasonal import seasonal_decompose, STL"
  }
]