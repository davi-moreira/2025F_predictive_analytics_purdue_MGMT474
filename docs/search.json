[
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, time series, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025F_predictive_analytics_purdue_MGMT474/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 1007\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required):\n\nGoogle Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\nMicrosoft Copilot: is an AI-powered assistant designed to enhance productivity and streamline workflows across various applications and services. It utilizes large language models and is integrated within Microsoft 365 apps like Word, Excel, PowerPoint, Outlook, and Teams, providing real-time, context-aware assistance for tasks such as drafting documents, analyzing data, managing projects, and communicating more efficiently. Users can leverage Copilot to automate repetitive tasks, generate ideas, summarize information, and access data across their work environment and the web, all within a secure and privacy-conscious framework.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Deep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#overview",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Deep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#deep-learning-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\n\nEarly Rise (1980s)\n\nNeural networks first gained popularity.\n\nHigh levels of excitement, with dedicated conferences (e.g., NeurIPS, Snowbird).\n\n1990s Shift\n\nEmergence of other methods (SVMs, Random Forests, Boosting).\n\nNeural networks receded into the background.\n\nResurgence (2010)\n\nRebranded and refined under the banner of Deep Learning.\n\nBy the 2020s, became extremely successful and widely adopted.\n\nKey Drivers of Success\n\nRapid increases in computing power (GPUs, parallel computing).\n\nAvailability of large-scale datasets.\n\nUser-friendly deep learning libraries (e.g., TensorFlow, PyTorch).\n\n\n\n\nMuch of the credit goes to three pioneers and their students:\n\nYann LeCun, Geoffrey Hinton, and Yoshua Bengio,\nwho received the 2019 ACM Turing Award for their work in Neural Networks."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#ai-visionaries-interviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#ai-visionaries-interviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AI Visionaries: Interviews",
    "text": "AI Visionaries: Interviews\n\n\n\n\n\n\n\n\n Yann LeCunThe Future of AIDec 16, 2023 \n\n\n\n\n\n\n Geoffrey Hinton60 Minutes InterviewOct 9, 2023 \n\n\n\n\n\n\n Yoshua BengioPath to Human-Level AIApr 24, 2024"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#what-are-deep-learning-frameworks",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#what-are-deep-learning-frameworks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What Are Deep Learning Frameworks?",
    "text": "What Are Deep Learning Frameworks?\n\nDeep learning frameworks reduce boilerplate code, handle tensor operations efficiently, and make it easier to prototype and iterate on new architectures.\nSoftware libraries designed to streamline the creation, training, and deployment of neural networks.\n\nProvide pre-built functions, automatic differentiation, and GPU/TPU support.\n\nNecessity: They allow researchers and developers to focus on model design rather than low-level implementation details."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#pytorch-and-tensor-flow",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#pytorch-and-tensor-flow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PyTorch and Tensor Flow",
    "text": "PyTorch and Tensor Flow\n\n\n\n\nWhat is PyTorch?\n\nDeveloped primarily by Facebook (Meta) and released on September 2016.\nEmphasizes a dynamic computation graph (eager execution).\nHighly “Pythonic”: feels natural for Python developers.\nStrong community presence in academia and research.\n\n\n\n\nWhy is PyTorch Necessary?\n\nEase of Use & Debugging\n\nEvaluate expressions immediately without building a separate graph.\n\nMore intuitive for experimenting with complex, innovative models.\n\nResearch Focus\n\nQuickly prototype new ideas and iterate.\n\nActive Ecosystem\n\nLibraries like torchvision, torchaudio, and others for specialized tasks.\n\n\n\n\n\n\nHow to begin\n\nhttps://pytorch.org/tutorials/beginner/basics/intro.html.\nThere is also a YouTube Series (PyTorch Beginner Series) also here (Introduction to PyTorch)\n\n\n\n\n\n\nWhat is TensorFlow?\n\nDeveloped primarily by Google and released in November 2015.\nHistorically used a static graph approach (with an “eager mode” added later).\nComes with extensive tools for deployment (mobile, web, and production).\nLarge ecosystem with well-integrated components (e.g., TensorBoard, TFX, TensorFlow Lite).\n\n\n\n\n\nWhy is TensorFlow Necessary?\n\nProduction-Ready\n\nStrong support for model serving at scale in enterprise environments.\n\nComprehensive Ecosystem\n\nVisualization (TensorBoard), data processing (TFX), and model deployment pipelines.\n\n\nCross-Platform & Hardware Support\n\nEasily deploy models to cloud infrastructures, mobile devices, and specialized hardware (TPUs).\n\n\n\n\n\n\nHow to begin\n\nhttps://www.tensorflow.org/tutorials. There is also a Quick Start!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#key-differences",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#key-differences",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Key Differences",
    "text": "Key Differences\n\n\n\n\n\n\n\n\nAspect\nPyTorch\nTensorFlow\n\n\n\n\nComputation Graph\nDynamic graph (eager execution by default).\nHistorically static graph with a build-and-execute phase (now supports eager execution).\n\n\nDebugging & Development Style\nMore straightforward for Python developers, immediate error feedback.\nCan be trickier to debug in graph mode; eager mode helps but is relatively newer.\n\n\nDeployment & Production\nTorchServe and growing enterprise support, but historically overshadowed by TensorFlow’s tools.\nTensorFlow Serving, TensorFlow Lite, and easy Google Cloud integration.\n\n\n\n\nWhile the fundamental math and building blocks are similar, the biggest difference typically lies in how you prototype, debug, and deploy models."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#similarities",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#similarities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Similarities",
    "text": "Similarities\n\n\n\n\n\n\n\nSimilarity\nDescription\n\n\n\n\nWide Range of Neural Network Layers\nConvolutional, Recurrent, Transformers, etc. Both frameworks maintain robust libraries for standard and advanced layers.\n\n\nAuto-Differentiation\nNo need to manually compute gradients; backpropagation is handled automatically.\n\n\nGPU Acceleration\nBoth leverage CUDA (NVIDIA GPUs) or other backends to speed up training.\n\n\nRich Communities\nAbundant tutorials, example code, pretrained models, and Q&A forums.\n\n\n\n\nDespite differing philosophies, PyTorch and TensorFlow share many core functionalities and have large, supportive user communities."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Advantages and Disadvantages",
    "text": "Comparison of Advantages and Disadvantages\n\n\n\n\n\n\n\n\n\nPyTorch\nTensorFlow\n\n\n\n\nAdvantages\n- Intuitive, Pythonic Syntax: Feels like standard Python, reducing friction for experimentation  - Dynamic Graph Execution: Simplifies debugging and model design  - Research & Academia Favorite: widely used in cutting-edge papers\n- Static Graph Optimization: Graph-based execution can be highly optimized for speed and memory usage  - Extensive Production Ecosystem: Includes TensorFlow Serving, TensorFlow Lite, TFX for data pipelines  - Large Corporate Adoption: Backed by Google, widely used in enterprise settings\n\n\nDisadvantages\n- Deployment Maturity: Production tooling and ecosystem are improving but still behind TensorFlow  - Smaller Enterprise Adoption: Historically overshadowed by TensorFlow’s widespread adoption\n- Learning Curve: The graph-based approach can be challenging for newcomers  - Historically Less Intuitive: Older APIs and tutorials can be confusing, though Eager Mode improves usability"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#recommendations",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#recommendations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommendations",
    "text": "Recommendations\n\n\n\nChoose PyTorch if:\n\nYour focus is on rapid experimentation and academic research\nYou prioritize a Pythonic workflow and easy debugging\nYou prefer a dynamic graph approach (about it).\nYou are working on cutting-edge models with high flexibility\nYou value seamless interaction with Python libraries\n\n\n\n\nChoose TensorFlow if:\n\nYou need robust production and deployment pipelines\nYou plan to integrate with Google Cloud services\nYou require support for mobile/edge devices (e.g., TensorFlow Lite)\nYou benefit from static graph optimization for performance\nYou want an end-to-end ecosystem (TFX, TensorBoard, Serving)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#tensors-in-pytorch",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#tensors-in-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tensors in PyTorch",
    "text": "Tensors in PyTorch"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#datasets-dataloaders-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders\n\n\n\nThe code below extracts a single image‑tensor from the training_data used in the tutorial (you can use test_data the same way), prints its basic properties, and visualizes it.\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Choose the index of the image you wish to inspect\nidx = 0  # e.g., the first image; change as desired\n\n# Fetch the sample\nimage_tensor, label = training_data[idx]   # image_tensor is a 1×28×28 tensor\n\n# Inspect the raw tensor values\nprint(\"Shape :\", image_tensor.shape)  # torch.Size([1, 28, 28])\nprint(\"Label :\", label) # integer class id\nprint(\"Tensor (first 5 rows):\\n\", image_tensor[0, :5, :])\n\n# Visualize the image\nplt.imshow(image_tensor.squeeze(), cmap=\"gray\")\nplt.title(f\"Fashion‑MNIST class{label}\")\nplt.axis(\"off\")\nplt.show()\n\n\n\nHow it works\n\nIndex selection – set idx to any integer in range(len(training_data)).\n\nDataset access – indexing the dataset returns (image, label) with the transform already applied (here, ToTensor() scales to [0,1]).\n\nInspection – the printed tensor slice lets you verify pixel values, and plt.imshow renders the sample for visual confirmation.\n\n\nTo see a different image you just need to adjust the index."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#transforms",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#transforms",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Transforms",
    "text": "Transforms"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nWhat Are We Doing?\nWe are defining a neural network class using PyTorch. This network is designed to work with images, specifically 28×28 grayscale images like those from the FashionMNIST dataset. The network will output 10 values, one for each digit from 0 to 9.\nStep-by-Step Breakdown\n\nclass NeuralNetwork(nn.Module):\n\nWe create a new neural network class called NeuralNetwork. It inherits from PyTorch’s nn.Module, which is the base class for all neural network models.\n\ndef __init__(self): and super().__init__()\n\n__init__ is the constructor. It’s run when we create the model.\nsuper().__init__() tells Python to also run the initialization code from the parent class (nn.Module). This is required for PyTorch to keep track of everything inside the model.\n\nself.flatten = nn.Flatten():\n\nchanges the input from a 2D image (28×28) into a 1D vector (784 values), which is easier for linear layers to handle."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-2",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\nHere we build the main body of the neural network.\nself.linear_relu_stack = nn.Sequential(\n    nn.Linear(28*28, 512),\n    nn.ReLU(),\n    nn.Linear(512, 512),\n    nn.ReLU(),\n    nn.Linear(512, 10),\n)\nIn most contexts when we say “how many layers?” we refer to the learnable ones. So this network has three fully‑connected (Linear) layers, with ReLU activations in between.\n\nYou can think of the linear layer as a filter that projects the image into a new space with 512 dimensions. These new values are not pixels anymore, but rather abstract features learned by the network."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-3",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nFirst layer nn.Linear(28*28, 512): takes the 784 values from the image and transforms them into 512 values. A Linear(784, 512) layer performs:\n\nA matrix multiplication between the input vector (length 784) and a weight matrix of size [784 × 512], followed by adding a bias vector of length 512.\nMathematically: \\[\n\\text{output} = x \\cdot W + b\n\\]\nx is the input vector: shape [784]\nW is the weight matrix: shape [784 × 512]\nb is the bias vector: shape [512]\nThe result (output) is a new vector of shape [512]\n\n\n\nEach of the 512 output values is a linear combination of all 784 pixel values in the input image. By default, PyTorch initializes weights using Kaiming Uniform Initialization (a variant of He initialization), which works well with ReLU activation functions."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-4",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nnn.ReLU(): applies the ReLU activation function, which keeps positive numbers and turns negative numbers into zero. This adds non-linearity to the model.\nSecond layernn.Linear(512, 512): takes those 512 values and again outputs 512 values. This is a hidden layer, helping the model learn more complex patterns.\nnn.ReLU(): Another non-linear transformation.\nThird (Final) layer:nn.Linear(512, 10): takes the 512 values and produces 10 output values.\n\nThese are called logits, and each one corresponds to a digit class (0 to 9)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-5",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nforward(self, x): This is the forward pass, the function that runs when we send data through the model.\nStep-by-step:\n\n\nx = self.flatten(x): Convert the 28×28 image into a 1D tensor with 784 values.\nlogits = self.linear_relu_stack(x): Pass the input through the series of layers.\nreturn logits: Output the final predictions (raw scores for each class).\n\n\nIn summary this neural network:\n\nTakes an image (28×28) as input,\nFlattens it into a vector,\nPasses it through two fully connected layers with ReLU,\nOutputs a vector of size 10 (one for each digit)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-6",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\ntorch.rand(1, 28, 28, device=device): Creates a random image with shape [1, 28, 28]\n\n1 is the batch size (just one image)\n28×28 is the image dimension\ndevice=device ensures the tensor goes to CPU or GPU (wherever the model is)\n\n\n\n\n# To see tensor:\nprint(X)\n\n\n\nLet’s say the tensor shown is:\nX = torch.tensor([[\n    [0.1177, 0.2669, 0.6367, 0.6148, 0.3085, ...],  # row 0\n    [0.8672, 0.3645, 0.4822, 0.9566, 0.8999, ...],  # row 1\n    ...\n]])\n\n\nThis is a 3D tensor of shape [1, 28, 28]:\n\nThe first dimension 1 is the batch size,\nThe next two are height and width of the image.\n\nThe full index of 0.2669 in the 3D tensor is: X[0, 0, 1].\n\n0 → first (and only) image in the batch\n0 → first row of the image\n1 → second column in that row"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-7",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#build-the-neural-network-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\nlogits = model(X): This calls the model with input X.\n\nBehind the scenes, it runs model.forward(X)\nOutput: a vector of 10 values (called logits), one for each class (digits 0 through 9)\n\nNote: We do not call model.forward() directly — PyTorch manages hooks and gradients when we use model(X)\npred_probab = nn.Softmax(dim=1)(logits): Applies softmax to the raw output logits\n\nSoftmax turns logits into probabilities (values between 0 and 1 that sum to 1)\ndim=1 means we apply softmax across the 10 output class values (not across the batch)\n\ny_pred = pred_probab.argmax(1): Picks the index of the largest probability, i.e., the predicted class\n\nargmax(1) returns the class with the highest probability from each row (here we have just one row)\n\nprint(f\"Predicted class: {y_pred}\"): Prints the predicted digit class (0 through 9)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Automatic Differentiation with torch.autograd",
    "text": "Automatic Differentiation with torch.autograd"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#optimizing-model-parameters",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#optimizing-model-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Optimizing Model Parameters",
    "text": "Optimizing Model Parameters"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#save-and-load-the-model",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#save-and-load-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Save and Load the Model",
    "text": "Save and Load the Model"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#introduction-to-pytorch---youtube-series",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#introduction-to-pytorch---youtube-series",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Introduction to PyTorch - YouTube Series",
    "text": "Introduction to PyTorch - YouTube Series\n\n\n\nPro tip: Use Colab with a GPU runtime to speed up operations Runtime &gt; Change runtime type &gt; GPU"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#neural-networks---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#neural-networks---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Neural Networks - Video",
    "text": "Neural Networks - Video\n\n\n\n\n\n\nBut what is a neural network?"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\n\nNetwork Diagram of Single Layer Neural Network"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Introduction and Layers Overview",
    "text": "Single Layer Neural Network: Introduction and Layers Overview\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nNeural networks are often displayed using network diagrams, as shown in the figure.\n\nInput Layer (Orange Circles):\n\n\\(X_1, X_2, X_3, X_4\\)\nThese are observed variables from the dataset.\n\nHidden Layer (Blue Circles):\n\n\\(A_1, A_2, A_3, A_4, A_5\\)\nThese are transformations (activations) computed from the inputs.\n\nOutput Layer (Pink Circle):\n\n\\(f(X) \\to Y\\)\n\\(Y\\) is also observed, e.g., a label or continuous response."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Observed vs. Latent",
    "text": "Single Layer Neural Network: Observed vs. Latent\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nWhere is the observed data?\n\n\\(X_j\\) are observed (the input features).\n\\(Y\\) is observed (the response or label).\nThe hidden units (\\(A_k\\)) are not observed; they’re learned transformations."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Hidden Layer as Transformations",
    "text": "Single Layer Neural Network: Hidden Layer as Transformations\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nIn the hidden layer, each activation \\(A_k\\) is computed as:\n\\[\nA_k = g\\Bigl(w_{k0} + \\sum_{j=1}^4 w_{kj} X_j\\Bigr),\n\\]\n\nIn the formula, these \\(h_k(X)\\) are the same as the activations \\(A_k\\).\n\\(h_k(X)\\) = \\(g(w_{k0} + \\sum_{j=1}^p w_{kj} X_j)\\).\n\\(g(\\cdot)\\) is a nonlinear function (e.g., ReLU, sigmoid, tanh).\n\\(w_{kj}\\) are the weights learned during training.\nEach hidden unit has a different set of weights, hence different transformations."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-training-the-network",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-training-the-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Training the Network",
    "text": "Single Layer Neural Network: Training the Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\n\nThe network learns all weights \\(w_{kj}, w_{k0}, \\beta_k, \\beta_0\\) during training.\nObjective: predict \\(Y\\) from \\(X\\) accurately.\nKey insight: Hidden layer learns useful transformations on the fly to help approximate the true function mapping \\(X\\) to \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-details",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#single-layer-neural-network-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Details",
    "text": "Single Layer Neural Network: Details\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A_k = h_k(X) = g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j)\\) are called the activations in the hidden layer. We can think of it as a non-linear tranformation of a linear function.\n\\(g(z)\\) is called the activation function. Two popular activation functions are: the sigmoid and rectified linear (ReLU).\nActivation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.\nSo the activations are like derived features — nonlinear transformations of linear combinations of the features.\nThe model is fit by minimizing \\(\\sum_{i=1}^{n} (y_i - f(x_i))^2\\) (e.g., for regression)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#nn-example-mnist-digits",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#nn-example-mnist-digits",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NN Example: MNIST Digits",
    "text": "NN Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights).\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent - Video",
    "text": "Gradient Descent - Video\n\n\n\n\n\n\nGradient descent, how neural networks learn"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-intuition---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-intuition---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Intuition - Video",
    "text": "Backpropagation Intuition - Video\n\n\n\n\n\n\nBackpropagation, intuitively"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-calculus---video",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#backpropagation-calculus---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Calculus - Video",
    "text": "Backpropagation Calculus - Video\n\n\n\n\n\n\nBackpropagation calculus"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#fitting-neural-networks-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#fitting-neural-networks-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Neural Networks",
    "text": "Fitting Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\min_{\\{w_k\\}_{1}^K, \\beta} \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2, \\quad \\text{where}\n\\]\n\\[\nf(x_i) = \\beta_0 + \\sum_{k=1}^K \\beta_k g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}\\right).\n\\]\nThis problem is difficult because the objective is non-convex.\nDespite this, effective algorithms have evolved that can optimize complex neural network problems efficiently."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#non-convex-functions-and-gradient-descent",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#non-convex-functions-and-gradient-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non Convex Functions and Gradient Descent",
    "text": "Non Convex Functions and Gradient Descent\nLet \\(R(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - f_\\theta(x_i))^2\\) with \\(\\theta = (\\{w_k\\}_{1}^K, \\beta)\\).\n\n\n\n\n\n\n\n\n\n\nStart with a guess \\(\\theta^0\\) for all the parameters in \\(\\theta\\), and set \\(t = 0\\).\nIterate until the objective \\(R(\\theta)\\) fails to decrease:\n\nFind a vector \\(\\delta\\) that reflects a small change in \\(\\theta\\), such that \\(\\theta^{t+1} = \\theta^t + \\delta\\) reduces the objective; i.e., \\(R(\\theta^{t+1}) &lt; R(\\theta^t)\\).\nSet \\(t \\gets t + 1\\)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent-continued",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#gradient-descent-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent Continued",
    "text": "Gradient Descent Continued\n\nIn this simple example, we reached the global minimum.\nIf we had started a little to the left of \\(\\theta^0\\), we would have gone in the other direction and ended up in a local minimum.\nAlthough \\(\\theta\\) is multi-dimensional, we have depicted the process as one-dimensional. It is much harder to identify whether one is in a local minimum in high dimensions.\nHow to find a direction \\(\\delta\\) that points downhill? We compute the gradient vector: \\[\n\\nabla R(\\theta^t) = \\frac{\\partial R(\\theta)}{\\partial \\theta} \\bigg|_{\\theta = \\theta^t}\n\\]\ni.e., the vector of partial derivatives at the current guess \\(\\theta^t\\).\nThe gradient points uphill, so our update is \\(\\delta = - \\rho \\nabla R(\\theta^t)\\) or \\[\n\\theta^{t+1} \\gets \\theta^t - \\rho \\nabla R(\\theta^t),\n\\] where \\(\\rho\\) is the learning rate (typically small, e.g., \\(\\rho = 0.001\\))."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#gradients-and-backpropagation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#gradients-and-backpropagation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradients and Backpropagation",
    "text": "Gradients and Backpropagation\n\n\\[\nR(\\theta) = \\sum_{i=1}^n R_i(\\theta) \\text{ is a sum, so gradient is sum of gradients.}\n\\]\n\\[\nR_i(\\theta) = \\frac{1}{2}(y_i - f_\\theta(x_i))^2 = \\frac{1}{2} \\left( y_i - \\beta_0 - \\sum_{k=1}^K \\beta_k g\\left( w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij} \\right) \\right)^2\n\\]\nFor ease of notation, let\n\\[\nz_{ik} = w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}.\n\\]\nBackpropagation uses the chain rule for differentiation:\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial \\beta_k} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\beta_k}\n= -(y_i - f_\\theta(x_i)) \\cdot g(z_{ik}).\n\\]\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial w_{kj}} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial g(z_{ik})} \\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}} \\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}}\n= -(y_i - f_\\theta(x_i)) \\cdot \\beta_k \\cdot g'(z_{ik}) \\cdot x_{ij}.\n\\]"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#tricks-of-the-trade",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#tricks-of-the-trade",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tricks of the Trade",
    "text": "Tricks of the Trade\n\nSlow learning. Gradient descent is slow, and a small learning rate \\(\\rho\\) slows it even further. With early stopping, this is a form of regularization.\nStochastic gradient descent. Rather than compute the gradient using all the data, use a small minibatch drawn at random at each step. E.g. for MNIST data, with \\(n = 60K\\), we use minibatches of 128 observations.\nAn epoch is a count of iterations and amounts to the number of minibatch updates such that \\(n\\) samples in total have been processed; i.e. \\(60K/128 \\approx 469\\) for MNIST.\nRegularization. Ridge and lasso regularization can be used to shrink the weights at each layer. Two other popular forms of regularization are dropout and augmentation, discussed next."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#dropout-learning",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#dropout-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dropout Learning",
    "text": "Dropout Learning\n\n\n\n\n\n\n\n\n\n\nAt each Stochastic Gradient Descent (SGD) update, randomly remove units with probability \\(\\phi\\), and scale up the weights of those retained by \\(1/(1-\\phi)\\) to compensate.\nIn simple scenarios like linear regression, a version of this process can be shown to be equivalent to ridge regularization.\nAs in ridge, the other units stand in for those temporarily removed, and their weights are drawn closer together.\nSimilar to randomly omitting variables when growing trees in random forests."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#ridge-and-data-augmentation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#ridge-and-data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge and Data Augmentation",
    "text": "Ridge and Data Augmentation\n\n\n\n\n\n\n\n\n\n\nMake many copies of each \\((x_i, y_i)\\) and add a small amount of Gaussian noise to the \\(x_i\\) — a little cloud around each observation — but leave the copies of \\(y_i\\) alone!\nThis makes the fit robust to small perturbations in \\(x_i\\), and is equivalent to ridge regularization in an OLS setting."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation-on-the-fly",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation-on-the-fly",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation on the Fly",
    "text": "Data Augmentation on the Fly\n\n\n\n\n\n\n\n\n\n\nData augmentation is especially effective with SGD, here demonstrated for a CNN and image classification.\nNatural transformations are made of each training image when it is sampled by SGD, thus ultimately making a cloud of images around each original training image.\nThe label is left unchanged — in each case still tiger.\nImproves performance of CNN and is similar to ridge."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#double-descent",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#double-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Double Descent",
    "text": "Double Descent\n\nWith neural networks, it seems better to have too many hidden units than too few.\nLikewise more hidden layers better than few.\nRunning stochastic gradient descent till zero training error often gives good out-of-sample error.\nIncreasing the number of units or layers and again training till zero error sometimes gives even better out-of-sample error.\nWhat happened to overfitting and the usual bias-variance trade-off?\n\nBelkin, Hsu, Ma, and Mandal (arXiv 2018) Reconciling Modern Machine Learning and the Bias-Variance Trade-off."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#the-double-descent-error-curve",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#the-double-descent-error-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Double-Descent Error Curve",
    "text": "The Double-Descent Error Curve\n\n\n\n\n\n\n\n\n\n\nWhen \\(d \\leq 20\\), model is OLS, and we see usual bias-variance trade-off.\nWhen \\(d &gt; 20\\), we revert to minimum-norm. As \\(d\\) increases above 20, \\(\\sum_{j=1}^d \\hat{\\beta}_j^2\\) decreases since it is easier to achieve zero error, and hence less wiggly solutions."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#less-wiggly-solutions",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#less-wiggly-solutions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Less Wiggly Solutions",
    "text": "Less Wiggly Solutions\n\n\n\n\n\n\n\n\n\n\nTo achieve a zero-residual solution with \\(d = 20\\) is a real stretch!\nEasier for larger \\(d\\)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#some-facts",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#some-facts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Facts",
    "text": "Some Facts\n\nIn a wide linear model (\\(p \\gg n\\)) fit by least squares, SGD with a small step size leads to a minimum norm zero-residual solution.\nStochastic gradient flow — i.e. the entire path of SGD solutions — is somewhat similar to ridge path.\nBy analogy, deep and wide neural networks fit by SGD down to zero training error often give good solutions that generalize well.\nIn particular cases with high signal-to-noise ratio — e.g. image recognition — are less prone to overfitting; the zero-error solution is mostly signal!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-introduction",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-introduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN: Introduction",
    "text": "CNN: Introduction\n\nNeural networks rebounded around 2010 with big successes in image classification.\nAround that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#the-cifar100-database",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#the-cifar100-database",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The CIFAR100 Database",
    "text": "The CIFAR100 Database\n\n\n\n\n\n\n\n\n\n\n\nThe figure shows 75 images drawn from the CIFAR100 database.\nThis database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale).\nEach image has a resolution of 32 × 32 pixels, with three eight-bit numbers per pixel representing red, green, and blue. The numbers for each image are organized in a three-dimensional array called a feature map.\nThe first two axes are spatial (both 32-dimensional), and the third is the channel axis, representing the three (blue, green or red) colors.\nThere is a designated training set of 50,000 images, and a test set of 10,000."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#the-convolutional-network-hierarchy",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#the-convolutional-network-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Convolutional Network Hierarchy",
    "text": "The Convolutional Network Hierarchy\n\n\n\n\n\n\n\n\n\n\n\nCNNs mimic, to some degree, how humans classify images, by recognizing specific features or patterns anywhere in the image that distinguish each particular object class.\nThe network first identifies low-level features in the input image, such as small edges or patches of color.\nThese low-level features are then combined to form higher-level features, such as parts of ears or eyes. Eventually, the presence or absence of these higher-level features contributes to the probability of any given output class.\nThis hierarchical construction is achieved by combining two specialized types of hidden layers: convolution layers and pooling layers:\nConvolution layers search for instances of small patterns in the image.\nPooling layers downsample these results to select a prominent subset.\nTo achieve state-of-the-art results, contemporary neural network architectures often use many convolution and pooling layers."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-layer",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Layer",
    "text": "Convolution Layer\n\n\nA convolution layer is made up of a large number of convolution filters, each of which is a template that determines whether a particular local feature is present in an image.\nA convolution filter relies on a very simple operation, called a convolution, which basically amounts to repeatedly multiplying matrix elements and then adding the results. \\[\n\\text{Input Image} =\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l\n\\end{bmatrix}\n\\quad \\text{Convolution Filter} =\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{bmatrix}.\n\\]\nWhen we convolve the image with the filter, we get the result: \\[\n\\text{Convolved Image} =\n\\begin{bmatrix}\na\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}.\n\\]\nthe convolution filter is applied to every 2 × 2 submatrix of the original image in order to obtain the convolved image.\nIf a 2 × 2 submatrix of the original image resembles the convolution filter, then it will have a large value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that resemble the convolution filter.\nThe filter is itself an image and represents a small shape, edge, etc.\nThe filters are learned during training."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-example",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#convolution-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Example",
    "text": "Convolution Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe idea of convolution with a filter is to find common patterns that occur in different parts of the image.\nThe two filters shown here highlight vertical and horizontal stripes.\nThe result of the convolution is a new feature map.\nSince images have three color channels, the filter does as well: one filter per channel, and dot-products are summed.\nThe weights in the filters are learned by the network."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#pooling-layer",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#pooling-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pooling Layer",
    "text": "Pooling Layer\nA pooling layer provides a way to condense a large image into a smaller summary image.\n\\[\n\\text{Max pool}\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]\n\nEach non-overlapping \\(2 \\times 2\\) block is replaced by its maximum.\nThis sharpens the feature identification.\nAllows for locational invariance.\nReduces the dimension by a factor of 4 — i.e., factor of 2 in each dimension."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#architecture-of-a-cnn",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#architecture-of-a-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Architecture of a CNN",
    "text": "Architecture of a CNN\n\n\n\n\n\n\n\n\n\n\nMany convolve + pool layers.\nFilters are typically small, e.g., each channel \\(3 \\times 3\\).\nEach filter creates a new channel in the convolution layer.\nAs pooling reduces size, the number of filters/channels is typically increased.\nNumber of layers can be very large.\nE.g., resnet50 trained on imagenet 1000-class image database has 50 layers!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\n\n\n\n\n\n\n\n\n\n\nAn additional important trick used with image modeling is data augmentation.\nEssentially, each training image is replicated many times, with each replicate randomly distorted in a natural way such that human recognition is unaffected.\nTypical distortions are zoom, horizontal and vertical shift, shear, small rotations, and in this case horizontal flips.\nAt face value this is a way of increasing the training set considerably with somewhat different examples, and thus protects against overfitting.\nIn fact we can see this as a form of regularization: we build a cloud of images around each original image, all with the same label."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN Example: Pretrained Networks to Classify Images",
    "text": "CNN Example: Pretrained Networks to Classify Images\n\n\n\n\n\n\n\n\n\nHere we use the 50-layer resnet50 network trained on the 1000-class imagenet corpus to classify some photographs.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-imdb-movie-reviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-imdb-movie-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification: IMDB Movie Reviews",
    "text": "Document Classification: IMDB Movie Reviews\nThe IMDB corpus consists of user-supplied movie ratings for a large collection of movies. Each has been labeled for sentiment as positive or negative. Here is the beginning of a negative review:\n\nThis has to be one of the worst films of the 1990s. When my friends & I were watching this film (being the target audience it was aimed at) we just sat & watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn …\n\nWe have labeled training and test sets, each consisting of 25,000 reviews, and each balanced with regard to sentiment.\nGoal: We want to build a classifier to predict the sentiment of a review."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#featurization-bag-of-words",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#featurization-bag-of-words",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Featurization: Bag-of-Words",
    "text": "Featurization: Bag-of-Words\nDocuments have different lengths and consist of sequences of words. How do we create features \\(X\\) to characterize a document?\n\nFrom a dictionary, identify the 10K most frequently occurring words.\nCreate a binary vector of length \\(p = 10K\\) for each document, and score a 1 in every position that the corresponding word occurred.\nWith \\(n\\) documents, we now have an \\(n \\times p\\) sparse feature matrix \\(\\mathbf{X}\\).\nWe compare a lasso logistic regression model to a two-hidden-layer neural network on the next slide. (No convolutions here!)\nBag-of-words are unigrams. We can instead use bigrams (occurrences of adjacent word pairs) and, in general, m-grams."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews",
    "text": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews\n\n\n\n\n\n\n\n\n\n\n\nSimpler lasso logistic regression model works as well as neural network in this case.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#recurrent-neural-networks---rnn-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#recurrent-neural-networks---rnn-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks - RNN",
    "text": "Recurrent Neural Networks - RNN\n\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#simple-recurrent-neural-network-architecture",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#simple-recurrent-neural-network-architecture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Recurrent Neural Network Architecture",
    "text": "Simple Recurrent Neural Network Architecture\n\n\n\n\n\n\n\n\n\n\nThe hidden layer is a sequence of vectors \\(A_\\ell\\), receiving as input \\(X_\\ell\\) as well as \\(A_{\\ell-1}\\). \\(A_\\ell\\) produces an output \\(O_\\ell\\).\nThe same weights \\(\\mathbf{W}\\), \\(\\mathbf{U}\\), and \\(\\mathbf{B}\\) are used at each step in the sequence — hence the term recurrent.\nThe \\(A_\\ell\\) sequence represents an evolving model for the response that is updated as each element \\(X_\\ell\\) is processed."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-in-detail",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN in Detail",
    "text": "RNN in Detail\n\nSuppose \\(X_\\ell = (X_{\\ell1}, X_{\\ell2}, \\ldots, X_{\\ell p})\\) has \\(p\\) components, and \\(A_\\ell = (A_{\\ell1}, A_{\\ell2}, \\ldots, A_{\\ell K})\\) has \\(K\\) components. Then the computation at the \\(k\\)-th components of hidden unit \\(A_\\ell\\) is:\n\\[\nA_{\\ell k} = g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_{\\ell j} + \\sum_{s=1}^{K} u_{ks} A_{\\ell-1,s}\\right)\n\\]\n\\[\nO_\\ell = \\beta_0 + \\sum_{k=1}^{K} \\beta_k A_{\\ell k}\n\\]\nOften we are concerned only with the prediction \\(O_L\\) at the last unit. For squared error loss, and \\(n\\) sequence/response pairs, we would minimize:\n\\[\n\\sum_{i=1}^{n} (y_i - o_{iL})^2 = \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} x_{iL,j} + \\sum_{s=1}^{K} u_{ks} a_{i,L-1,s}\\right)\\right)\\right)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-for-document-classification-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-for-document-classification-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN for Document Classification",
    "text": "RNN for Document Classification\n\nThe document feature is a sequence of words \\(\\{\\mathcal{W}_\\ell\\}_{1}^{L}\\). We typically truncate/pad the documents to the same number \\(L\\) of words (we use \\(L = 500\\)).\nEach word \\(\\mathcal{W}_\\ell\\) is represented as a one-hot encoded binary vector \\(X_\\ell\\) (dummy variable) of length \\(10K\\), with all zeros and a single one in the position for that word in the dictionary.\nThis results in an extremely sparse feature representation and would not work well.\nInstead, we use a lower-dimensional pretrained word embedding matrix \\(\\mathbf{E}\\) (\\(m \\times 10K\\), next slide).\nThis reduces the binary feature vector of length \\(10K\\) to a real feature vector of dimension \\(m \\ll 10K\\) (e.g., \\(m\\) in the low hundreds)."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Word Embedding - RNN Example: IMDB Reviews",
    "text": "Word Embedding - RNN Example: IMDB Reviews\n\nReview:\n\nthis is one of the best films actually the best I have ever seen the film starts one fall day…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings are pretrained on very large corpora of documents, using methods similar to principal components. word2vec and GloVe are popular.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-time-series-forecasting",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-time-series-forecasting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN: Time Series Forecasting",
    "text": "RNN: Time Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew-York Stock Exchange Data\nThree daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#autocorrelation",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#autocorrelation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\n\n\n\n\n\n\nThe autocorrelation at lag \\(\\ell\\) is the correlation of all pairs \\((v_t, v_{t-\\ell})\\) that are \\(\\ell\\) trading days apart.\nThese sizable correlations give us confidence that past values will be helpful in predicting the future.\nThis is a curious prediction problem: the response \\(v_t\\) is also a feature \\(v_{t-\\ell}\\)!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-forecaster",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Forecaster",
    "text": "RNN Forecaster\nWe only have one series of data! How do we set up for an RNN?\nWe extract many short mini-series of input sequences \\(\\mathbf{X} = \\{ X_1, X_2, \\ldots, X_L \\}\\) with a predefined length \\(L\\) known as the lag:\n\\[\nX_1 = \\begin{pmatrix}\nv_{t-L} \\\\\nr_{t-L} \\\\\nz_{t-L}\n\\end{pmatrix}, \\quad\nX_2 = \\begin{pmatrix}\nv_{t-L+1} \\\\\nr_{t-L+1} \\\\\nz_{t-L+1}\n\\end{pmatrix}, \\quad\n\\cdots, \\quad\nX_L = \\begin{pmatrix}\nv_{t-1} \\\\\nr_{t-1} \\\\\nz_{t-1}\n\\end{pmatrix}, \\quad \\text{and} \\quad Y = v_t.\n\\]\nSince \\(T = 6,051\\), with \\(L = 5\\), we can create 6,046 such \\((X, Y)\\) pairs.\nWe use the first 4,281 as training data, and the following 1,770 as test data. We fit an RNN with 12 hidden units per lag step (i.e., per \\(A_\\ell\\))."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-results-for-nyse-data",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#rnn-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Results for NYSE Data",
    "text": "RNN Results for NYSE Data\n\n\n\n\n\n\n\n\n\nThe figure shows predictions and truth for the test period.\n\\[\nR^2 = 0.42 \\text{ for RNN}\n\\]\n\\(R^2 = 0.18\\) for the naive approach — uses yesterday’s value of Log trading volume to predict that of today.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-forecaster",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Forecaster",
    "text": "Autoregression Forecaster\nThe RNN forecaster is similar in structure to a traditional autoregression procedure.\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\nv_{L+1} \\\\\nv_{L+2} \\\\\nv_{L+3} \\\\\n\\vdots \\\\\nv_T\n\\end{bmatrix}, \\quad\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & v_L & v_{L-1} & \\cdots & v_1 \\\\\n1 & v_{L+1} & v_L & \\cdots & v_2 \\\\\n1 & v_{L+2} & v_{L+1} & \\cdots & v_3 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & v_{T-1} & v_{T-2} & \\cdots & v_{T-L}\n\\end{bmatrix}.\n\\]\nFit an OLS regression of \\(\\mathbf{y}\\) on \\(\\mathbf{M}\\), giving:\n\\[\n\\hat{v}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 v_{t-1} + \\hat{\\beta}_2 v_{t-2} + \\cdots + \\hat{\\beta}_L v_{t-L}.\n\\]\nKnown as an order-\\(L\\) autoregression model or \\(AR(L)\\).\nFor the NYSE data, we can include lagged versions of DJ_return and log_volatility in matrix \\(\\mathbf{M}\\), resulting in \\(3L + 1\\) columns."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-results-for-nyse-data",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#autoregression-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Results for NYSE Data",
    "text": "Autoregression Results for NYSE Data\n\n\\(R^2 = 0.41 \\text{ for } AR(5) \\text{ model (16 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for RNN model (205 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for } AR(5) \\text{ model fit by neural network.}\\)\n\\(R^2 = 0.46 \\text{ for all models if we include } \\textbf{day_of_week} \\text{ of day being predicted.}\\)"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#summary-of-rnns",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#summary-of-rnns",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary of RNNs",
    "text": "Summary of RNNs\n\nWe have presented the simplest of RNNs. Many more complex variations exist.\nOne variation treats the sequence as a one-dimensional image, and uses CNNs for fitting. For example, a sequence of words using an embedding representation can be viewed as an image, and the CNN convolves by sliding a convolutional filter along the sequence.\nCan have additional hidden layers, where each hidden layer is a sequence, and treats the previous hidden layer as an input sequence.\nCan have output also be a sequence, and input and output share the hidden units. So called seq2seq learning are used for language translation."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#when-to-use-deep-learning-1",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#when-to-use-deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\n\nCNNs have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.\nRNNs have had big wins in speech modeling, language translation, and forecasting.\n\n\nShould we always use deep learning models?\n\nOften the big successes occur when the signal to noise ratio is high — e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.\nFor noisier data, simpler models can often work better:\n\nOn the NYSE data, the AR(5) model is much simpler than an RNN, and performed as well.\nOn the IMDB review data, a linear model fit (e.g. with glmnet) did as well as the neural network, and better than the RNN."
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\nAs the authors suggest, I also endorse the Occam’s razor principle — we prefer simpler models if they work as well. More interpretable!"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#additional-material",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Additional Material",
    "text": "Additional Material\n\n3Blue1Brown: Neural Networks\nDeep Learning, by Ian Goodfellow and Yoshua Bengio and Aaron Courvill\nWelch Labs: Neural Networks Demystified\nWelch Labs: Learning To See\nDistill: A Gentle Introduction to Graph Neural Networks\nNeural Networks and Deep Learning, by Michael Nielsen\nCITS4012 Natural Language Processing\nDeep Learning with PyTorch Step-by-Step"
  },
  {
    "objectID": "lecture_slides/11_deep_learning/11_deep_learning.html#summary",
    "href": "lecture_slides/11_deep_learning/11_deep_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nDeep Learning Renaissance\n\nNeural networks first rose to prominence in the 1980s, waned in the 1990s, then surged again around 2010.\nAdvances in computing (GPUs) and availability of massive labeled datasets propelled deep learning success.\n\nFrameworks (PyTorch vs. TensorFlow)\n\nPyTorch is known for its dynamic graph and Pythonic syntax; widely used in research.\nTensorFlow has an extensive production ecosystem, ideal for enterprise and deployment.\n\nEssential Concepts\n\nAutomatic differentiation, gradient descent, and backpropagation are at the core of training neural networks.\n\n\n\n\n\n\nCNNs and RNNs\n\nCNNs excel in image classification by learning local patterns via convolution and pooling layers.\n\nRNNs (and variants like LSTM, GRU) handle sequential data for tasks like language modeling and time-series forecasting.\n\nWhen to Use Deep Learning\n\nWorks best on large datasets with high signal-to-noise ratio (e.g., image, text).\n\nSimpler models often perform well on noisier tasks or smaller datasets.\n\nOver-parameterization can still generalize due to “double-descent” effects.\n\nPractical Tips\n\nUse regularization (dropout, data augmentation, weight decay) to mitigate overfitting.\n\nMonitor convergence with appropriate learning rates and consider mini-batch stochastic gradient descent."
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, time series, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 1007\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required):\n\nGoogle Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\nMicrosoft Copilot: is an AI-powered assistant designed to enhance productivity and streamline workflows across various applications and services. It utilizes large language models and is integrated within Microsoft 365 apps like Word, Excel, PowerPoint, Outlook, and Teams, providing real-time, context-aware assistance for tasks such as drafting documents, analyzing data, managing projects, and communicating more efficiently. Users can leverage Copilot to automate repetitive tasks, generate ideas, summarize information, and access data across their work environment and the web, all within a secure and privacy-conscious framework.\n\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance\n3%\n\n\nParticipation\n7%\n\n\nQuizzes\n10%\n\n\nHomework\n30%\n\n\nCourse Case Competition\n20%\n\n\nFinal Project\n30%\n\n\n\n\nAttendance\nAttend your classes. If you do not attend class, you will likely not succeed. The instructor will take attendance and keep the attendance record.\n\n\nParticipation\nParticipate in activities, and complete any participatory exercises. Almost every class the instructor you will be required to take participatory activities. Your participation grade will be based on this record.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nCourse Case Competition\nWwe will have a semester-long, team-based predictive analytics competition hosted on Kaggle. Students must work in teams and will be allowed up to five submissions per day. The Kaggle platform will automatically evaluate submissions and maintain a leaderboard throughout the competition period.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (30%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPeer Review (10%): Each group will review and provide constructive feedback on other teams’ posters. This process encourages engagement, enhances critical analysis skills, and promotes a culture of constructive critique.\nPoster Presentation at the Purdue Undergraduate Research Conference (20%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\nInstructor/TA Evaluation (20%): After the Undergraduate Research Conference your instructor and the TA will evaluate your final submission based on a rubric that will be shared.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nReadings ISLP\nMaterial*\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction.\nCh. 1; Ch. 2;\nslidespodcast**video**book lab\n- Video: Statistical Learning: 2.1 Introduction to Regression Models- Video: Statistical Learning: 2.2 Dimensionality and Structured Models- Video: Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff- Video: Statistical Learning: 2.4 Classification- Video: Statistical Learning: 2.Py Data Types, Arrays, and Basics - 2023- Video: Statistical Learning: 2.Py.3 Graphics - 2023- Video: Statistical Learning: 2.Py Indexing and Dataframes - 2023\n\n\nWeek 2\nLinear Regression\nCh. 3.\nslidespodcastvideonotebookbook lab\n- Video: Statistical Learning: 3.1 Simple linear regression- Video: Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals- Video: Statistical Learning: 3.3 Multiple Linear Regression- Video: Statistical Learning: 3.4 Some important questions- Video: Statistical Learning: 3.5 Extensions of the Linear Model- Video: Statistical Learning: 3.Py Linear Regression and statsmodels Package - 2023- Video: Statistical Learning: 3.Py Multiple Linear Regression Package - 2023- Video: Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023\n\n\nWeek 3\nClassification\nCh. 4\nslidespodcast - TBPvideo TBPbook lab\n- Video: Statistical Learning: 4.1 Introduction to Classification Problems- Video: Statistical Learning: 4.2 Logistic Regression- Video: Statistical Learning: 4.3 Multivariate Logistic Regression- Video: Statistical Learning: 4.4 Logistic Regression Case Control Sampling and Multiclass- Video: Statistical Learning: 4.5 Discriminant Analysis- Video: Statistical Learning: 4.6 Gaussian Discriminant Analysis (One Variable)- Video: Statistical Learning: 4.7 Gaussian Discriminant Analysis (Many Variables)- Video: Statistical Learning: 4.8 Generalized Linear Models- Video: Statistical Learning: 4.9 Quadratic Discriminant Analysis and Naive Bayes- Video: Statistical Learning: 4.Py Logistic Regression I 2023- Video: Statistical Learning: 4.Py Linear Discriminant Analysis (LDA) I 2023- Video: Statistical Learning: 4.Py K-Nearest Neighbors (KNN) I 2023\n\n\nWeek 4\nResampling Methods\nCh. 5\nslidespodcast - TBPvideo TBPbook lab\n- Video: Statistical Learning: 5.1 Cross Validation- Video: Statistical Learning: 5.2 K-fold Cross Validation- Video: Statistical Learning: 5.3 Cross Validation the wrong and right way- Video: Statistical Learning: 5.4 The Bootstrap- Video: Statistical Learning: 5.5 More on the Bootstrap- Video: Statistical Learning: 5.Py Cross-Validation I 2023- Video: Statistical Learning: 5.Py Bootstrap I 2023- Book Chapter: Modern Dive -Bootstrapping and Confidence Intervals\n\n\nWeek 5\nLinear Model Selection & Regularization\nCh. 6\nslidespodcast - TBPvideo TBPbook lab\n- Video: Statistical Learning: 6.1 Introduction and Best Subset Selection- Video: Statistical Learning: 6.2 Stepwise Selection- Video: Statistical Learning: 6.3 Backward stepwise selection- Video: Statistical Learning: 6.4 Estimating test error- Video: Statistical Learning: 6.5 Validation and cross validation- Video: Statistical Learning: 6.6 Shrinkage methods and ridge regression- Video: Statistical Learning: 6.7 The Lasso- Video: Statistical Learning: 6.8 Tuning parameter selection- Video: Statistical Learning: 6.9 Dimension Reduction Methods- Video: Statistical Learning: 6.10 Principal Components Regression and Partial Least Squares- Video: Statistical Learning: 6.Py Stepwise Regression I 2023- Video: Statistical Learning: 6.Py Ridge Regression and the Lasso I 2023\n\n\nWeek 6\nBeyond Linearity\nCh. 7\nslidespodcast - TBPvideo TBPbook lab\n- Video: Statistical Learning: 7.1 Polynomials and Step Functions- Video: Statistical Learning: 7.2 Piecewise Polynomials and Splines- Video: Statistical Learning: 7.3 Smoothing Splines- Video: Statistical Learning: 7.4 Generalized Additive Models and Local Regression- Video: Statistical Learning: 7.Py Polynomial Regressions and Step Functions I 2023- Video: Statistical Learning: 7.Py Splines I 2023- Video: Statistical Learning: 7.Py Generalized Additive Models (GAMs) I 2023\n\n\nWeek 7\nTree-Based Methods\nCh. 8\nslidespodcast - TBPvideo TBPbook lab\n- Video: Statistical Learning: 8.1 Tree based methods- Video: Statistical Learning: 8.2 More details on Trees- Video: Statistical Learning: 8.3 Classification Trees- Video: Statistical Learning: 8.4 Bagging- Video: Statistical Learning: 8.5 Boosting- Video: Statistical Learning: 8.6 Bayesian Additive Regression Trees- Video: Statistical Learning: 8.Py Tree-Based Methods I 2023\n\n\nWeek 8\nTime Series\nSlides\nslides - TBPpodcast - TBPvideo TBPlab - TBP\n- Video: TBP- Video: TBP\n\n\nWeek 09\nUnsupervised Learning\nCh. 12\nslidespodcast - TBPvideo TBPbook lab\n- Video: Statistical Learning: 12.1 Principal Components- Video: Statistical Learning: 12.2 Higher order principal components- Video: Statistical Learning: 12.3 k means Clustering- Video: Statistical Learning: 12.4 Hierarchical Clustering- Video: Statistical Learning: 12.5 Matrix Completion- Video: Statistical Learning: 12.6 Breast Cancer Example- Video: Statistical Learning: 12.Py Principal Components I 2023- Video: Statistical Learning: 12.Py Clustering I 2023- Video: Statistical Learning: 12.Py Application: NCI60 Data I 2023\n\n\nWeek 10\nElements of Data Communication\n.\nslidespodcast - TBPvideo TBP\nPoster TemplateRubricVideo: Creating a Professional Poster\n\n\nWeek 11\nFinal Project\n.\n.\n.\n\n\nWeek 12\nFinal Project\n.\n.\n.\n\n\nWeek 13\nFinal Project\n.\n.\n.\n\n\nWeek 14\nDeep Learning\nCh. 10\nslidespodcast - TBPvideo TBPbook labcourse lab\n- Video: Statistical Learning: 10.1 Introduction to Neural Networks- Video: Statistical Learning: 10.2 Convolutional Neural Networks- Video: Statistical Learning: 10.3 Document Classification- Video: Statistical Learning: 10.4 Recurrent Neural Networks- Video: Statistical Learning: 10.6 Fitting Neural Networks- Video: Statistical Learning: 10.7 Interpolation and Double Descent- Video: Statistical Learning: 10.Py Single Layer Model: Hitters Data I 2023- Video: Statistical Learning: 10.Py Multilayer Model: MNIST Digit Data I 2023- Video: Statistical Learning: 10.Py Convolutional Neural Network: CIFAR Image Data I 2023- Video: Statistical Learning: 10.Py Document Classification and Recurrent Neural Networks I 2023\n\n\nWeek 15\nDeep Learning\nCh. 10\n.\n.\n\n\nWeek 16\nSpecial Topic: LLM\nslides - TBPpodcast - TBPvideo TBPlab - TBP\n.\n.\n\n\n\n* The majority of the course slides and labs are based on the ISLP book, “An Introduction to Statistical Learning with Applications in Python” by James, G., Witten, D., Hastie, T., and Tibshirani, R., and have been adapted to suit the specific needs of our course. ** This material was generated with Google NotebookLM based on the slides I prepared for the Predictive Analytics course.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overview",
    "href": "lecture_slides/01_introduction/01_introduction.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nSupervised Learning\nUnsupervised Learning\nStatistical Learning Overview\n\nWhat is Statistical Learning?\nParametric and Structured Models\nAssessing Model Accuracy\nClassification Problems\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Management Department at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#students",
    "href": "lecture_slides/01_introduction/01_introduction.html#students",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Students",
    "text": "Students\n\n\nWhat do you expect to gain from this Predictive Analytics course?\n\n\n\nIt is your turn! - 10 minutes\nPresent yourself to your left/right colleague and ask:\nCollect her/his answer and submit your first Participation Assignment!"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\n\n\n\nCourse Info:\n\nBrightspace - Official.\nCourse Webpage - Supplementary.\n\nSyllabus\n\nClass Times & Location: check the course syllabus.\n\nOffice Hours: check the course syllabus for group and individual appointments.\n\nSchedule and Materials:\n\nPodcast (before class)\n\nRequired Readings (before class)\n\nLecture Slides (before class)\n\nLecture Video (during class)\n\nBook labs (during/after class)\n\nSupplementary Material (after class)\n\n\n\n\nCourse Tracks\n\nStandard Track\n\nExternal Case Competition (bonus)\n\nAssessments\n\nAttendance\n\nParticipation\n\nQuizzes\n\nHomework\n\nCourse Case Competition\n\nFinal Project"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "href": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Spam Detection",
    "text": "Spam Detection\n\n\n\nData from 4601 emails sent to an individual (named George, at HP Labs, before 2000). Each is labeled as spam or email.\nGoal: build a customized spam filter.\nInput features: relative frequencies of 57 of the most commonly occurring words and punctuation marks in these email messages.\n\n\n\n\nWord\nSpam\nEmail\n\n\n\n\ngeorge\n0.00\n1.27\n\n\nyou\n2.26\n1.27\n\n\nhp\n0.02\n0.90\n\n\nfree\n0.52\n0.07\n\n\n!\n0.51\n0.11\n\n\nedu\n0.01\n0.29\n\n\nremove\n0.28\n0.01\n\n\n\nAverage percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "href": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Zip Code",
    "text": "Zip Code\n\n\nIdentify the numbers in a handwritten zip code."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Netflix Prize",
    "text": "Netflix Prize\n\n\n\n\n\n\n\n\n\n\n\nVideo: Winning the Netflix Prize\nNetflix Prize - Wiki"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "href": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Starting point",
    "text": "Starting point\n\n\n\n\nOutcome measurement \\(Y\\) (also called dependent variable, response, target).\nVector of \\(p\\) predictor measurements \\(X\\) (also called inputs, regressors, covariates, features, independent variables).\nIn the regression problem, \\(Y\\) is quantitative (e.g., price, blood pressure).\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (e.g., survived/died, digit 0–9, cancer class of tissue sample).\nWe have training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "href": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Objectives",
    "text": "Objectives\nOn the basis of the training data, we would like to:\n\nAccurately predict unseen test cases.\nUnderstand which inputs affect the outcome, and how.\nAssess the quality of our predictions and inferences."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "href": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nIt is important to understand the ideas behind the various techniques, in order to know how and when to use them.\nWe wil understand the simpler methods first to grasp the more sophisticated ones later.\nIt is important to accurately assess the performance of a method, to know how well or how badly it is working."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\n\nDefinition:\nLearning the structure or patterns in data without labeled outputs.\nThe algorithm is unsupervised because no outcome variable guides the learning.\nGoal:\n\nDiscover hidden structures in \\(X\\).\n\nGroup, reduce, or represent data meaningfully.\n\n\n\n\nExamples of Methods:\n\nClustering: k-means, hierarchical clustering\n\nDimensionality Reduction: PCA, t-SNE\n\nApplications:\n\nCustomer segmentation\n\nMarket basket analysis\n\nReducing dimensionality of large datasets\n\nCharacteristics:\n\nDifficult to know how well we are doing.\n\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\n\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#notation",
    "href": "lecture_slides/01_introduction/01_introduction.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\n\n\n\n\n\n\n\n\n\n\nThere can be many \\(Y\\) values at \\(X=4\\). A good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\n\nIs also defined for a vector \\(\\mathbf{X}\\).\n\n\\[\nf(\\mathbf{x}) = f(x_1, x_2, x_3) = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\n\n\nIs the ideal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{is the function that minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error.\n\nEven if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is a distribution of possible \\(Y\\) values.\n\n\n\n\n\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "href": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large.\n\nReason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point \\((0, 0)\\).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point \\((0, 0)\\).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBotton panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\nRed points are simulated values for income from the model:\n\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\nLinear regression model fit to the simulated data:\n\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\n\nHere we use a technique called a thin-plate spline to fit a flexible surface. We control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "href": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. We tunned the parameter all the way down to zero and this surface actually goes through every single data point.\n\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "href": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso.\n\nIntermediate: Least squares, Generalized Additive Models, Trees.\n\nHigh flexibility: Support Vector Machines, Deep Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\), the Mean Squared Error (MSE):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\nBias (tends to underfit)\n\nDefinition: The error introduced by approximating a real-world problem with a simplified model. High bias implies underfitting.\n\nImplication: High bias ⇒ the model misses important patterns (systematic error).\n\nTypical causes: Too-simple model (e.g., overly rigid assumptions), insufficient features, heavy regularization.\n\n\nVariance (tends to overfit)\n\nDefinition: The amount by which the model’s prediction would change if trained on a different training dataset. High variance implies overfitting.\n\nImplication: High variance ⇒ the model is overly sensitive to noise (unstable across samples).\n\nTypical causes: Overly flexible model, too many features vs. observations, weak regularization.\n\n\n\n\n\n\n\n\nThe trade-off we manage\n\n\n\nIncreasing flexibility ↓ bias but ↑ variance.\n\nGoal: choose complexity that minimizes expected test error.\n\n\n\n\nDecomposition of expected test MSE\n\\[\n\\mathbb{E}\\big[(Y-\\hat f(X))^2\\big]\n   \\;=\\; \\underbrace{\\big(\\mathrm{Bias}[\\hat f(X)]\\big)^2}_{\\text{misspecification}}\n   \\;+\\; \\underbrace{\\mathrm{Var}[\\hat f(X)]}_{\\text{sensitivity}}\n   \\;+\\; \\underbrace{\\sigma^2}_{\\text{irreducible noise}}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Panel: Model Fits\n\nBlack Curve: The true generating function, representing the underlying relationship we want to estimate.\nData Points: Observations generated from the black curve, with added noise (error).\nFitted Models:\n\nOrange Line: A simple linear model (low flexibility).\nBlue Line: A moderately flexible model, likely a spline or thin plate spline.\nGreen Line: A highly flexible model that closely fits the data points but may overfit.\n\n\nKey Insight:\nThe green model captures the data points well but risks overfitting, while the orange model is too rigid and misses the underlying structure. The blue model strikes a balance.\n\nBotton Panel: Mean Squared Error (MSE)\n\nGray Curve: Training data MSE.\n\nDecreases consistently as flexibility increases.\nFlexible models fit the training data well, but this does not generalize to test data.\n\nRed Curve: Test data MSE across models of increasing flexibility.\n\nStarts high for rigid models (orange line).\nDecreases to a minimum (optimal model complexity, blue line).\nIncreases again for overly flexible models (green line), due to overfitting.\n\n\nKey Takeaway:\nThere is an optimal model complexity (the “magic point”) where test data MSE is minimized. Beyond this point, models become overly complex and generalization performance deteriorates."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off: Other Examples",
    "text": "Bias-Variance Trade-off: Other Examples\n\n\n\n\n\nHere, the truth is smoother, so smoother fits and linear models perform well.\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off of the Examples",
    "text": "Bias-Variance Trade-off of the Examples\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Problems",
    "text": "Classification Problems\n\n\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "href": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ideal Classifier and Bayes Decision Rule",
    "text": "Ideal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest-Neighbor Averaging",
    "text": "Nearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification: Some Details",
    "text": "Classification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "href": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: K-Nearest Neighbors in Two Dimensions",
    "text": "Example: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 10",
    "text": "KNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 1 vs. K = 100",
    "text": "KNN: K = 1 vs. K = 100\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN Error Rates",
    "text": "KNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure illustrates how training errors (blue curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line is the bayes error, used as reference for comparison."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nStatistical Learning and Predictive Analytics\n\nGoal: Build models to predict outcomes and understand relationships between inputs (predictors) and responses.\nSupervised Learning: Focuses on predicting \\(Y\\) (response) using \\(X\\) (predictors) via models like regression and classification.\nUnsupervised Learning: Focuses on finding patterns in data without predefined responses (e.g., clustering).\n\nBias-Variance Trade-off\n\nKey Trade-off: Model flexibility affects bias and variance:\n\nHigh flexibility → Low bias but high variance (overfitting).\nLow flexibility → High bias but low variance (underfitting).\n\nGoal: Find the optimal flexibility that minimizes test error.\n\n\nTechniques and Applications\n\nParametric Models:\n\nSimpler and interpretable (e.g., linear regression).\nOften used as approximations.\n\nFlexible Models:\n\nHandle complex patterns (e.g., splines, SVMs, deep learning).\nRequire careful tuning to avoid overfitting.\n\n\nPractical Considerations\n\nAssessing Model Accuracy:\n\nUse test data to calculate MSE.\nBalance between training performance and generalizability.\n\n\nKey Challenges\n\nCurse of Dimensionality:\n\nHigh-dimensional data affects distance-based methods like KNN.\nLarger neighborhoods needed, losing “locality.”"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#supervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#supervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\n\n\n\nDefinition:\nLearning a mapping from inputs \\(X\\) to an output \\(Y\\) using labeled data.\nThe algorithm is supervised because the correct answers are known during training.\nGoal:\n\nPredict \\(Y\\) for new unseen \\(X\\).\n\nMinimize prediction error.\n\n\n\n\nExamples of Methods:\n\nLinear Regression, Logistic Regression\n\nDecision Trees, Random Forests, SVMs\n\nNeural Networks\n\nApplications:\n\nPredicting stock prices\n\nDiagnosing diseases\n\nSpam email detection"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#overview",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nRecap\nLinear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\n\nLinear regression is a simple approach to supervised learning. It assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\ldots, X_p\\) is linear.\nTrue regression functions are never linear!\n\n\n\n\n\n\n\n\n\n\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-for-the-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#linear-regression-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression for the Advertising Data",
    "text": "Linear Regression for the Advertising Data\n\n\nConsider the advertising data shown:\n\n\n\n\n\n\n\n\n\nSuppose that, in our job as business analysts, we are asked to suggest, based on this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation?"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Linear Regression using a single predictor \\(X\\)",
    "text": "Simple Linear Regression using a single predictor \\(X\\)\n\n\n\nWe assume a model:\n\n\\[\n  Y = \\beta_0 + \\beta_1X + \\epsilon,\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are two unknown constants that represent the intercept and slope, also known as coefficients or parameters, and \\(\\epsilon\\) is the error term.\n\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the model coefficients, we predict future sales using:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X = x\\). The hat symbol denotes an estimated value."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation of the parameters by least squares",
    "text": "Estimation of the parameters by least squares\n\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) be the prediction for \\(Y\\) based on the \\(i\\)th value of \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nWe define the residual sum of squares (RSS) as:\n\n\\[\n    RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,\n\\]\nor equivalently as:\n\\[\n    RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\nThe least squares approach selects the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS. The minimizing values can be shown to be:\n\n\\[\n    \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad\n    \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\nwhere \\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\) and \\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\) are the sample means."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#example-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#example-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Advertising Data",
    "text": "Example: Advertising Data\n\n\n\n\n\n\n\n\n\n\n\nThe least squares fit for the regression of sales onto TV is shown. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Accuracy of the Coefficient Estimates",
    "text": "Assessing the Accuracy of the Coefficient Estimates\n\n\n\nThe standard error of an estimator reflects how it varies under repeated sampling:\n\n\\[\n  SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right].\n\\]\nwhere \\(\\sigma^2 = Var(\\epsilon)\\)\n\n\nThese standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. It has the form:\n\n\\[\n  \\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1).\n\\]\n\n\n\nThere is approximately a 95% chance that the interval:\n\n\\[\n  \\left[ \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1) \\right]\n\\]\nwill contain the true value of \\(\\beta_1\\) (under a scenario where we obtained repeated samples like the present sample).\n\nFor the advertising data, the 95% confidence interval for \\(\\beta_1\\) is:\n\n\\[\n  [0.042, 0.053].\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nStandard errors can be used to perform hypothesis tests on coefficients. The most common hypothesis test involves testing the null hypothesis:\n\n\\[\n  H_0: \\text{There is no relationship between } X \\text{ and } Y\n\\] versus the alternative hypothesis:\n\\[\n  H_A: \\text{There is some relationship between } X \\text{ and } Y.\n\\]\n\n\nMathematically, this corresponds to testing:\n\n\\[\n  H_0: \\beta_1 = 0\n\\] versus:\n\\[\n  H_A: \\beta_1 \\neq 0,\n\\]\nsince if \\(\\beta_1 = 0\\), then the model reduces to \\(Y = \\beta_0 + \\epsilon\\), and \\(X\\) is not associated with \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#hypothesis-testing-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nTo test the null hypothesis (\\(H_0\\)), compute a \\(t\\)-statistic as follows:\n\n\\[\n  t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.\n\\]\n\nThe \\(t\\)-statistic follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom under the null hypothesis (\\(\\beta_1 = 0\\)).\nUsing statistical software, we can compute the \\(p\\)-value to determine the likelihood of observing a \\(t\\)-statistic as extreme as the one calculated."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-the-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for the Advertising Data",
    "text": "Results for the Advertising Data\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Overall Accuracy of the Model",
    "text": "Assessing the Overall Accuracy of the Model\n\n\n\nResidual Standard Error (RSE):\n\n\\[\n  RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\] where the Residual Sum of Square (RSS) is \\(\\sum_{i=1}^n (y_i - \\hat{y})^2\\).\n\n\n\\(R^2\\), the fraction of variance explained:\n\n\\[\n  R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\] where TSS is the Total Sums of Squares.\n\n\n\nIt can be shown that in this Simple Linear Regression setting that \\(R^2 = r^2\\), where \\(r\\) is the correlation between X and Y:\n\n\\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#advertising-data-results",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#advertising-data-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advertising Data Results",
    "text": "Advertising Data Results\n\nKey metrics for model accuracy:\n\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n3.26\n\n\nR²\n0.612\n\n\nF-statistic\n312.1"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#multiple-linear-regression-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#multiple-linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nHere our model is\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon,\n\\]\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\nIn the advertising example, the model becomes\n\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon.\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interpreting-regression-coefficients",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interpreting-regression-coefficients",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpreting Regression Coefficients",
    "text": "Interpreting Regression Coefficients\n\nThe ideal scenario is when the predictors are uncorrelated — a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed” are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically.\nInterpretations become hazardous — when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation and Prediction for Multiple Regression",
    "text": "Estimation and Prediction for Multiple Regression\n\n\n\nGiven estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\), we can make predictions using the formula:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + \\cdots + \\hat{\\beta}_px_p.\n\\]\n\n\nWe estimate \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) as the values that minimize the sum of squared residuals:\n\n\\[\n  \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2\n             = \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_{i1} - \\hat{\\beta}_2x_{i2} - \\cdots - \\hat{\\beta}_px_{ip} \\right)^2.\n\\]\n\nThis is done using standard statistical software. The values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Advertising Data",
    "text": "Results for Advertising Data\n\n\n\nRegression Coefficients\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599\n\n\n\n\n\n\nCorrelations\n\n\n\n\nPredictor\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n\n\n1.0000\n0.2283\n\n\nsales\n\n\n\n1.0000"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#some-important-questions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#some-important-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Important Questions",
    "text": "Some Important Questions\n\n\nIs at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response?\nDo all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#is-at-least-one-predictor-useful",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#is-at-least-one-predictor-useful",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is at Least One Predictor Useful?",
    "text": "Is at Least One Predictor Useful?\nFor the first question, we can use the F-statistic:\n\\[\nF = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n1.69\n\n\n\\(R^2\\)\n0.897\n\n\nF-statistic\n570\n\n\n\n\nThe F-statistic is huge and it’s p-value is less than \\(.0001\\). This says that there’s a strong association of the predictors on the outcome variable."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#forward-selection",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#forward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Selection",
    "text": "Forward Selection\n\nBegin with the null model — a model that contains an intercept but no predictors.\nFit \\(p\\) Simple Linear Regressions and add to the null model the variable that results in the lowest RSS.\nAdd to that model the variable that results in the lowest RSS amongst all two-variable models.\nContinue until some stopping rule is satisfied:\n\nFor example, when all remaining variables have a p-value above some threshold."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#backward-selection",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#backward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Selection",
    "text": "Backward Selection\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new (\\(p - 1\\))-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached:\n\nFor instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#model-selection",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#model-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Model Selection",
    "text": "Model Selection\n\nWe will discuss other criterias for choosing an “optimal” member in the path of models produced by forward or backward stepwise selection, including:\n\nMallow’s \\(C_p\\)\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\nAdjusted \\(R^2\\)\nCross-validation (CV)"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nSome predictors are qualitative, taking discrete values.\nCategorical predictors can be represented using factor variables.\nQualitative variables: Student (Student Status), Status (Marital Status), Own (Owns a House)."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\nSuppose we investigate differences in credit card balance between those who own a house and those who do not, ignoring the other variables. We create a new variable:\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if } i\\text{th person owns a house} \\\\\n0 & \\text{if } i\\text{th person does not own a house}\n\\end{cases}\n\\]\nResulting model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if } i\\text{th person owns a house} \\\\\n\\beta_0 + \\epsilon_i & \\text{if } i\\text{th person does not own a house.}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-2",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#credit-card-data-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\n\nResults for the model:\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n509.80\n33.13\n15.389\n&lt; 0.0001\n\n\nOwn [Yes]\n19.73\n46.05\n0.429\n0.6690\n\n\n\n\nWe see the coefficient is 19.73, but it’s not significant. The p value is 0.66 which is not significant (&gt; 0.05). So, owing a house don’t generally means a higher credit card balance than not owing one."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors with More Than Two Levels",
    "text": "Qualitative Predictors with More Than Two Levels\n\nWith more than two levels, we create additional dummy variables.\nFor example, for the `region`` variable, we create two dummy variables:\n\\[\nx_{i1} =\n\\begin{cases}\n      1 & \\text{if i-th person is from the South} \\\\\n      0 & \\text{if i-th person is not from the South}\n    \\end{cases}\n\\]\n\\[\nx_{i2} = \\begin{cases}\n      1 & \\text{if i-th person is from the West} \\\\\n      0 & \\text{if i-th person is not from the West}\n    \\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#qualitative-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nBoth variables can be used in the regression equation to obtain the model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i =\n\\begin{cases}\n      \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if i-th person is from the South} \\\\\n      \\beta_0 + \\beta_2 + \\epsilon_i & \\text{if i-th person is from the West}\\\\\n      \\beta_0 + \\epsilon_i & \\text{if i-th person is from the East (baseline)}\n    \\end{cases}\n\\] \nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — East in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-ethnicity",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#results-for-ethnicity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Ethnicity",
    "text": "Results for Ethnicity\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n531.00\n46.32\n11.464\n&lt; 0.0001\n\n\nethnicity \\(South\\)\n-18.69\n65.02\n-0.287\n0.7740\n\n\nethnicity \\(West\\)\n-12.50\n56.68\n-0.221\n0.8260\n\n\n\n\nThe coefficient -18.69 compares South to East and that’s not significant. Likewise, the West to East is also not significant.\n\nNote: the choice of the baseline does not affect the fit of the model. The residual sum of sum of squares will be the same no matter which category we chose as the baseline. At its turn, the p-values will potentially change as we change the baseline category."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\nIn our previous analysis of the Advertising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media.\n\nFor example, the linear model\n\\[\n\\widehat{\\text{sales}} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper}\n\\]\nstates that the average effect on sales of a one-unit increase in TV is always \\(\\beta_1\\), regardless of the amount spent on radio."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\n\nBut suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\nIn this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\nIn marketing, this is known as a synergy effect, and in statistics, it is referred to as an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interaction-in-advertising-data",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interaction-in-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interaction in Advertising Data",
    "text": "Interaction in Advertising Data\n\n\nWhen levels of TV or radio are low, true sales are lower than predicted.\nSplitting advertising between TV and radio underestimates sales."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#modeling-interactions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#modeling-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Modeling Interactions",
    "text": "Modeling Interactions\nModel takes the form:\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{radio} \\times \\text{TV}) + \\epsilon\n\\]\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n6.7502\n0.248\n27.23\n&lt; 0.0001\n\n\nTV\n0.0191\n0.002\n12.70\n&lt; 0.0001\n\n\nradio\n0.0289\n0.009\n3.24\n0.0014\n\n\nTV × radio\n0.0011\n0.000\n20.73\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interpretation",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe results in this table suggest that interactions are important.The p-value for the interaction term TV \\(\\times\\) radio is extremely low, indicating that there is strong evidence for ( H_A : \\(\\beta_3 \\neq 0\\)).\nThe ( \\(R^2\\) ) for the interaction model is 96.8%, compared to only 89.7% for the model that predicts sales using TV and radio without an interaction term.\nThis means that (\\(\\frac{96.8 - 89.7}{100 - 89.7}\\)) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\nThe coefficient estimates in the table suggest that an increase in TV advertising of $1,000 is associated with increased sales of (\\(\\hat{\\beta}_1 + \\hat{\\beta}_3 \\times \\text{radio}\\)) \\(\\times 1000 = 19 + 1.1 \\times \\text{radio} \\text{ units}.\\)\nAn increase in radio advertising of $1,000 will be associated with an increase in sales of (\\(\\hat{\\beta}_2 + \\hat{\\beta}_3 \\times \\text{TV}\\)) \\(\\times 1000 = 29 + 1.1 \\times \\text{TV} \\text{ units}.\\)"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#hierarchy",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchy",
    "text": "Hierarchy\n\nSometimes it is the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.\nThe hierarchy principle:\n\nIf we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.\n\nThe rationale for this principle is that interactions are hard to interpret in a model without main effects.\nSpecifically, the interaction terms also contain main effects, if the model has no main effect terms."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions Between Qualitative and Quantitative Variables",
    "text": "Interactions Between Qualitative and Quantitative Variables\nConsider the Credit data set, and suppose that we wish to predict balance using income (quantitative) and student (qualitative).\nWithout an interaction term, the model takes the form:\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]\n\\[\n= \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_0 + \\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n\\beta_0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#with-interactions-it-takes-the-form",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#with-interactions-it-takes-the-form",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "With Interactions, It Takes the Form",
    "text": "With Interactions, It Takes the Form\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\\[\n=\n\\begin{cases}\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & \\text{if student} \\\\\n\\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{if not student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#visualizing-interactions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#visualizing-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\n\n\nLeft: no interaction between income and student.\nRight: with an interaction term between income and student."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-effects-of-predictors-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-effects-of-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear effects of predictors",
    "text": "Non-linear effects of predictors\n\nPolynomial regression on Auto data"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-regression-results",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#non-linear-regression-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear regression results",
    "text": "Non-linear regression results\nThe figure suggests that the following model\n\\[\nmpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon\n\\]\nmay provide a better fit.\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n56.9001\n1.8004\n31.6\n&lt; 0.0001\n\n\nhorsepower\n-0.4662\n0.0311\n-15.0\n&lt; 0.0001\n\n\n\\(\\text{horsepower}^2\\)\n0.0012\n0.0001\n10.1\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#what-we-did-not-cover",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#what-we-did-not-cover",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What we did not cover",
    "text": "What we did not cover\n\n\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#generalizations-of-the-linear-model",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#generalizations-of-the-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\n\nIn much of the rest of the course we discuss methods that expand the scope of linear models and how they are fit:\n\nClassification problems: logistic regression, support vector machines.\nNon-linearity: kernel smoothing, splines, generalized additive models; nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests, boosting (these also capture non-linearities).\nRegularized fitting: Ridge regression and lasso."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#summary-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nLinear Regression:\n\nA foundational supervised learning method.\nAssumes a linear relationship between predictors (\\(X\\)) and the response (\\(Y\\)).\nUseful for both prediction and understanding relationships.\n\nSimple vs. Multiple Regression:\n\nSimple regression: one predictor.\nMultiple regression: multiple predictors.\n\nKey Metrics:\n\nResidual Standard Error (RSE), \\(R^2\\), and F-statistic.\nConfidence intervals and hypothesis testing for coefficients.\n\n\n\n\nQualitative Predictors:\n\nUse dummy variables for categorical predictors.\nInterpret results based on chosen baselines.\n\nInteractions:\n\nModels with interaction terms (e.g., \\(X_1 \\times X_2\\)) capture synergistic effects.\n\nNon-linear Effects:\n\nPolynomial regression accounts for curvature in data.\n\nChallenges:\n\nMulticollinearity, outliers, high leverage points.\nOverfitting vs. underfitting: balance flexibility and interpretability."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#recap",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#recap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\nWhat is Statistical Learning?\n\n\nA framework to learn the relationship between predictors \\(X\\) and an outcome \\(Y\\) for prediction and inference, using parametric (e.g., linear/logistic regression) and non-parametric methods (e.g., trees, kNN).\n\n\n\n\nWhat is the difference between supervised and unsupervised learning?\n\n\nSupervised: learn \\(f: X \\to Y\\) from labeled data to predict/explain \\(Y\\). Unsupervised: find structure in \\(X\\) without labels (e.g., clustering, dimensionality reduction).\n\n\n\n\nWhat is the curse of dimensionality?\n\n\nWith many features/predictors, data become sparse; distances degrade, required sample size grows rapidly, and generalization deteriorates, especially for local/partitioning methods.\n\n\n\n\n\nWhat is the difference between overfitting and underfitting?\n\n\nOverfitting: too flexible; captures noise (high variance); low training error, poor test error. Underfitting: too simple; misses signal (high bias); high error on both train and test.\n\n\n\n\nWhy assess accuracy on test data rather than training data?\n\n\nTraining accuracy is optimistically biased (evaluated on seen data). Test accuracy estimates out-of-sample performance and reveals overfitting."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#recap-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#recap-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recap",
    "text": "Recap\n\n\nWhat is Statistical Learning?\n\n\n\nA framework for estimating and interpreting the relationship between predictors \\(X\\) and an outcome \\(Y\\) for prediction and inference. Methods can be parametric (e.g., linear/logistic regression) or non-parametric (e.g., trees, kNN).\n\n\n\nWhat is the difference between supervised and unsupervised learning?\n\n\n\nSupervised: learn \\(f: X \\rightarrow Y\\) using labeled outcomes to predict or explain \\(Y\\).\nUnsupervised: find structure in \\(X\\) without labels (e.g., clustering, dimensionality reduction).\n\n\n\nWhat is the curse of dimensionality?\n\n\n\nAs the number of features grows, data become sparse and distances lose meaning; sample requirements rise exponentially, hurting generalization and making naive nearest-neighbor/partitioning methods unreliable.\n\n\n\n\nWhat is the difference between overfitting and underfitting?\n\n\n\nOverfitting: model captures noise (high variance), low training error, poor test performance.\nUnderfitting: model too simple (high bias), misses signal, high error on both train and test.\n\n\n\nWhy is test accuracy preferred over training accuracy?\n\n\n\nTraining accuracy is optimistically biased because the model is evaluated on data it has seen. Test (or CV) accuracy estimates out-of-sample performance, detecting overfitting and guiding model/parameter selection."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#participation",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#participation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Participation",
    "text": "Participation\n\n\nIn pairs, ask AI:\n\nIn the context of Statistical Learning, what is the fundamental difference between a regression problem and a classification problem in supervised learning, based on the nature of the outcome variable?\n\n\n\n\n\nPresent the response you got to your right/left classmate. Do your understanding converge?\nSubmit your participation response."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#deciding-on-the-important-variables-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions",
    "text": "Guiding Questions\n\n\n\n\nIs there a relationship between advertising budget and sales?\nOur first goal is to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, one might argue that no money should be spent on advertising.\nHow strong is the relationship between advertising budget and sales?\nAssuming a relationship exists, how strong is it? Does knowledge of the advertising budget provide a lot of information about product sales?\nWhich media are associated with sales?\nAre all three media—TV, radio, and newspaper—associated with sales, or just one or two? We must separate the individual contribution of each medium to sales when money is spent on all three.\nHow large is the association between each medium and sales?\nFor every dollar spent on a particular medium, by what amount will sales increase? How accurately can we predict this amount?\n\n\n\nHow accurately can we predict future sales?\nFor any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?\nIs the relationship linear?\nIf the relationship between advertising expenditure and sales is approximately linear, then linear regression is appropriate. If not, we may need to transform the predictor(s) or the response so that linear regression can be used.\nIs there synergy among the advertising media?\nPerhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect; in statistics, it is called an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-13",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-13",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions (1–3)",
    "text": "Guiding Questions (1–3)\n\nIs there a relationship between advertising budget and sales?\nOur first goal is to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, one might argue that no money should be spent on advertising.\nHow strong is the relationship between advertising budget and sales?\nAssuming a relationship exists, how strong is it? Does knowledge of the advertising budget provide a lot of information about product sales?\nWhich media are associated with sales?\nAre all three media—TV, radio, and newspaper—associated with sales, or just one or two? We must separate the individual contribution of each medium to sales when money is spent on all three."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-45",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-45",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions (4–5)",
    "text": "Guiding Questions (4–5)\n\nHow large is the association between each medium and sales?\nFor every dollar spent on a particular medium, by what amount will sales increase? How accurately can we predict this amount?\nHow accurately can we predict future sales?\nFor any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?"
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-67",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#guiding-questions-67",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Guiding Questions (6–7)",
    "text": "Guiding Questions (6–7)\n\nIs the relationship linear?\nIf the relationship between advertising expenditure and sales is approximately linear, then linear regression is appropriate. If not, we may need to transform the predictor(s) or the response so that linear regression can be used.\nIs there synergy among the advertising media?\nPerhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect; in statistics, it is called an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-guiding-questions",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-guiding-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Marketing Plan: Guiding Questions",
    "text": "The Marketing Plan: Guiding Questions\n\n\n\n\nIs there a relationship between advertising budget and sales?\nOur first goal is to determine whether the data provide evidence of an association between advertising expenditure and sales. If the evidence is weak, one might argue that no money should be spent on advertising.\nHow strong is the relationship between advertising budget and sales?\nAssuming a relationship exists, how strong is it? Does knowledge of the advertising budget provide a lot of information about product sales?\nWhich media are associated with sales?\nAre all three media—TV, radio, and newspaper—associated with sales, or just one or two? We must separate the individual contribution of each medium to sales when money is spent on all three.\nHow large is the association between each medium and sales?\nFor every dollar spent on a particular medium, by what amount will sales increase? How accurately can we predict this amount?\n\n\n\nHow accurately can we predict future sales?\nFor any given level of television, radio, or newspaper advertising, what is our prediction for sales, and what is the accuracy of this prediction?\nIs the relationship linear?\nIf the relationship between advertising expenditure and sales is approximately linear, then linear regression is appropriate. If not, we may need to transform the predictor(s) or the response so that linear regression can be used.\nIs there synergy among the advertising media?\nPerhaps spending $50,000 on television advertising and $50,000 on radio advertising is associated with higher sales than allocating $100,000 to either television or radio individually. In marketing, this is known as a synergy effect; in statistics, it is called an interaction effect."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Marketing Plan: Answers",
    "text": "The Marketing Plan: Answers\n\n\n\n1.Is there a relationship between sales and advertising budget?\nFit a multiple regression of sales on TV, radio, and newspaper, and test\n\\[\nH_0:\\ \\beta_{\\text{TV}}=\\beta_{\\text{radio}}=\\beta_{\\text{newspaper}}=0.\n\\]\nUsing the \\(F\\)-statistic, the very small \\(p\\)-value indicates clear evidence of a relationship between advertising and sales.\n\n2. How strong is the relationship?\nTwo accuracy measures:\n\nRSE: estimates the standard deviation of the response from the population regression line. For Advertising, RSE \\(\\approx\\) 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average. In the data set, the mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23 %.\n\\(R^2\\) (variance explained): records the percentage of variability in the response that is explained by the predictors. Predictors explain ~90% of the variance in sales.\n\n\n\n\n3. Which media are associated with sales?\nCheck \\(p\\)-values for each predictor’s \\(t\\)-statistic. The \\(p\\)-values for TV and radio are low, but newspaper is not, suggesting only TV and radio are related to sales.\n\n\n4. How large is the association between each medium and sales?\nWe used SEs to build 95% CIs for coefficients related to TV, Radio, Newspaper.\nTV and radio intervals are narrow and far from 0, which means a strong evidence of association. Newspaper CI includes 0, not significant given TV and radio.\n\n\n5. How accurately can we predict future sales?\nAccuracy depends on whether predicting an individual response, \\(Y=f(X)+\\epsilon\\), or the average response, \\(f(X)\\).\n\nIndividual \\(\\rightarrow\\) prediction interval\nAverage \\(\\rightarrow\\) confidence interval\n\n\nPrediction intervals are always wider because they include irreducible error uncertainty from \\(\\epsilon\\)."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#section-1",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#section-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "5. How accurately can we predict future sales?\nUse (3.21). Accuracy depends on whether predicting an individual response, \\(Y=f(X)+\\epsilon\\), or the average response, \\(f(X)\\) (Sec. 3.2.2).\n- Individual ⇒ prediction interval\n- Average ⇒ confidence interval\nPrediction intervals are always wider because they include irreducible error uncertainty from \\(\\epsilon\\).\n6. Is the relationship linear?\nResidual plots (Sec. 3.3.3) help diagnose nonlinearity; linear relationships yield residuals with no pattern. For Advertising, Figure 3.5 indicates a nonlinear effect; transformations (Sec. 3.3.2) can accommodate nonlinearity within linear regression.\n\n7. Is there synergy among the advertising media?\nStandard linear regression assumes additivity. This is interpretable but may be unrealistic. Include an interaction term (Sec. 3.3.2) to allow non-additive relationships; a small \\(p\\)-value on the interaction indicates synergy. For Advertising, Figure 3.5 suggests non-additivity; adding an interaction increases \\(R^2\\) substantially—from ~90% to almost 97%."
  },
  {
    "objectID": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-2",
    "href": "lecture_slides/02_linear_regression/02_linear_regression.html#the-marketing-plan-answers-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Marketing Plan: Answers",
    "text": "The Marketing Plan: Answers\n\n\n\n6. Is the relationship linear?\nResidual plots help diagnose nonlinearity; linear relationships yield residuals with no pattern. Transformations can accommodate nonlinearity within linear regression.\n\n\n\n7. Is there synergy among the advertising media?\nStandard linear regression assumes additivity. This is interpretable but may be unrealistic. An interaction term allows non-additive relationships; a small \\(p\\)-value on the interaction indicates synergy. For Advertising, adding an interaction increases \\(R^2\\) substantially — from ~90% to almost 97%."
  }
]